{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Modelling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Metrics \n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data\n",
    "test = pd.DataFrame({'x':[0,1,2,3,5,6,7], 'y':[10,20,30,40, 60,80,70]})\n",
    "\n",
    "x, y = test[['x']], test[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Linear regression model from scratch using full gradient descent\n",
    "\n",
    "def LinearRegression(X_train, y_train, epochs = 1000, learning_rate = 0.01):\n",
    "    # Initializing random weight with a random normal distribution by default, mean is 0 and std is 1\n",
    "    w1 = abs(np.random.normal())\n",
    "    b = 0\n",
    "    \n",
    "    # Making initial prediction\n",
    "    y_pred = X_train*w1 + b\n",
    "    \n",
    "    # Creating a list that stores the mean squared error\n",
    "    mse_list = []\n",
    "    \n",
    "    # Looping through epochs and performing gradient descent\n",
    "    for x in range(epochs):\n",
    "        print('Epoch '+str(x+1)+' ============================')\n",
    "        print('weight{}, bias{}'.format(w1, b))\n",
    "\n",
    "        # Negative gradient of loss with respect to weight(w1) multiplied by learning rate \n",
    "        x1_neg_gradient = ((y_train - y_pred.values)*X_train.values)*learning_rate\n",
    "        \n",
    "        # Negative gradient of loss with respect to intercept or bias(b) multiplied by learning rate\n",
    "        c_neg_gradient = (y_train - y_pred.values)*learning_rate\n",
    "\n",
    "        # Calculating the Mean Squared Error between predicted and real for first time\n",
    "        mse = mean_squared_error(y_train, y_pred)\n",
    "        \n",
    "        # Printing the loss function(mse in my case) on screen\n",
    "        print('Train loss: {}'.format(mse), '\\n')\n",
    "        \n",
    "        # Appending the loss to the mse_list\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Updating the weight and bias with the negative of the gradient of the loss with respect to the weight and bias.\n",
    "        for a, d in zip(x1_neg_gradient.values, c_neg_gradient.values):\n",
    "            w1 += a\n",
    "            b += d \n",
    "        \n",
    "        # Calculating new Y predicted with updated weight and bias\n",
    "        y_pred = X_train*w1 + b\n",
    "        \n",
    "        #...loop continues\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ============================\n",
      "weight1.1917622321468417, bias0\n",
      "Train loss: 2081.7622703595375 \n",
      "\n",
      "Epoch 2 ============================\n",
      "weight[14.41397706], bias[2.81397706]\n",
      "Train loss: 220.45126820768652 \n",
      "\n",
      "Epoch 3 ============================\n",
      "weight[10.56529101], bias[2.25764417]\n",
      "Train loss: 65.77974062652582 \n",
      "\n",
      "Epoch 4 ============================\n",
      "weight[11.62249556], bias[2.66393924]\n",
      "Train loss: 52.038937592266 \n",
      "\n",
      "Epoch 5 ============================\n",
      "weight[11.27125565], bias[2.78806456]\n",
      "Train loss: 49.9757458801115 \n",
      "\n",
      "Epoch 6 ============================\n",
      "weight[11.32576315], bias[2.98779868]\n",
      "Train loss: 48.918215472168086 \n",
      "\n",
      "Epoch 7 ============================\n",
      "weight[11.26474516], bias[3.16046962]\n",
      "Train loss: 47.98349602840786 \n",
      "\n",
      "Epoch 8 ============================\n",
      "weight[11.23794845], bias[3.33569791]\n",
      "Train loss: 47.096922496042865 \n",
      "\n",
      "Epoch 9 ============================\n",
      "weight[11.20232487], bias[3.50509143]\n",
      "Train loss: 46.25062649842976 \n",
      "\n",
      "Epoch 10 ============================\n",
      "weight[11.17022009], bias[3.67117706]\n",
      "Train loss: 45.44233025899358 \n",
      "\n",
      "Epoch 11 ============================\n",
      "weight[11.13806469], bias[3.83334184]\n",
      "Train loss: 44.670290539779955 \n",
      "\n",
      "Epoch 12 ============================\n",
      "weight[11.10686243], bias[3.99187239]\n",
      "Train loss: 43.932877982348586 \n",
      "\n",
      "Epoch 13 ============================\n",
      "weight[11.07630364], bias[4.14679434]\n",
      "Train loss: 43.22853925338807 \n",
      "\n",
      "Epoch 14 ============================\n",
      "weight[11.04645648], bias[4.29820586]\n",
      "Train loss: 42.55579093161806 \n",
      "\n",
      "Epoch 15 ============================\n",
      "weight[11.01728104], bias[4.44618189]\n",
      "Train loss: 41.91321614899521 \n",
      "\n",
      "Epoch 16 ============================\n",
      "weight[10.9887689], bias[4.59080171]\n",
      "Train loss: 41.299461587301366 \n",
      "\n",
      "Epoch 17 ============================\n",
      "weight[10.96090305], bias[4.73214106]\n",
      "Train loss: 40.71323462633974 \n",
      "\n",
      "Epoch 18 ============================\n",
      "weight[10.93366941], bias[4.87027445]\n",
      "Train loss: 40.15330062143322 \n",
      "\n",
      "Epoch 19 ============================\n",
      "weight[10.90705347], bias[5.00527458]\n",
      "Train loss: 39.618480303144885 \n",
      "\n",
      "Epoch 20 ============================\n",
      "weight[10.88104127], bias[5.13721253]\n",
      "Train loss: 39.107647293633704 \n",
      "\n",
      "Epoch 21 ============================\n",
      "weight[10.85561909], bias[5.26615774]\n",
      "Train loss: 38.619725734405385 \n",
      "\n",
      "Epoch 22 ============================\n",
      "weight[10.83077356], bias[5.39217812]\n",
      "Train loss: 38.15368802046119 \n",
      "\n",
      "Epoch 23 ============================\n",
      "weight[10.8064916], bias[5.51534]\n",
      "Train loss: 37.70855263607314 \n",
      "\n",
      "Epoch 24 ============================\n",
      "weight[10.78276042], bias[5.63570821]\n",
      "Train loss: 37.283382087626855 \n",
      "\n",
      "Epoch 25 ============================\n",
      "weight[10.75956753], bias[5.75334614]\n",
      "Train loss: 36.877280929179044 \n",
      "\n",
      "Epoch 26 ============================\n",
      "weight[10.73690072], bias[5.8683157]\n",
      "Train loss: 36.48939387657077 \n",
      "\n",
      "Epoch 27 ============================\n",
      "weight[10.71474806], bias[5.98067743]\n",
      "Train loss: 36.11890400612524 \n",
      "\n",
      "Epoch 28 ============================\n",
      "weight[10.69309788], bias[6.09049048]\n",
      "Train loss: 35.765031034135625 \n",
      "\n",
      "Epoch 29 ============================\n",
      "weight[10.67193879], bias[6.19781265]\n",
      "Train loss: 35.42702967352046 \n",
      "\n",
      "Epoch 30 ============================\n",
      "weight[10.65125965], bias[6.30270046]\n",
      "Train loss: 35.10418806418419 \n",
      "\n",
      "Epoch 31 ============================\n",
      "weight[10.63104957], bias[6.40520911]\n",
      "Train loss: 34.79582627377866 \n",
      "\n",
      "Epoch 32 ============================\n",
      "weight[10.61129792], bias[6.50539257]\n",
      "Train loss: 34.50129486570635 \n",
      "\n",
      "Epoch 33 ============================\n",
      "weight[10.59199428], bias[6.60330359]\n",
      "Train loss: 34.21997353135123 \n",
      "\n",
      "Epoch 34 ============================\n",
      "weight[10.57312851], bias[6.69899371]\n",
      "Train loss: 33.95126978365465 \n",
      "\n",
      "Epoch 35 ============================\n",
      "weight[10.55469067], bias[6.79251331]\n",
      "Train loss: 33.69461770928683 \n",
      "\n",
      "Epoch 36 ============================\n",
      "weight[10.53667105], bias[6.88391162]\n",
      "Train loss: 33.449476776784074 \n",
      "\n",
      "Epoch 37 ============================\n",
      "weight[10.51906016], bias[6.97323675]\n",
      "Train loss: 33.21533069814312 \n",
      "\n",
      "Epoch 38 ============================\n",
      "weight[10.50184874], bias[7.06053574]\n",
      "Train loss: 32.9916863414735 \n",
      "\n",
      "Epoch 39 ============================\n",
      "weight[10.48502772], bias[7.14585454]\n",
      "Train loss: 32.77807269241967 \n",
      "\n",
      "Epoch 40 ============================\n",
      "weight[10.46858826], bias[7.22923807]\n",
      "Train loss: 32.57403986216354 \n",
      "\n",
      "Epoch 41 ============================\n",
      "weight[10.45252168], bias[7.31073022]\n",
      "Train loss: 32.379158139920115 \n",
      "\n",
      "Epoch 42 ============================\n",
      "weight[10.43681954], bias[7.39037391]\n",
      "Train loss: 32.193017087929164 \n",
      "\n",
      "Epoch 43 ============================\n",
      "weight[10.42147357], bias[7.46821104]\n",
      "Train loss: 32.01522467703787 \n",
      "\n",
      "Epoch 44 ============================\n",
      "weight[10.40647569], bias[7.54428261]\n",
      "Train loss: 31.845406461053642 \n",
      "\n",
      "Epoch 45 ============================\n",
      "weight[10.39181801], bias[7.61862866]\n",
      "Train loss: 31.68320478812795 \n",
      "\n",
      "Epoch 46 ============================\n",
      "weight[10.3774928], bias[7.69128833]\n",
      "Train loss: 31.5282780475105 \n",
      "\n",
      "Epoch 47 ============================\n",
      "weight[10.36349253], bias[7.76229988]\n",
      "Train loss: 31.380299950087686 \n",
      "\n",
      "Epoch 48 ============================\n",
      "weight[10.34980982], bias[7.83170068]\n",
      "Train loss: 31.238958841189373 \n",
      "\n",
      "Epoch 49 ============================\n",
      "weight[10.33643748], bias[7.89952728]\n",
      "Train loss: 31.103957044217132 \n",
      "\n",
      "Epoch 50 ============================\n",
      "weight[10.32336846], bias[7.96581537]\n",
      "Train loss: 30.97501023371185 \n",
      "\n",
      "Epoch 51 ============================\n",
      "weight[10.31059588], bias[8.03059987]\n",
      "Train loss: 30.85184683653924 \n",
      "\n",
      "Epoch 52 ============================\n",
      "weight[10.29811302], bias[8.09391486]\n",
      "Train loss: 30.73420745993395 \n",
      "\n",
      "Epoch 53 ============================\n",
      "weight[10.28591331], bias[8.1557937]\n",
      "Train loss: 30.62184434519542 \n",
      "\n",
      "Epoch 54 ============================\n",
      "weight[10.27399032], bias[8.21626895]\n",
      "Train loss: 30.51452084588711 \n",
      "\n",
      "Epoch 55 ============================\n",
      "weight[10.26233778], bias[8.27537244]\n",
      "Train loss: 30.412010929438118 \n",
      "\n",
      "Epoch 56 ============================\n",
      "weight[10.25094955], bias[8.33313531]\n",
      "Train loss: 30.314098701099493 \n",
      "\n",
      "Epoch 57 ============================\n",
      "weight[10.23981964], bias[8.38958794]\n",
      "Train loss: 30.220577949251076 \n",
      "\n",
      "Epoch 58 ============================\n",
      "weight[10.22894218], bias[8.44476007]\n",
      "Train loss: 30.13125171110178 \n",
      "\n",
      "Epoch 59 ============================\n",
      "weight[10.21831146], bias[8.49868075]\n",
      "Train loss: 30.045931857869327 \n",
      "\n",
      "Epoch 60 ============================\n",
      "weight[10.20792187], bias[8.55137834]\n",
      "Train loss: 29.96443869856437 \n",
      "\n",
      "Epoch 61 ============================\n",
      "weight[10.19776795], bias[8.60288061]\n",
      "Train loss: 29.886600601546053 \n",
      "\n",
      "Epoch 62 ============================\n",
      "weight[10.18784435], bias[8.65321466]\n",
      "Train loss: 29.81225363305057 \n",
      "\n",
      "Epoch 63 ============================\n",
      "weight[10.17814584], bias[8.70240699]\n",
      "Train loss: 29.741241211932934 \n",
      "\n",
      "Epoch 64 ============================\n",
      "weight[10.16866732], bias[8.7504835]\n",
      "Train loss: 29.673413779893117 \n",
      "\n",
      "Epoch 65 ============================\n",
      "weight[10.1594038], bias[8.7974695]\n",
      "Train loss: 29.608628486493654 \n",
      "\n",
      "Epoch 66 ============================\n",
      "weight[10.15035041], bias[8.84338972]\n",
      "Train loss: 29.54674888830408 \n",
      "\n",
      "Epoch 67 ============================\n",
      "weight[10.14150237], bias[8.88826834]\n",
      "Train loss: 29.487644661539292 \n",
      "\n",
      "Epoch 68 ============================\n",
      "weight[10.13285503], bias[8.93212899]\n",
      "Train loss: 29.43119132758647 \n",
      "\n",
      "Epoch 69 ============================\n",
      "weight[10.12440384], bias[8.97499475]\n",
      "Train loss: 29.37726999084227 \n",
      "\n",
      "Epoch 70 ============================\n",
      "weight[10.11614434], bias[9.0168882]\n",
      "Train loss: 29.325767088308478 \n",
      "\n",
      "Epoch 71 ============================\n",
      "weight[10.10807219], bias[9.05783138]\n",
      "Train loss: 29.276574150418362 \n",
      "\n",
      "Epoch 72 ============================\n",
      "weight[10.10018314], bias[9.09784586]\n",
      "Train loss: 29.229587572590724 \n",
      "\n",
      "Epoch 73 ============================\n",
      "weight[10.09247304], bias[9.1369527]\n",
      "Train loss: 29.18470839702917 \n",
      "\n",
      "Epoch 74 ============================\n",
      "weight[10.08493782], bias[9.17517248]\n",
      "Train loss: 29.1418421043089 \n",
      "\n",
      "Epoch 75 ============================\n",
      "weight[10.07757353], bias[9.21252533]\n",
      "Train loss: 29.1008984143104 \n",
      "\n",
      "Epoch 76 ============================\n",
      "weight[10.07037627], bias[9.24903091]\n",
      "Train loss: 29.061791096081873 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 ============================\n",
      "weight[10.06334228], bias[9.28470844]\n",
      "Train loss: 29.024437786229054 \n",
      "\n",
      "Epoch 78 ============================\n",
      "weight[10.05646783], bias[9.3195767]\n",
      "Train loss: 28.988759815451164 \n",
      "\n",
      "Epoch 79 ============================\n",
      "weight[10.04974931], bias[9.35365405]\n",
      "Train loss: 28.954682042856355 \n",
      "\n",
      "Epoch 80 ============================\n",
      "weight[10.04318319], bias[9.38695844]\n",
      "Train loss: 28.922132697708452 \n",
      "\n",
      "Epoch 81 ============================\n",
      "weight[10.03676601], bias[9.41950738]\n",
      "Train loss: 28.891043228271858 \n",
      "\n",
      "Epoch 82 ============================\n",
      "weight[10.03049439], bias[9.45131802]\n",
      "Train loss: 28.861348157435298 \n",
      "\n",
      "Epoch 83 ============================\n",
      "weight[10.02436502], bias[9.48240711]\n",
      "Train loss: 28.832984944811635 \n",
      "\n",
      "Epoch 84 ============================\n",
      "weight[10.01837469], bias[9.512791]\n",
      "Train loss: 28.805893855022163 \n",
      "\n",
      "Epoch 85 ============================\n",
      "weight[10.01252023], bias[9.54248571]\n",
      "Train loss: 28.78001783188901 \n",
      "\n",
      "Epoch 86 ============================\n",
      "weight[10.00679857], bias[9.57150685]\n",
      "Train loss: 28.755302378269825 \n",
      "\n",
      "Epoch 87 ============================\n",
      "weight[10.0012067], bias[9.59986971]\n",
      "Train loss: 28.73169544128182 \n",
      "\n",
      "Epoch 88 ============================\n",
      "weight[9.99574166], bias[9.62758923]\n",
      "Train loss: 28.709147302674214 \n",
      "\n",
      "Epoch 89 ============================\n",
      "weight[9.99040059], bias[9.65467998]\n",
      "Train loss: 28.687610474117186 \n",
      "\n",
      "Epoch 90 ============================\n",
      "weight[9.98518066], bias[9.68115624]\n",
      "Train loss: 28.66703959718735 \n",
      "\n",
      "Epoch 91 ============================\n",
      "weight[9.98007914], bias[9.70703195]\n",
      "Train loss: 28.647391347838845 \n",
      "\n",
      "Epoch 92 ============================\n",
      "weight[9.97509334], bias[9.73232072]\n",
      "Train loss: 28.628624345159416 \n",
      "\n",
      "Epoch 93 ============================\n",
      "weight[9.97022063], bias[9.75703586]\n",
      "Train loss: 28.610699064218466 \n",
      "\n",
      "Epoch 94 ============================\n",
      "weight[9.96545844], bias[9.7811904]\n",
      "Train loss: 28.593577752824242 \n",
      "\n",
      "Epoch 95 ============================\n",
      "weight[9.96080428], bias[9.80479705]\n",
      "Train loss: 28.577224352014248 \n",
      "\n",
      "Epoch 96 ============================\n",
      "weight[9.95625568], bias[9.82786823]\n",
      "Train loss: 28.561604420112182 \n",
      "\n",
      "Epoch 97 ============================\n",
      "weight[9.95181026], bias[9.85041609]\n",
      "Train loss: 28.546685060190594 \n",
      "\n",
      "Epoch 98 ============================\n",
      "weight[9.94746568], bias[9.8724525]\n",
      "Train loss: 28.53243485078738 \n",
      "\n",
      "Epoch 99 ============================\n",
      "weight[9.94321964], bias[9.89398906]\n",
      "Train loss: 28.518823779729185 \n",
      "\n",
      "Epoch 100 ============================\n",
      "weight[9.93906991], bias[9.91503712]\n",
      "Train loss: 28.505823180923187 \n",
      "\n",
      "Epoch 101 ============================\n",
      "weight[9.93501431], bias[9.93560774]\n",
      "Train loss: 28.493405673984146 \n",
      "\n",
      "Epoch 102 ============================\n",
      "weight[9.93105071], bias[9.95571176]\n",
      "Train loss: 28.48154510656849 \n",
      "\n",
      "Epoch 103 ============================\n",
      "weight[9.92717701], bias[9.97535977]\n",
      "Train loss: 28.470216499295613 \n",
      "\n",
      "Epoch 104 ============================\n",
      "weight[9.92339117], bias[9.9945621]\n",
      "Train loss: 28.459395993138877 \n",
      "\n",
      "Epoch 105 ============================\n",
      "weight[9.91969121], bias[10.01332887]\n",
      "Train loss: 28.44906079917667 \n",
      "\n",
      "Epoch 106 ============================\n",
      "weight[9.91607518], bias[10.03166996]\n",
      "Train loss: 28.439189150596576 \n",
      "\n",
      "Epoch 107 ============================\n",
      "weight[9.91254117], bias[10.04959502]\n",
      "Train loss: 28.429760256853122 \n",
      "\n",
      "Epoch 108 ============================\n",
      "weight[9.90908731], bias[10.06711349]\n",
      "Train loss: 28.420754259880677 \n",
      "\n",
      "Epoch 109 ============================\n",
      "weight[9.90571181], bias[10.08423459]\n",
      "Train loss: 28.41215219227063 \n",
      "\n",
      "Epoch 110 ============================\n",
      "weight[9.90241286], bias[10.10096734]\n",
      "Train loss: 28.403935937324878 \n",
      "\n",
      "Epoch 111 ============================\n",
      "weight[9.89918875], bias[10.11732053]\n",
      "Train loss: 28.39608819089985 \n",
      "\n",
      "Epoch 112 ============================\n",
      "weight[9.89603777], bias[10.1333028]\n",
      "Train loss: 28.388592424963015 \n",
      "\n",
      "Epoch 113 ============================\n",
      "weight[9.89295826], bias[10.14892254]\n",
      "Train loss: 28.381432852783057 \n",
      "\n",
      "Epoch 114 ============================\n",
      "weight[9.88994861], bias[10.16418797]\n",
      "Train loss: 28.374594395681736 \n",
      "\n",
      "Epoch 115 ============================\n",
      "weight[9.88700722], bias[10.17910715]\n",
      "Train loss: 28.36806265127714 \n",
      "\n",
      "Epoch 116 ============================\n",
      "weight[9.88413255], bias[10.19368792]\n",
      "Train loss: 28.36182386315056 \n",
      "\n",
      "Epoch 117 ============================\n",
      "weight[9.88132309], bias[10.20793795]\n",
      "Train loss: 28.3558648918748 \n",
      "\n",
      "Epoch 118 ============================\n",
      "weight[9.87857735], bias[10.22186475]\n",
      "Train loss: 28.35017318734091 \n",
      "\n",
      "Epoch 119 ============================\n",
      "weight[9.8758939], bias[10.23547566]\n",
      "Train loss: 28.344736762327084 \n",
      "\n",
      "Epoch 120 ============================\n",
      "weight[9.87327131], bias[10.24877783]\n",
      "Train loss: 28.339544167251947 \n",
      "\n",
      "Epoch 121 ============================\n",
      "weight[9.87070821], bias[10.26177826]\n",
      "Train loss: 28.334584466061244 \n",
      "\n",
      "Epoch 122 ============================\n",
      "weight[9.86820325], bias[10.27448382]\n",
      "Train loss: 28.329847213195414 \n",
      "\n",
      "Epoch 123 ============================\n",
      "weight[9.86575511], bias[10.28690117]\n",
      "Train loss: 28.32532243159027 \n",
      "\n",
      "Epoch 124 ============================\n",
      "weight[9.86336249], bias[10.29903686]\n",
      "Train loss: 28.321000591664404 \n",
      "\n",
      "Epoch 125 ============================\n",
      "weight[9.86102415], bias[10.31089728]\n",
      "Train loss: 28.316872591249194 \n",
      "\n",
      "Epoch 126 ============================\n",
      "weight[9.85873885], bias[10.32248868]\n",
      "Train loss: 28.312929736418784 \n",
      "\n",
      "Epoch 127 ============================\n",
      "weight[9.85650539], bias[10.33381714]\n",
      "Train loss: 28.309163723179893 \n",
      "\n",
      "Epoch 128 ============================\n",
      "weight[9.85432259], bias[10.34488865]\n",
      "Train loss: 28.30556661998308 \n",
      "\n",
      "Epoch 129 ============================\n",
      "weight[9.8521893], bias[10.35570902]\n",
      "Train loss: 28.302130851017775 \n",
      "\n",
      "Epoch 130 ============================\n",
      "weight[9.8501044], bias[10.36628396]\n",
      "Train loss: 28.298849180257516 \n",
      "\n",
      "Epoch 131 ============================\n",
      "weight[9.84806679], bias[10.37661902]\n",
      "Train loss: 28.295714696219875 \n",
      "\n",
      "Epoch 132 ============================\n",
      "weight[9.8460754], bias[10.38671966]\n",
      "Train loss: 28.292720797410343 \n",
      "\n",
      "Epoch 133 ============================\n",
      "weight[9.84412918], bias[10.39659119]\n",
      "Train loss: 28.289861178419223 \n",
      "\n",
      "Epoch 134 ============================\n",
      "weight[9.84222711], bias[10.4062388]\n",
      "Train loss: 28.28712981664148 \n",
      "\n",
      "Epoch 135 ============================\n",
      "weight[9.84036818], bias[10.41566758]\n",
      "Train loss: 28.28452095959317 \n",
      "\n",
      "Epoch 136 ============================\n",
      "weight[9.83855142], bias[10.42488249]\n",
      "Train loss: 28.282029112795662 \n",
      "\n",
      "Epoch 137 ============================\n",
      "weight[9.83677586], bias[10.43388837]\n",
      "Train loss: 28.279649028204044 \n",
      "\n",
      "Epoch 138 ============================\n",
      "weight[9.83504058], bias[10.44268998]\n",
      "Train loss: 28.277375693154422 \n",
      "\n",
      "Epoch 139 ============================\n",
      "weight[9.83334467], bias[10.45129194]\n",
      "Train loss: 28.275204319806587 \n",
      "\n",
      "Epoch 140 ============================\n",
      "weight[9.83168721], bias[10.45969878]\n",
      "Train loss: 28.273130335060355 \n",
      "\n",
      "Epoch 141 ============================\n",
      "weight[9.83006736], bias[10.46791494]\n",
      "Train loss: 28.271149370924718 \n",
      "\n",
      "Epoch 142 ============================\n",
      "weight[9.82848425], bias[10.47594473]\n",
      "Train loss: 28.269257255317786 \n",
      "\n",
      "Epoch 143 ============================\n",
      "weight[9.82693705], bias[10.48379237]\n",
      "Train loss: 28.267450003280373 \n",
      "\n",
      "Epoch 144 ============================\n",
      "weight[9.82542494], bias[10.49146202]\n",
      "Train loss: 28.265723808583598 \n",
      "\n",
      "Epoch 145 ============================\n",
      "weight[9.82394713], bias[10.49895769]\n",
      "Train loss: 28.264075035711954 \n",
      "\n",
      "Epoch 146 ============================\n",
      "weight[9.82250284], bias[10.50628334]\n",
      "Train loss: 28.26250021220727 \n",
      "\n",
      "Epoch 147 ============================\n",
      "weight[9.82109132], bias[10.51344283]\n",
      "Train loss: 28.26099602135511 \n",
      "\n",
      "Epoch 148 ============================\n",
      "weight[9.81971181], bias[10.52043991]\n",
      "Train loss: 28.259559295199328 \n",
      "\n",
      "Epoch 149 ============================\n",
      "weight[9.81836359], bias[10.52727828]\n",
      "Train loss: 28.258187007870507 \n",
      "\n",
      "Epoch 150 ============================\n",
      "weight[9.81704595], bias[10.53396154]\n",
      "Train loss: 28.25687626921282 \n",
      "\n",
      "Epoch 151 ============================\n",
      "weight[9.8157582], bias[10.54049321]\n",
      "Train loss: 28.25562431869735 \n",
      "\n",
      "Epoch 152 ============================\n",
      "weight[9.81449966], bias[10.54687671]\n",
      "Train loss: 28.254428519607934 \n",
      "\n",
      "Epoch 153 ============================\n",
      "weight[9.81326967], bias[10.55311543]\n",
      "Train loss: 28.253286353488413 \n",
      "\n",
      "Epoch 154 ============================\n",
      "weight[9.81206758], bias[10.55921262]\n",
      "Train loss: 28.25219541483802 \n",
      "\n",
      "Epoch 155 ============================\n",
      "weight[9.81089275], bias[10.56517152]\n",
      "Train loss: 28.251153406045766 \n",
      "\n",
      "Epoch 156 ============================\n",
      "weight[9.80974457], bias[10.57099526]\n",
      "Train loss: 28.250158132550855 \n",
      "\n",
      "Epoch 157 ============================\n",
      "weight[9.80862244], bias[10.57668689]\n",
      "Train loss: 28.249207498221413 \n",
      "\n",
      "Epoch 158 ============================\n",
      "weight[9.80752576], bias[10.58224942]\n",
      "Train loss: 28.248299500939222 \n",
      "\n",
      "Epoch 159 ============================\n",
      "weight[9.80645396], bias[10.58768578]\n",
      "Train loss: 28.247432228383463 \n",
      "\n",
      "Epoch 160 ============================\n",
      "weight[9.80540646], bias[10.59299883]\n",
      "Train loss: 28.246603854003222 \n",
      "\n",
      "Epoch 161 ============================\n",
      "weight[9.80438273], bias[10.59819136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.245812633170384 \n",
      "\n",
      "Epoch 162 ============================\n",
      "weight[9.80338222], bias[10.60326611]\n",
      "Train loss: 28.245056899505396 \n",
      "\n",
      "Epoch 163 ============================\n",
      "weight[9.8024044], bias[10.60822575]\n",
      "Train loss: 28.244335061367888 \n",
      "\n",
      "Epoch 164 ============================\n",
      "weight[9.80144876], bias[10.61307289]\n",
      "Train loss: 28.24364559850426 \n",
      "\n",
      "Epoch 165 ============================\n",
      "weight[9.8005148], bias[10.61781008]\n",
      "Train loss: 28.242987058846154 \n",
      "\n",
      "Epoch 166 ============================\n",
      "weight[9.79960203], bias[10.62243982]\n",
      "Train loss: 28.242358055452037 \n",
      "\n",
      "Epoch 167 ============================\n",
      "weight[9.79870996], bias[10.62696455]\n",
      "Train loss: 28.241757263586383 \n",
      "\n",
      "Epoch 168 ============================\n",
      "weight[9.79783812], bias[10.63138664]\n",
      "Train loss: 28.24118341792959 \n",
      "\n",
      "Epoch 169 ============================\n",
      "weight[9.79698606], bias[10.63570843]\n",
      "Train loss: 28.24063530991304 \n",
      "\n",
      "Epoch 170 ============================\n",
      "weight[9.79615332], bias[10.63993218]\n",
      "Train loss: 28.24011178517389 \n",
      "\n",
      "Epoch 171 ============================\n",
      "weight[9.79533948], bias[10.64406013]\n",
      "Train loss: 28.23961174112375 \n",
      "\n",
      "Epoch 172 ============================\n",
      "weight[9.79454409], bias[10.64809445]\n",
      "Train loss: 28.239134124626595 \n",
      "\n",
      "Epoch 173 ============================\n",
      "weight[9.79376675], bias[10.65203726]\n",
      "Train loss: 28.238677929780785 \n",
      "\n",
      "Epoch 174 ============================\n",
      "weight[9.79300704], bias[10.65589063]\n",
      "Train loss: 28.238242195800588 \n",
      "\n",
      "Epoch 175 ============================\n",
      "weight[9.79226456], bias[10.65965659]\n",
      "Train loss: 28.237826004992435 \n",
      "\n",
      "Epoch 176 ============================\n",
      "weight[9.79153892], bias[10.66333714]\n",
      "Train loss: 28.23742848082259 \n",
      "\n",
      "Epoch 177 ============================\n",
      "weight[9.79082975], bias[10.6669342]\n",
      "Train loss: 28.23704878607074 \n",
      "\n",
      "Epoch 178 ============================\n",
      "weight[9.79013665], bias[10.67044966]\n",
      "Train loss: 28.236686121066924 \n",
      "\n",
      "Epoch 179 ============================\n",
      "weight[9.78945928], bias[10.67388539]\n",
      "Train loss: 28.236339722007262 \n",
      "\n",
      "Epoch 180 ============================\n",
      "weight[9.78879728], bias[10.67724319]\n",
      "Train loss: 28.23600885934534 \n",
      "\n",
      "Epoch 181 ============================\n",
      "weight[9.78815029], bias[10.68052482]\n",
      "Train loss: 28.235692836255854 \n",
      "\n",
      "Epoch 182 ============================\n",
      "weight[9.78751797], bias[10.68373201]\n",
      "Train loss: 28.23539098716677 \n",
      "\n",
      "Epoch 183 ============================\n",
      "weight[9.7869], bias[10.68686646]\n",
      "Train loss: 28.235102676357883 \n",
      "\n",
      "Epoch 184 ============================\n",
      "weight[9.78629605], bias[10.6899298]\n",
      "Train loss: 28.23482729662163 \n",
      "\n",
      "Epoch 185 ============================\n",
      "weight[9.7857058], bias[10.69292366]\n",
      "Train loss: 28.23456426798457 \n",
      "\n",
      "Epoch 186 ============================\n",
      "weight[9.78512893], bias[10.69584962]\n",
      "Train loss: 28.2343130364855 \n",
      "\n",
      "Epoch 187 ============================\n",
      "weight[9.78456515], bias[10.6987092]\n",
      "Train loss: 28.23407307300925 \n",
      "\n",
      "Epoch 188 ============================\n",
      "weight[9.78401416], bias[10.70150392]\n",
      "Train loss: 28.233843872171946 \n",
      "\n",
      "Epoch 189 ============================\n",
      "weight[9.78347566], bias[10.70423525]\n",
      "Train loss: 28.233624951256864 \n",
      "\n",
      "Epoch 190 ============================\n",
      "weight[9.78294938], bias[10.70690462]\n",
      "Train loss: 28.233415849197602 \n",
      "\n",
      "Epoch 191 ============================\n",
      "weight[9.78243504], bias[10.70951345]\n",
      "Train loss: 28.23321612560714 \n",
      "\n",
      "Epoch 192 ============================\n",
      "weight[9.78193236], bias[10.7120631]\n",
      "Train loss: 28.233025359850252 \n",
      "\n",
      "Epoch 193 ============================\n",
      "weight[9.78144109], bias[10.71455491]\n",
      "Train loss: 28.23284315015783 \n",
      "\n",
      "Epoch 194 ============================\n",
      "weight[9.78096096], bias[10.71699021]\n",
      "Train loss: 28.23266911278053 \n",
      "\n",
      "Epoch 195 ============================\n",
      "weight[9.78049172], bias[10.71937026]\n",
      "Train loss: 28.2325028811805 \n",
      "\n",
      "Epoch 196 ============================\n",
      "weight[9.78003312], bias[10.72169633]\n",
      "Train loss: 28.232344105259667 \n",
      "\n",
      "Epoch 197 ============================\n",
      "weight[9.77958493], bias[10.72396964]\n",
      "Train loss: 28.23219245062222 \n",
      "\n",
      "Epoch 198 ============================\n",
      "weight[9.7791469], bias[10.72619138]\n",
      "Train loss: 28.232047597870384 \n",
      "\n",
      "Epoch 199 ============================\n",
      "weight[9.77871881], bias[10.72836273]\n",
      "Train loss: 28.23190924193168 \n",
      "\n",
      "Epoch 200 ============================\n",
      "weight[9.77830043], bias[10.73048482]\n",
      "Train loss: 28.2317770914166 \n",
      "\n",
      "Epoch 201 ============================\n",
      "weight[9.77789154], bias[10.73255878]\n",
      "Train loss: 28.2316508680047 \n",
      "\n",
      "Epoch 202 ============================\n",
      "weight[9.77749192], bias[10.7345857]\n",
      "Train loss: 28.23153030585854 \n",
      "\n",
      "Epoch 203 ============================\n",
      "weight[9.77710137], bias[10.73656664]\n",
      "Train loss: 28.23141515106384 \n",
      "\n",
      "Epoch 204 ============================\n",
      "weight[9.77671968], bias[10.73850264]\n",
      "Train loss: 28.231305161094667 \n",
      "\n",
      "Epoch 205 ============================\n",
      "weight[9.77634664], bias[10.74039473]\n",
      "Train loss: 28.23120010430256 \n",
      "\n",
      "Epoch 206 ============================\n",
      "weight[9.77598207], bias[10.74224391]\n",
      "Train loss: 28.231099759428904 \n",
      "\n",
      "Epoch 207 ============================\n",
      "weight[9.77562577], bias[10.74405114]\n",
      "Train loss: 28.231003915138704 \n",
      "\n",
      "Epoch 208 ============================\n",
      "weight[9.77527754], bias[10.74581738]\n",
      "Train loss: 28.23091236957555 \n",
      "\n",
      "Epoch 209 ============================\n",
      "weight[9.77493722], bias[10.74754355]\n",
      "Train loss: 28.230824929936603 \n",
      "\n",
      "Epoch 210 ============================\n",
      "weight[9.77460462], bias[10.74923057]\n",
      "Train loss: 28.23074141206649 \n",
      "\n",
      "Epoch 211 ============================\n",
      "weight[9.77427956], bias[10.75087932]\n",
      "Train loss: 28.230661640069172 \n",
      "\n",
      "Epoch 212 ============================\n",
      "weight[9.77396187], bias[10.75249067]\n",
      "Train loss: 28.230585445938143 \n",
      "\n",
      "Epoch 213 ============================\n",
      "weight[9.77365139], bias[10.75406548]\n",
      "Train loss: 28.23051266920175 \n",
      "\n",
      "Epoch 214 ============================\n",
      "weight[9.77334795], bias[10.75560456]\n",
      "Train loss: 28.23044315658612 \n",
      "\n",
      "Epoch 215 ============================\n",
      "weight[9.7730514], bias[10.75710873]\n",
      "Train loss: 28.23037676169152 \n",
      "\n",
      "Epoch 216 ============================\n",
      "weight[9.77276157], bias[10.75857879]\n",
      "Train loss: 28.230313344684568 \n",
      "\n",
      "Epoch 217 ============================\n",
      "weight[9.77247831], bias[10.7600155]\n",
      "Train loss: 28.230252772003613 \n",
      "\n",
      "Epoch 218 ============================\n",
      "weight[9.77220149], bias[10.76141962]\n",
      "Train loss: 28.230194916077362 \n",
      "\n",
      "Epoch 219 ============================\n",
      "weight[9.77193094], bias[10.76279189]\n",
      "Train loss: 28.230139655056178 \n",
      "\n",
      "Epoch 220 ============================\n",
      "weight[9.77166652], bias[10.76413303]\n",
      "Train loss: 28.230086872555656 \n",
      "\n",
      "Epoch 221 ============================\n",
      "weight[9.77140811], bias[10.76544375]\n",
      "Train loss: 28.230036457411178 \n",
      "\n",
      "Epoch 222 ============================\n",
      "weight[9.77115555], bias[10.76672474]\n",
      "Train loss: 28.22998830344412 \n",
      "\n",
      "Epoch 223 ============================\n",
      "weight[9.77090873], bias[10.76797668]\n",
      "Train loss: 28.2299423092381 \n",
      "\n",
      "Epoch 224 ============================\n",
      "weight[9.7706675], bias[10.76920022]\n",
      "Train loss: 28.22989837792536 \n",
      "\n",
      "Epoch 225 ============================\n",
      "weight[9.77043175], bias[10.770396]\n",
      "Train loss: 28.229856416982688 \n",
      "\n",
      "Epoch 226 ============================\n",
      "weight[9.77020134], bias[10.77156466]\n",
      "Train loss: 28.229816338036716 \n",
      "\n",
      "Epoch 227 ============================\n",
      "weight[9.76997616], bias[10.77270681]\n",
      "Train loss: 28.229778056677755 \n",
      "\n",
      "Epoch 228 ============================\n",
      "weight[9.76975609], bias[10.77382306]\n",
      "Train loss: 28.229741492282006 \n",
      "\n",
      "Epoch 229 ============================\n",
      "weight[9.76954101], bias[10.77491398]\n",
      "Train loss: 28.22970656784158 \n",
      "\n",
      "Epoch 230 ============================\n",
      "weight[9.7693308], bias[10.77598016]\n",
      "Train loss: 28.229673209802577 \n",
      "\n",
      "Epoch 231 ============================\n",
      "weight[9.76912537], bias[10.77702216]\n",
      "Train loss: 28.229641347910214 \n",
      "\n",
      "Epoch 232 ============================\n",
      "weight[9.76892459], bias[10.77804052]\n",
      "Train loss: 28.22961091506047 \n",
      "\n",
      "Epoch 233 ============================\n",
      "weight[9.76872837], bias[10.77903578]\n",
      "Train loss: 28.229581847159235 \n",
      "\n",
      "Epoch 234 ============================\n",
      "weight[9.7685366], bias[10.78000847]\n",
      "Train loss: 28.229554082986862 \n",
      "\n",
      "Epoch 235 ============================\n",
      "weight[9.76834918], bias[10.78095909]\n",
      "Train loss: 28.22952756406972 \n",
      "\n",
      "Epoch 236 ============================\n",
      "weight[9.76816601], bias[10.78188815]\n",
      "Train loss: 28.22950223455663 \n",
      "\n",
      "Epoch 237 ============================\n",
      "weight[9.767987], bias[10.78279613]\n",
      "Train loss: 28.229478041101384 \n",
      "\n",
      "Epoch 238 ============================\n",
      "weight[9.76781205], bias[10.78368352]\n",
      "Train loss: 28.22945493275062 \n",
      "\n",
      "Epoch 239 ============================\n",
      "weight[9.76764106], bias[10.78455079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.229432860836003 \n",
      "\n",
      "Epoch 240 ============================\n",
      "weight[9.76747396], bias[10.78539838]\n",
      "Train loss: 28.229411778872265 \n",
      "\n",
      "Epoch 241 ============================\n",
      "weight[9.76731064], bias[10.78622674]\n",
      "Train loss: 28.229391642458975 \n",
      "\n",
      "Epoch 242 ============================\n",
      "weight[9.76715103], bias[10.78703632]\n",
      "Train loss: 28.229372409187025 \n",
      "\n",
      "Epoch 243 ============================\n",
      "weight[9.76699504], bias[10.78782753]\n",
      "Train loss: 28.229354038549552 \n",
      "\n",
      "Epoch 244 ============================\n",
      "weight[9.76684258], bias[10.78860079]\n",
      "Train loss: 28.229336491856404 \n",
      "\n",
      "Epoch 245 ============================\n",
      "weight[9.76669359], bias[10.78935651]\n",
      "Train loss: 28.22931973215268 \n",
      "\n",
      "Epoch 246 ============================\n",
      "weight[9.76654797], bias[10.7900951]\n",
      "Train loss: 28.229303724141012 \n",
      "\n",
      "Epoch 247 ============================\n",
      "weight[9.76640566], bias[10.79081693]\n",
      "Train loss: 28.229288434107144 \n",
      "\n",
      "Epoch 248 ============================\n",
      "weight[9.76626658], bias[10.79152238]\n",
      "Train loss: 28.22927382984897 \n",
      "\n",
      "Epoch 249 ============================\n",
      "weight[9.76613065], bias[10.79221184]\n",
      "Train loss: 28.229259880608588 \n",
      "\n",
      "Epoch 250 ============================\n",
      "weight[9.7659978], bias[10.79288565]\n",
      "Train loss: 28.22924655700779 \n",
      "\n",
      "Epoch 251 ============================\n",
      "weight[9.76586797], bias[10.79354418]\n",
      "Train loss: 28.229233830985887 \n",
      "\n",
      "Epoch 252 ============================\n",
      "weight[9.76574108], bias[10.79418778]\n",
      "Train loss: 28.229221675740718 \n",
      "\n",
      "Epoch 253 ============================\n",
      "weight[9.76561707], bias[10.79481677]\n",
      "Train loss: 28.22921006567233 \n",
      "\n",
      "Epoch 254 ============================\n",
      "weight[9.76549588], bias[10.7954315]\n",
      "Train loss: 28.229198976328934 \n",
      "\n",
      "Epoch 255 ============================\n",
      "weight[9.76537743], bias[10.79603229]\n",
      "Train loss: 28.229188384355414 \n",
      "\n",
      "Epoch 256 ============================\n",
      "weight[9.76526167], bias[10.79661944]\n",
      "Train loss: 28.22917826744412 \n",
      "\n",
      "Epoch 257 ============================\n",
      "weight[9.76514853], bias[10.79719328]\n",
      "Train loss: 28.229168604287967 \n",
      "\n",
      "Epoch 258 ============================\n",
      "weight[9.76503796], bias[10.7977541]\n",
      "Train loss: 28.22915937453553 \n",
      "\n",
      "Epoch 259 ============================\n",
      "weight[9.7649299], bias[10.79830221]\n",
      "Train loss: 28.22915055874818 \n",
      "\n",
      "Epoch 260 ============================\n",
      "weight[9.76482429], bias[10.79883787]\n",
      "Train loss: 28.229142138359013 \n",
      "\n",
      "Epoch 261 ============================\n",
      "weight[9.76472108], bias[10.79936139]\n",
      "Train loss: 28.229134095634 \n",
      "\n",
      "Epoch 262 ============================\n",
      "weight[9.76462021], bias[10.79987304]\n",
      "Train loss: 28.22912641363458 \n",
      "\n",
      "Epoch 263 ============================\n",
      "weight[9.76452162], bias[10.80037307]\n",
      "Train loss: 28.229119076181615 \n",
      "\n",
      "Epoch 264 ============================\n",
      "weight[9.76442527], bias[10.80086177]\n",
      "Train loss: 28.229112067821934 \n",
      "\n",
      "Epoch 265 ============================\n",
      "weight[9.76433111], bias[10.80133938]\n",
      "Train loss: 28.229105373795303 \n",
      "\n",
      "Epoch 266 ============================\n",
      "weight[9.76423908], bias[10.80180616]\n",
      "Train loss: 28.22909898000345 \n",
      "\n",
      "Epoch 267 ============================\n",
      "weight[9.76414914], bias[10.80226235]\n",
      "Train loss: 28.229092872980555 \n",
      "\n",
      "Epoch 268 ============================\n",
      "weight[9.76406124], bias[10.80270819]\n",
      "Train loss: 28.229087039864684 \n",
      "\n",
      "Epoch 269 ============================\n",
      "weight[9.76397534], bias[10.80314392]\n",
      "Train loss: 28.229081468370776 \n",
      "\n",
      "Epoch 270 ============================\n",
      "weight[9.76389138], bias[10.80356976]\n",
      "Train loss: 28.22907614676489 \n",
      "\n",
      "Epoch 271 ============================\n",
      "weight[9.76380933], bias[10.80398595]\n",
      "Train loss: 28.229071063839083 \n",
      "\n",
      "Epoch 272 ============================\n",
      "weight[9.76372913], bias[10.80439269]\n",
      "Train loss: 28.229066208888394 \n",
      "\n",
      "Epoch 273 ============================\n",
      "weight[9.76365076], bias[10.80479021]\n",
      "Train loss: 28.22906157168788 \n",
      "\n",
      "Epoch 274 ============================\n",
      "weight[9.76357417], bias[10.80517871]\n",
      "Train loss: 28.2290571424711 \n",
      "\n",
      "Epoch 275 ============================\n",
      "weight[9.76349931], bias[10.8055584]\n",
      "Train loss: 28.22905291190987 \n",
      "\n",
      "Epoch 276 ============================\n",
      "weight[9.76342615], bias[10.80592948]\n",
      "Train loss: 28.229048871094125 \n",
      "\n",
      "Epoch 277 ============================\n",
      "weight[9.76335465], bias[10.80629214]\n",
      "Train loss: 28.229045011513694 \n",
      "\n",
      "Epoch 278 ============================\n",
      "weight[9.76328477], bias[10.80664658]\n",
      "Train loss: 28.229041325039855 \n",
      "\n",
      "Epoch 279 ============================\n",
      "weight[9.76321648], bias[10.80699297]\n",
      "Train loss: 28.229037803908653 \n",
      "\n",
      "Epoch 280 ============================\n",
      "weight[9.76314973], bias[10.80733151]\n",
      "Train loss: 28.229034440704286 \n",
      "\n",
      "Epoch 281 ============================\n",
      "weight[9.7630845], bias[10.80766237]\n",
      "Train loss: 28.22903122834347 \n",
      "\n",
      "Epoch 282 ============================\n",
      "weight[9.76302075], bias[10.80798572]\n",
      "Train loss: 28.22902816006073 \n",
      "\n",
      "Epoch 283 ============================\n",
      "weight[9.76295845], bias[10.80830174]\n",
      "Train loss: 28.22902522939404 \n",
      "\n",
      "Epoch 284 ============================\n",
      "weight[9.76289756], bias[10.80861059]\n",
      "Train loss: 28.22902243017114 \n",
      "\n",
      "Epoch 285 ============================\n",
      "weight[9.76283804], bias[10.80891244]\n",
      "Train loss: 28.229019756496623 \n",
      "\n",
      "Epoch 286 ============================\n",
      "weight[9.76277988], bias[10.80920744]\n",
      "Train loss: 28.229017202739445 \n",
      "\n",
      "Epoch 287 ============================\n",
      "weight[9.76272304], bias[10.80949574]\n",
      "Train loss: 28.22901476352127 \n",
      "\n",
      "Epoch 288 ============================\n",
      "weight[9.76266749], bias[10.80977751]\n",
      "Train loss: 28.229012433704863 \n",
      "\n",
      "Epoch 289 ============================\n",
      "weight[9.7626132], bias[10.81005289]\n",
      "Train loss: 28.229010208383396 \n",
      "\n",
      "Epoch 290 ============================\n",
      "weight[9.76256014], bias[10.81032202]\n",
      "Train loss: 28.229008082870177 \n",
      "\n",
      "Epoch 291 ============================\n",
      "weight[9.76250828], bias[10.81058504]\n",
      "Train loss: 28.229006052688664 \n",
      "\n",
      "Epoch 292 ============================\n",
      "weight[9.7624576], bias[10.8108421]\n",
      "Train loss: 28.22900411356309 \n",
      "\n",
      "Epoch 293 ============================\n",
      "weight[9.76240807], bias[10.81109333]\n",
      "Train loss: 28.22900226140961 \n",
      "\n",
      "Epoch 294 ============================\n",
      "weight[9.76235966], bias[10.81133886]\n",
      "Train loss: 28.22900049232731 \n",
      "\n",
      "Epoch 295 ============================\n",
      "weight[9.76231235], bias[10.81157882]\n",
      "Train loss: 28.228998802590468 \n",
      "\n",
      "Epoch 296 ============================\n",
      "weight[9.76226612], bias[10.81181334]\n",
      "Train loss: 28.228997188640147 \n",
      "\n",
      "Epoch 297 ============================\n",
      "weight[9.76222093], bias[10.81204254]\n",
      "Train loss: 28.22899564707742 \n",
      "\n",
      "Epoch 298 ============================\n",
      "weight[9.76217677], bias[10.81226654]\n",
      "Train loss: 28.22899417465556 \n",
      "\n",
      "Epoch 299 ============================\n",
      "weight[9.76213361], bias[10.81248545]\n",
      "Train loss: 28.22899276827358 \n",
      "\n",
      "Epoch 300 ============================\n",
      "weight[9.76209143], bias[10.81269941]\n",
      "Train loss: 28.22899142496936 \n",
      "\n",
      "Epoch 301 ============================\n",
      "weight[9.7620502], bias[10.81290851]\n",
      "Train loss: 28.228990141913933 \n",
      "\n",
      "Epoch 302 ============================\n",
      "weight[9.76200991], bias[10.81311286]\n",
      "Train loss: 28.228988916405008 \n",
      "\n",
      "Epoch 303 ============================\n",
      "weight[9.76197053], bias[10.81331258]\n",
      "Train loss: 28.228987745861616 \n",
      "\n",
      "Epoch 304 ============================\n",
      "weight[9.76193205], bias[10.81350777]\n",
      "Train loss: 28.22898662781839 \n",
      "\n",
      "Epoch 305 ============================\n",
      "weight[9.76189444], bias[10.81369854]\n",
      "Train loss: 28.228985559920744 \n",
      "\n",
      "Epoch 306 ============================\n",
      "weight[9.76185769], bias[10.81388497]\n",
      "Train loss: 28.228984539919516 \n",
      "\n",
      "Epoch 307 ============================\n",
      "weight[9.76182176], bias[10.81406718]\n",
      "Train loss: 28.228983565666546 \n",
      "\n",
      "Epoch 308 ============================\n",
      "weight[9.76178665], bias[10.81424526]\n",
      "Train loss: 28.228982635109926 \n",
      "\n",
      "Epoch 309 ============================\n",
      "weight[9.76175234], bias[10.81441929]\n",
      "Train loss: 28.228981746289794 \n",
      "\n",
      "Epoch 310 ============================\n",
      "weight[9.76171881], bias[10.81458938]\n",
      "Train loss: 28.22898089733433 \n",
      "\n",
      "Epoch 311 ============================\n",
      "weight[9.76168604], bias[10.81475561]\n",
      "Train loss: 28.228980086455522 \n",
      "\n",
      "Epoch 312 ============================\n",
      "weight[9.76165401], bias[10.81491807]\n",
      "Train loss: 28.228979311945494 \n",
      "\n",
      "Epoch 313 ============================\n",
      "weight[9.7616227], bias[10.81507684]\n",
      "Train loss: 28.2289785721732 \n",
      "\n",
      "Epoch 314 ============================\n",
      "weight[9.76159211], bias[10.81523201]\n",
      "Train loss: 28.228977865580507 \n",
      "\n",
      "Epoch 315 ============================\n",
      "weight[9.76156221], bias[10.81538367]\n",
      "Train loss: 28.228977190679327 \n",
      "\n",
      "Epoch 316 ============================\n",
      "weight[9.76153299], bias[10.81553188]\n",
      "Train loss: 28.228976546048223 \n",
      "\n",
      "Epoch 317 ============================\n",
      "weight[9.76150443], bias[10.81567673]\n",
      "Train loss: 28.228975930329586 \n",
      "\n",
      "Epoch 318 ============================\n",
      "weight[9.76147652], bias[10.8158183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228975342226644 \n",
      "\n",
      "Epoch 319 ============================\n",
      "weight[9.76144924], bias[10.81595665]\n",
      "Train loss: 28.228974780500785 \n",
      "\n",
      "Epoch 320 ============================\n",
      "weight[9.76142259], bias[10.81609187]\n",
      "Train loss: 28.228974243969 \n",
      "\n",
      "Epoch 321 ============================\n",
      "weight[9.76139653], bias[10.81622401]\n",
      "Train loss: 28.2289737315013 \n",
      "\n",
      "Epoch 322 ============================\n",
      "weight[9.76137107], bias[10.81635317]\n",
      "Train loss: 28.228973242018306 \n",
      "\n",
      "Epoch 323 ============================\n",
      "weight[9.76134618], bias[10.81647939]\n",
      "Train loss: 28.2289727744892 \n",
      "\n",
      "Epoch 324 ============================\n",
      "weight[9.76132186], bias[10.81660275]\n",
      "Train loss: 28.22897232792934 \n",
      "\n",
      "Epoch 325 ============================\n",
      "weight[9.76129809], bias[10.81672331]\n",
      "Train loss: 28.228971901398246 \n",
      "\n",
      "Epoch 326 ============================\n",
      "weight[9.76127486], bias[10.81684113]\n",
      "Train loss: 28.22897149399748 \n",
      "\n",
      "Epoch 327 ============================\n",
      "weight[9.76125216], bias[10.81695629]\n",
      "Train loss: 28.228971104869206 \n",
      "\n",
      "Epoch 328 ============================\n",
      "weight[9.76122997], bias[10.81706883]\n",
      "Train loss: 28.22897073319373 \n",
      "\n",
      "Epoch 329 ============================\n",
      "weight[9.76120829], bias[10.81717882]\n",
      "Train loss: 28.228970378188315 \n",
      "\n",
      "Epoch 330 ============================\n",
      "weight[9.76118709], bias[10.81728631]\n",
      "Train loss: 28.22897003910528 \n",
      "\n",
      "Epoch 331 ============================\n",
      "weight[9.76116638], bias[10.81739137]\n",
      "Train loss: 28.22896971523057 \n",
      "\n",
      "Epoch 332 ============================\n",
      "weight[9.76114614], bias[10.81749404]\n",
      "Train loss: 28.228969405882 \n",
      "\n",
      "Epoch 333 ============================\n",
      "weight[9.76112636], bias[10.81759438]\n",
      "Train loss: 28.228969110408023 \n",
      "\n",
      "Epoch 334 ============================\n",
      "weight[9.76110702], bias[10.81769245]\n",
      "Train loss: 28.228968828186485 \n",
      "\n",
      "Epoch 335 ============================\n",
      "weight[9.76108813], bias[10.81778829]\n",
      "Train loss: 28.228968558622835 \n",
      "\n",
      "Epoch 336 ============================\n",
      "weight[9.76106966], bias[10.81788196]\n",
      "Train loss: 28.22896830114943 \n",
      "\n",
      "Epoch 337 ============================\n",
      "weight[9.76105161], bias[10.81797351]\n",
      "Train loss: 28.228968055224048 \n",
      "\n",
      "Epoch 338 ============================\n",
      "weight[9.76103397], bias[10.81806297]\n",
      "Train loss: 28.22896782032866 \n",
      "\n",
      "Epoch 339 ============================\n",
      "weight[9.76101673], bias[10.81815041]\n",
      "Train loss: 28.228967595968633 \n",
      "\n",
      "Epoch 340 ============================\n",
      "weight[9.76099989], bias[10.81823587]\n",
      "Train loss: 28.22896738167142 \n",
      "\n",
      "Epoch 341 ============================\n",
      "weight[9.76098342], bias[10.81831938]\n",
      "Train loss: 28.22896717698567 \n",
      "\n",
      "Epoch 342 ============================\n",
      "weight[9.76096733], bias[10.81840101]\n",
      "Train loss: 28.228966981480283 \n",
      "\n",
      "Epoch 343 ============================\n",
      "weight[9.7609516], bias[10.81848078]\n",
      "Train loss: 28.228966794743542 \n",
      "\n",
      "Epoch 344 ============================\n",
      "weight[9.76093623], bias[10.81855874]\n",
      "Train loss: 28.228966616382188 \n",
      "\n",
      "Epoch 345 ============================\n",
      "weight[9.76092121], bias[10.81863493]\n",
      "Train loss: 28.228966446020568 \n",
      "\n",
      "Epoch 346 ============================\n",
      "weight[9.76090653], bias[10.8187094]\n",
      "Train loss: 28.22896628329982 \n",
      "\n",
      "Epoch 347 ============================\n",
      "weight[9.76089218], bias[10.81878217]\n",
      "Train loss: 28.228966127877282 \n",
      "\n",
      "Epoch 348 ============================\n",
      "weight[9.76087816], bias[10.8188533]\n",
      "Train loss: 28.228965979425645 \n",
      "\n",
      "Epoch 349 ============================\n",
      "weight[9.76086445], bias[10.81892281]\n",
      "Train loss: 28.228965837632256 \n",
      "\n",
      "Epoch 350 ============================\n",
      "weight[9.76085106], bias[10.81899075]\n",
      "Train loss: 28.228965702198384 \n",
      "\n",
      "Epoch 351 ============================\n",
      "weight[9.76083797], bias[10.81905714]\n",
      "Train loss: 28.228965572838963 \n",
      "\n",
      "Epoch 352 ============================\n",
      "weight[9.76082517], bias[10.81912203]\n",
      "Train loss: 28.228965449281436 \n",
      "\n",
      "Epoch 353 ============================\n",
      "weight[9.76081267], bias[10.81918544]\n",
      "Train loss: 28.228965331265574 \n",
      "\n",
      "Epoch 354 ============================\n",
      "weight[9.76080045], bias[10.81924742]\n",
      "Train loss: 28.22896521854288 \n",
      "\n",
      "Epoch 355 ============================\n",
      "weight[9.76078851], bias[10.81930799]\n",
      "Train loss: 28.228965110875958 \n",
      "\n",
      "Epoch 356 ============================\n",
      "weight[9.76077684], bias[10.81936719]\n",
      "Train loss: 28.228965008037985 \n",
      "\n",
      "Epoch 357 ============================\n",
      "weight[9.76076543], bias[10.81942505]\n",
      "Train loss: 28.228964909812447 \n",
      "\n",
      "Epoch 358 ============================\n",
      "weight[9.76075428], bias[10.81948159]\n",
      "Train loss: 28.228964815992423 \n",
      "\n",
      "Epoch 359 ============================\n",
      "weight[9.76074339], bias[10.81953685]\n",
      "Train loss: 28.228964726380323 \n",
      "\n",
      "Epoch 360 ============================\n",
      "weight[9.76073274], bias[10.81959086]\n",
      "Train loss: 28.228964640787435 \n",
      "\n",
      "Epoch 361 ============================\n",
      "weight[9.76072234], bias[10.81964364]\n",
      "Train loss: 28.2289645590335 \n",
      "\n",
      "Epoch 362 ============================\n",
      "weight[9.76071217], bias[10.81969522]\n",
      "Train loss: 28.22896448094631 \n",
      "\n",
      "Epoch 363 ============================\n",
      "weight[9.76070223], bias[10.81974564]\n",
      "Train loss: 28.228964406361438 \n",
      "\n",
      "Epoch 364 ============================\n",
      "weight[9.76069251], bias[10.81979491]\n",
      "Train loss: 28.22896433512172 \n",
      "\n",
      "Epoch 365 ============================\n",
      "weight[9.76068302], bias[10.81984306]\n",
      "Train loss: 28.22896426707726 \n",
      "\n",
      "Epoch 366 ============================\n",
      "weight[9.76067374], bias[10.81989012]\n",
      "Train loss: 28.228964202084665 \n",
      "\n",
      "Epoch 367 ============================\n",
      "weight[9.76066467], bias[10.81993612]\n",
      "Train loss: 28.22896414000702 \n",
      "\n",
      "Epoch 368 ============================\n",
      "weight[9.76065581], bias[10.81998107]\n",
      "Train loss: 28.228964080713713 \n",
      "\n",
      "Epoch 369 ============================\n",
      "weight[9.76064715], bias[10.820025]\n",
      "Train loss: 28.22896402407965 \n",
      "\n",
      "Epoch 370 ============================\n",
      "weight[9.76063868], bias[10.82006793]\n",
      "Train loss: 28.22896396998581 \n",
      "\n",
      "Epoch 371 ============================\n",
      "weight[9.76063041], bias[10.82010989]\n",
      "Train loss: 28.228963918318065 \n",
      "\n",
      "Epoch 372 ============================\n",
      "weight[9.76062233], bias[10.8201509]\n",
      "Train loss: 28.22896386896768 \n",
      "\n",
      "Epoch 373 ============================\n",
      "weight[9.76061443], bias[10.82019098]\n",
      "Train loss: 28.228963821830778 \n",
      "\n",
      "Epoch 374 ============================\n",
      "weight[9.7606067], bias[10.82023015]\n",
      "Train loss: 28.228963776808005 \n",
      "\n",
      "Epoch 375 ============================\n",
      "weight[9.76059916], bias[10.82026843]\n",
      "Train loss: 28.228963733804537 \n",
      "\n",
      "Epoch 376 ============================\n",
      "weight[9.76059178], bias[10.82030584]\n",
      "Train loss: 28.22896369272981 \n",
      "\n",
      "Epoch 377 ============================\n",
      "weight[9.76058457], bias[10.82034241]\n",
      "Train loss: 28.2289636534973 \n",
      "\n",
      "Epoch 378 ============================\n",
      "weight[9.76057753], bias[10.82037814]\n",
      "Train loss: 28.228963616024487 \n",
      "\n",
      "Epoch 379 ============================\n",
      "weight[9.76057064], bias[10.82041307]\n",
      "Train loss: 28.22896358023231 \n",
      "\n",
      "Epoch 380 ============================\n",
      "weight[9.76056391], bias[10.8204472]\n",
      "Train loss: 28.22896354604552 \n",
      "\n",
      "Epoch 381 ============================\n",
      "weight[9.76055733], bias[10.82048055]\n",
      "Train loss: 28.22896351339204 \n",
      "\n",
      "Epoch 382 ============================\n",
      "weight[9.76055091], bias[10.82051316]\n",
      "Train loss: 28.22896348220305 \n",
      "\n",
      "Epoch 383 ============================\n",
      "weight[9.76054463], bias[10.82054502]\n",
      "Train loss: 28.22896345241296 \n",
      "\n",
      "Epoch 384 ============================\n",
      "weight[9.76053849], bias[10.82057616]\n",
      "Train loss: 28.22896342395899 \n",
      "\n",
      "Epoch 385 ============================\n",
      "weight[9.76053249], bias[10.82060659]\n",
      "Train loss: 28.22896339678121 \n",
      "\n",
      "Epoch 386 ============================\n",
      "weight[9.76052662], bias[10.82063633]\n",
      "Train loss: 28.228963370822346 \n",
      "\n",
      "Epoch 387 ============================\n",
      "weight[9.76052089], bias[10.8206654]\n",
      "Train loss: 28.22896334602782 \n",
      "\n",
      "Epoch 388 ============================\n",
      "weight[9.76051529], bias[10.82069381]\n",
      "Train loss: 28.22896332234531 \n",
      "\n",
      "Epoch 389 ============================\n",
      "weight[9.76050982], bias[10.82072157]\n",
      "Train loss: 28.22896329972502 \n",
      "\n",
      "Epoch 390 ============================\n",
      "weight[9.76050447], bias[10.8207487]\n",
      "Train loss: 28.228963278119263 \n",
      "\n",
      "Epoch 391 ============================\n",
      "weight[9.76049924], bias[10.82077522]\n",
      "Train loss: 28.228963257482594 \n",
      "\n",
      "Epoch 392 ============================\n",
      "weight[9.76049413], bias[10.82080114]\n",
      "Train loss: 28.228963237771502 \n",
      "\n",
      "Epoch 393 ============================\n",
      "weight[9.76048914], bias[10.82082647]\n",
      "Train loss: 28.228963218944415 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394 ============================\n",
      "weight[9.76048425], bias[10.82085122]\n",
      "Train loss: 28.228963200961783 \n",
      "\n",
      "Epoch 395 ============================\n",
      "weight[9.76047949], bias[10.82087542]\n",
      "Train loss: 28.228963183785677 \n",
      "\n",
      "Epoch 396 ============================\n",
      "weight[9.76047482], bias[10.82089906]\n",
      "Train loss: 28.228963167379902 \n",
      "\n",
      "Epoch 397 ============================\n",
      "weight[9.76047027], bias[10.82092217]\n",
      "Train loss: 28.228963151709983 \n",
      "\n",
      "Epoch 398 ============================\n",
      "weight[9.76046582], bias[10.82094475]\n",
      "Train loss: 28.2289631367429 \n",
      "\n",
      "Epoch 399 ============================\n",
      "weight[9.76046146], bias[10.82096682]\n",
      "Train loss: 28.228963122447066 \n",
      "\n",
      "Epoch 400 ============================\n",
      "weight[9.76045721], bias[10.8209884]\n",
      "Train loss: 28.22896310879243 \n",
      "\n",
      "Epoch 401 ============================\n",
      "weight[9.76045305], bias[10.82100948]\n",
      "Train loss: 28.228963095750267 \n",
      "\n",
      "Epoch 402 ============================\n",
      "weight[9.76044899], bias[10.82103008]\n",
      "Train loss: 28.22896308329303 \n",
      "\n",
      "Epoch 403 ============================\n",
      "weight[9.76044502], bias[10.82105022]\n",
      "Train loss: 28.228963071394485 \n",
      "\n",
      "Epoch 404 ============================\n",
      "weight[9.76044114], bias[10.8210699]\n",
      "Train loss: 28.228963060029646 \n",
      "\n",
      "Epoch 405 ============================\n",
      "weight[9.76043735], bias[10.82108913]\n",
      "Train loss: 28.228963049174492 \n",
      "\n",
      "Epoch 406 ============================\n",
      "weight[9.76043364], bias[10.82110793]\n",
      "Train loss: 28.228963038806253 \n",
      "\n",
      "Epoch 407 ============================\n",
      "weight[9.76043002], bias[10.8211263]\n",
      "Train loss: 28.228963028903 \n",
      "\n",
      "Epoch 408 ============================\n",
      "weight[9.76042648], bias[10.82114425]\n",
      "Train loss: 28.22896301944392 \n",
      "\n",
      "Epoch 409 ============================\n",
      "weight[9.76042302], bias[10.8211618]\n",
      "Train loss: 28.22896301040908 \n",
      "\n",
      "Epoch 410 ============================\n",
      "weight[9.76041964], bias[10.82117895]\n",
      "Train loss: 28.228963001779537 \n",
      "\n",
      "Epoch 411 ============================\n",
      "weight[9.76041634], bias[10.82119571]\n",
      "Train loss: 28.228962993537 \n",
      "\n",
      "Epoch 412 ============================\n",
      "weight[9.76041311], bias[10.82121208]\n",
      "Train loss: 28.2289629856641 \n",
      "\n",
      "Epoch 413 ============================\n",
      "weight[9.76040995], bias[10.82122809]\n",
      "Train loss: 28.22896297814438 \n",
      "\n",
      "Epoch 414 ============================\n",
      "weight[9.76040687], bias[10.82124374]\n",
      "Train loss: 28.22896297096189 \n",
      "\n",
      "Epoch 415 ============================\n",
      "weight[9.76040385], bias[10.82125903]\n",
      "Train loss: 28.228962964101544 \n",
      "\n",
      "Epoch 416 ============================\n",
      "weight[9.76040091], bias[10.82127397]\n",
      "Train loss: 28.22896295754891 \n",
      "\n",
      "Epoch 417 ============================\n",
      "weight[9.76039803], bias[10.82128857]\n",
      "Train loss: 28.228962951290157 \n",
      "\n",
      "Epoch 418 ============================\n",
      "weight[9.76039522], bias[10.82130285]\n",
      "Train loss: 28.228962945312112 \n",
      "\n",
      "Epoch 419 ============================\n",
      "weight[9.76039247], bias[10.8213168]\n",
      "Train loss: 28.22896293960219 \n",
      "\n",
      "Epoch 420 ============================\n",
      "weight[9.76038978], bias[10.82133043]\n",
      "Train loss: 28.228962934148374 \n",
      "\n",
      "Epoch 421 ============================\n",
      "weight[9.76038715], bias[10.82134375]\n",
      "Train loss: 28.22896292893913 \n",
      "\n",
      "Epoch 422 ============================\n",
      "weight[9.76038458], bias[10.82135677]\n",
      "Train loss: 28.228962923963554 \n",
      "\n",
      "Epoch 423 ============================\n",
      "weight[9.76038207], bias[10.8213695]\n",
      "Train loss: 28.228962919211153 \n",
      "\n",
      "Epoch 424 ============================\n",
      "weight[9.76037962], bias[10.82138194]\n",
      "Train loss: 28.228962914671886 \n",
      "\n",
      "Epoch 425 ============================\n",
      "weight[9.76037723], bias[10.82139409]\n",
      "Train loss: 28.228962910336236 \n",
      "\n",
      "Epoch 426 ============================\n",
      "weight[9.76037488], bias[10.82140597]\n",
      "Train loss: 28.22896290619503 \n",
      "\n",
      "Epoch 427 ============================\n",
      "weight[9.76037259], bias[10.82141758]\n",
      "Train loss: 28.228962902239566 \n",
      "\n",
      "Epoch 428 ============================\n",
      "weight[9.76037036], bias[10.82142893]\n",
      "Train loss: 28.228962898461447 \n",
      "\n",
      "Epoch 429 ============================\n",
      "weight[9.76036817], bias[10.82144002]\n",
      "Train loss: 28.228962894852884 \n",
      "\n",
      "Epoch 430 ============================\n",
      "weight[9.76036603], bias[10.82145085]\n",
      "Train loss: 28.22896289140612 \n",
      "\n",
      "Epoch 431 ============================\n",
      "weight[9.76036395], bias[10.82146145]\n",
      "Train loss: 28.22896288811393 \n",
      "\n",
      "Epoch 432 ============================\n",
      "weight[9.76036191], bias[10.8214718]\n",
      "Train loss: 28.22896288496943 \n",
      "\n",
      "Epoch 433 ============================\n",
      "weight[9.76035991], bias[10.82148191]\n",
      "Train loss: 28.228962881965913 \n",
      "\n",
      "Epoch 434 ============================\n",
      "weight[9.76035796], bias[10.8214918]\n",
      "Train loss: 28.228962879097164 \n",
      "\n",
      "Epoch 435 ============================\n",
      "weight[9.76035606], bias[10.82150146]\n",
      "Train loss: 28.22896287635704 \n",
      "\n",
      "Epoch 436 ============================\n",
      "weight[9.76035419], bias[10.82151091]\n",
      "Train loss: 28.228962873739825 \n",
      "\n",
      "Epoch 437 ============================\n",
      "weight[9.76035238], bias[10.82152014]\n",
      "Train loss: 28.228962871239997 \n",
      "\n",
      "Epoch 438 ============================\n",
      "weight[9.7603506], bias[10.82152916]\n",
      "Train loss: 28.228962868852303 \n",
      "\n",
      "Epoch 439 ============================\n",
      "weight[9.76034886], bias[10.82153797]\n",
      "Train loss: 28.22896286657172 \n",
      "\n",
      "Epoch 440 ============================\n",
      "weight[9.76034716], bias[10.82154659]\n",
      "Train loss: 28.22896286439342 \n",
      "\n",
      "Epoch 441 ============================\n",
      "weight[9.7603455], bias[10.82155501]\n",
      "Train loss: 28.228962862312766 \n",
      "\n",
      "Epoch 442 ============================\n",
      "weight[9.76034388], bias[10.82156324]\n",
      "Train loss: 28.22896286032549 \n",
      "\n",
      "Epoch 443 ============================\n",
      "weight[9.76034229], bias[10.82157128]\n",
      "Train loss: 28.22896285842727 \n",
      "\n",
      "Epoch 444 ============================\n",
      "weight[9.76034074], bias[10.82157914]\n",
      "Train loss: 28.228962856614242 \n",
      "\n",
      "Epoch 445 ============================\n",
      "weight[9.76033923], bias[10.82158682]\n",
      "Train loss: 28.228962854882557 \n",
      "\n",
      "Epoch 446 ============================\n",
      "weight[9.76033775], bias[10.82159433]\n",
      "Train loss: 28.22896285322852 \n",
      "\n",
      "Epoch 447 ============================\n",
      "weight[9.7603363], bias[10.82160167]\n",
      "Train loss: 28.228962851648635 \n",
      "\n",
      "Epoch 448 ============================\n",
      "weight[9.76033489], bias[10.82160884]\n",
      "Train loss: 28.228962850139673 \n",
      "\n",
      "Epoch 449 ============================\n",
      "weight[9.76033351], bias[10.82161585]\n",
      "Train loss: 28.228962848698295 \n",
      "\n",
      "Epoch 450 ============================\n",
      "weight[9.76033216], bias[10.8216227]\n",
      "Train loss: 28.228962847321604 \n",
      "\n",
      "Epoch 451 ============================\n",
      "weight[9.76033084], bias[10.82162939]\n",
      "Train loss: 28.228962846006688 \n",
      "\n",
      "Epoch 452 ============================\n",
      "weight[9.76032955], bias[10.82163593]\n",
      "Train loss: 28.228962844750754 \n",
      "\n",
      "Epoch 453 ============================\n",
      "weight[9.76032828], bias[10.82164233]\n",
      "Train loss: 28.228962843551123 \n",
      "\n",
      "Epoch 454 ============================\n",
      "weight[9.76032705], bias[10.82164858]\n",
      "Train loss: 28.228962842405313 \n",
      "\n",
      "Epoch 455 ============================\n",
      "weight[9.76032585], bias[10.82165468]\n",
      "Train loss: 28.228962841310835 \n",
      "\n",
      "Epoch 456 ============================\n",
      "weight[9.76032467], bias[10.82166065]\n",
      "Train loss: 28.228962840265535 \n",
      "\n",
      "Epoch 457 ============================\n",
      "weight[9.76032352], bias[10.82166648]\n",
      "Train loss: 28.22896283926705 \n",
      "\n",
      "Epoch 458 ============================\n",
      "weight[9.7603224], bias[10.82167219]\n",
      "Train loss: 28.228962838313443 \n",
      "\n",
      "Epoch 459 ============================\n",
      "weight[9.7603213], bias[10.82167776]\n",
      "Train loss: 28.228962837402484 \n",
      "\n",
      "Epoch 460 ============================\n",
      "weight[9.76032023], bias[10.8216832]\n",
      "Train loss: 28.22896283653242 \n",
      "\n",
      "Epoch 461 ============================\n",
      "weight[9.76031918], bias[10.82168852]\n",
      "Train loss: 28.228962835701427 \n",
      "\n",
      "Epoch 462 ============================\n",
      "weight[9.76031815], bias[10.82169372]\n",
      "Train loss: 28.22896283490766 \n",
      "\n",
      "Epoch 463 ============================\n",
      "weight[9.76031715], bias[10.82169881]\n",
      "Train loss: 28.228962834149492 \n",
      "\n",
      "Epoch 464 ============================\n",
      "weight[9.76031617], bias[10.82170377]\n",
      "Train loss: 28.22896283342536 \n",
      "\n",
      "Epoch 465 ============================\n",
      "weight[9.76031521], bias[10.82170863]\n",
      "Train loss: 28.22896283273365 \n",
      "\n",
      "Epoch 466 ============================\n",
      "weight[9.76031428], bias[10.82171337]\n",
      "Train loss: 28.22896283207303 \n",
      "\n",
      "Epoch 467 ============================\n",
      "weight[9.76031336], bias[10.82171801]\n",
      "Train loss: 28.22896283144206 \n",
      "\n",
      "Epoch 468 ============================\n",
      "weight[9.76031247], bias[10.82172254]\n",
      "Train loss: 28.2289628308393 \n",
      "\n",
      "Epoch 469 ============================\n",
      "weight[9.7603116], bias[10.82172697]\n",
      "Train loss: 28.22896283026362 \n",
      "\n",
      "Epoch 470 ============================\n",
      "weight[9.76031074], bias[10.8217313]\n",
      "Train loss: 28.228962829713748 \n",
      "\n",
      "Epoch 471 ============================\n",
      "weight[9.76030991], bias[10.82173553]\n",
      "Train loss: 28.228962829188557 \n",
      "\n",
      "Epoch 472 ============================\n",
      "weight[9.76030909], bias[10.82173967]\n",
      "Train loss: 28.228962828686917 \n",
      "\n",
      "Epoch 473 ============================\n",
      "weight[9.7603083], bias[10.82174371]\n",
      "Train loss: 28.228962828207738 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474 ============================\n",
      "weight[9.76030752], bias[10.82174766]\n",
      "Train loss: 28.228962827750113 \n",
      "\n",
      "Epoch 475 ============================\n",
      "weight[9.76030676], bias[10.82175152]\n",
      "Train loss: 28.228962827312962 \n",
      "\n",
      "Epoch 476 ============================\n",
      "weight[9.76030601], bias[10.82175529]\n",
      "Train loss: 28.228962826895465 \n",
      "\n",
      "Epoch 477 ============================\n",
      "weight[9.76030529], bias[10.82175897]\n",
      "Train loss: 28.228962826496694 \n",
      "\n",
      "Epoch 478 ============================\n",
      "weight[9.76030458], bias[10.82176258]\n",
      "Train loss: 28.228962826115765 \n",
      "\n",
      "Epoch 479 ============================\n",
      "weight[9.76030388], bias[10.8217661]\n",
      "Train loss: 28.22896282575187 \n",
      "\n",
      "Epoch 480 ============================\n",
      "weight[9.7603032], bias[10.82176954]\n",
      "Train loss: 28.22896282540444 \n",
      "\n",
      "Epoch 481 ============================\n",
      "weight[9.76030254], bias[10.8217729]\n",
      "Train loss: 28.228962825072553 \n",
      "\n",
      "Epoch 482 ============================\n",
      "weight[9.76030189], bias[10.82177619]\n",
      "Train loss: 28.228962824755474 \n",
      "\n",
      "Epoch 483 ============================\n",
      "weight[9.76030126], bias[10.8217794]\n",
      "Train loss: 28.22896282445268 \n",
      "\n",
      "Epoch 484 ============================\n",
      "weight[9.76030064], bias[10.82178254]\n",
      "Train loss: 28.22896282416342 \n",
      "\n",
      "Epoch 485 ============================\n",
      "weight[9.76030004], bias[10.82178561]\n",
      "Train loss: 28.22896282388717 \n",
      "\n",
      "Epoch 486 ============================\n",
      "weight[9.76029945], bias[10.82178861]\n",
      "Train loss: 28.228962823623313 \n",
      "\n",
      "Epoch 487 ============================\n",
      "weight[9.76029887], bias[10.82179154]\n",
      "Train loss: 28.228962823371262 \n",
      "\n",
      "Epoch 488 ============================\n",
      "weight[9.7602983], bias[10.8217944]\n",
      "Train loss: 28.228962823130505 \n",
      "\n",
      "Epoch 489 ============================\n",
      "weight[9.76029775], bias[10.8217972]\n",
      "Train loss: 28.22896282290062 \n",
      "\n",
      "Epoch 490 ============================\n",
      "weight[9.76029721], bias[10.82179994]\n",
      "Train loss: 28.228962822680973 \n",
      "\n",
      "Epoch 491 ============================\n",
      "weight[9.76029668], bias[10.82180261]\n",
      "Train loss: 28.228962822471196 \n",
      "\n",
      "Epoch 492 ============================\n",
      "weight[9.76029617], bias[10.82180522]\n",
      "Train loss: 28.228962822270823 \n",
      "\n",
      "Epoch 493 ============================\n",
      "weight[9.76029567], bias[10.82180778]\n",
      "Train loss: 28.22896282207945 \n",
      "\n",
      "Epoch 494 ============================\n",
      "weight[9.76029517], bias[10.82181027]\n",
      "Train loss: 28.228962821896662 \n",
      "\n",
      "Epoch 495 ============================\n",
      "weight[9.76029469], bias[10.82181271]\n",
      "Train loss: 28.228962821722053 \n",
      "\n",
      "Epoch 496 ============================\n",
      "weight[9.76029422], bias[10.8218151]\n",
      "Train loss: 28.228962821555324 \n",
      "\n",
      "Epoch 497 ============================\n",
      "weight[9.76029376], bias[10.82181743]\n",
      "Train loss: 28.228962821395992 \n",
      "\n",
      "Epoch 498 ============================\n",
      "weight[9.76029331], bias[10.8218197]\n",
      "Train loss: 28.228962821243904 \n",
      "\n",
      "Epoch 499 ============================\n",
      "weight[9.76029288], bias[10.82182193]\n",
      "Train loss: 28.228962821098587 \n",
      "\n",
      "Epoch 500 ============================\n",
      "weight[9.76029245], bias[10.8218241]\n",
      "Train loss: 28.22896282095979 \n",
      "\n",
      "Epoch 501 ============================\n",
      "weight[9.76029203], bias[10.82182623]\n",
      "Train loss: 28.228962820827203 \n",
      "\n",
      "Epoch 502 ============================\n",
      "weight[9.76029162], bias[10.82182831]\n",
      "Train loss: 28.228962820700538 \n",
      "\n",
      "Epoch 503 ============================\n",
      "weight[9.76029122], bias[10.82183034]\n",
      "Train loss: 28.2289628205796 \n",
      "\n",
      "Epoch 504 ============================\n",
      "weight[9.76029083], bias[10.82183232]\n",
      "Train loss: 28.22896282046407 \n",
      "\n",
      "Epoch 505 ============================\n",
      "weight[9.76029044], bias[10.82183426]\n",
      "Train loss: 28.22896282035376 \n",
      "\n",
      "Epoch 506 ============================\n",
      "weight[9.76029007], bias[10.82183615]\n",
      "Train loss: 28.22896282024836 \n",
      "\n",
      "Epoch 507 ============================\n",
      "weight[9.76028971], bias[10.82183801]\n",
      "Train loss: 28.228962820147693 \n",
      "\n",
      "Epoch 508 ============================\n",
      "weight[9.76028935], bias[10.82183982]\n",
      "Train loss: 28.22896282005157 \n",
      "\n",
      "Epoch 509 ============================\n",
      "weight[9.760289], bias[10.82184159]\n",
      "Train loss: 28.228962819959698 \n",
      "\n",
      "Epoch 510 ============================\n",
      "weight[9.76028866], bias[10.82184332]\n",
      "Train loss: 28.228962819872006 \n",
      "\n",
      "Epoch 511 ============================\n",
      "weight[9.76028833], bias[10.821845]\n",
      "Train loss: 28.228962819788194 \n",
      "\n",
      "Epoch 512 ============================\n",
      "weight[9.760288], bias[10.82184666]\n",
      "Train loss: 28.228962819708197 \n",
      "\n",
      "Epoch 513 ============================\n",
      "weight[9.76028768], bias[10.82184827]\n",
      "Train loss: 28.228962819631732 \n",
      "\n",
      "Epoch 514 ============================\n",
      "weight[9.76028737], bias[10.82184985]\n",
      "Train loss: 28.22896281955875 \n",
      "\n",
      "Epoch 515 ============================\n",
      "weight[9.76028707], bias[10.82185139]\n",
      "Train loss: 28.228962819489 \n",
      "\n",
      "Epoch 516 ============================\n",
      "weight[9.76028677], bias[10.8218529]\n",
      "Train loss: 28.22896281942238 \n",
      "\n",
      "Epoch 517 ============================\n",
      "weight[9.76028648], bias[10.82185437]\n",
      "Train loss: 28.22896281935878 \n",
      "\n",
      "Epoch 518 ============================\n",
      "weight[9.7602862], bias[10.82185581]\n",
      "Train loss: 28.228962819297987 \n",
      "\n",
      "Epoch 519 ============================\n",
      "weight[9.76028592], bias[10.82185721]\n",
      "Train loss: 28.22896281923997 \n",
      "\n",
      "Epoch 520 ============================\n",
      "weight[9.76028565], bias[10.82185859]\n",
      "Train loss: 28.22896281918452 \n",
      "\n",
      "Epoch 521 ============================\n",
      "weight[9.76028538], bias[10.82185993]\n",
      "Train loss: 28.228962819131574 \n",
      "\n",
      "Epoch 522 ============================\n",
      "weight[9.76028512], bias[10.82186124]\n",
      "Train loss: 28.22896281908098 \n",
      "\n",
      "Epoch 523 ============================\n",
      "weight[9.76028487], bias[10.82186253]\n",
      "Train loss: 28.22896281903269 \n",
      "\n",
      "Epoch 524 ============================\n",
      "weight[9.76028462], bias[10.82186378]\n",
      "Train loss: 28.22896281898656 \n",
      "\n",
      "Epoch 525 ============================\n",
      "weight[9.76028438], bias[10.82186501]\n",
      "Train loss: 28.228962818942456 \n",
      "\n",
      "Epoch 526 ============================\n",
      "weight[9.76028415], bias[10.8218662]\n",
      "Train loss: 28.228962818900406 \n",
      "\n",
      "Epoch 527 ============================\n",
      "weight[9.76028392], bias[10.82186737]\n",
      "Train loss: 28.228962818860193 \n",
      "\n",
      "Epoch 528 ============================\n",
      "weight[9.76028369], bias[10.82186852]\n",
      "Train loss: 28.228962818821767 \n",
      "\n",
      "Epoch 529 ============================\n",
      "weight[9.76028347], bias[10.82186964]\n",
      "Train loss: 28.228962818785085 \n",
      "\n",
      "Epoch 530 ============================\n",
      "weight[9.76028325], bias[10.82187073]\n",
      "Train loss: 28.22896281875007 \n",
      "\n",
      "Epoch 531 ============================\n",
      "weight[9.76028304], bias[10.8218718]\n",
      "Train loss: 28.22896281871655 \n",
      "\n",
      "Epoch 532 ============================\n",
      "weight[9.76028284], bias[10.82187284]\n",
      "Train loss: 28.228962818684636 \n",
      "\n",
      "Epoch 533 ============================\n",
      "weight[9.76028264], bias[10.82187386]\n",
      "Train loss: 28.228962818654082 \n",
      "\n",
      "Epoch 534 ============================\n",
      "weight[9.76028244], bias[10.82187486]\n",
      "Train loss: 28.228962818624925 \n",
      "\n",
      "Epoch 535 ============================\n",
      "weight[9.76028225], bias[10.82187583]\n",
      "Train loss: 28.22896281859708 \n",
      "\n",
      "Epoch 536 ============================\n",
      "weight[9.76028206], bias[10.82187678]\n",
      "Train loss: 28.228962818570444 \n",
      "\n",
      "Epoch 537 ============================\n",
      "weight[9.76028188], bias[10.82187771]\n",
      "Train loss: 28.228962818545035 \n",
      "\n",
      "Epoch 538 ============================\n",
      "weight[9.7602817], bias[10.82187862]\n",
      "Train loss: 28.228962818520813 \n",
      "\n",
      "Epoch 539 ============================\n",
      "weight[9.76028152], bias[10.82187951]\n",
      "Train loss: 28.228962818497596 \n",
      "\n",
      "Epoch 540 ============================\n",
      "weight[9.76028135], bias[10.82188038]\n",
      "Train loss: 28.228962818475473 \n",
      "\n",
      "Epoch 541 ============================\n",
      "weight[9.76028118], bias[10.82188123]\n",
      "Train loss: 28.228962818454303 \n",
      "\n",
      "Epoch 542 ============================\n",
      "weight[9.76028102], bias[10.82188206]\n",
      "Train loss: 28.228962818434137 \n",
      "\n",
      "Epoch 543 ============================\n",
      "weight[9.76028086], bias[10.82188287]\n",
      "Train loss: 28.228962818414818 \n",
      "\n",
      "Epoch 544 ============================\n",
      "weight[9.7602807], bias[10.82188366]\n",
      "Train loss: 28.228962818396393 \n",
      "\n",
      "Epoch 545 ============================\n",
      "weight[9.76028055], bias[10.82188444]\n",
      "Train loss: 28.228962818378744 \n",
      "\n",
      "Epoch 546 ============================\n",
      "weight[9.7602804], bias[10.82188519]\n",
      "Train loss: 28.228962818361993 \n",
      "\n",
      "Epoch 547 ============================\n",
      "weight[9.76028026], bias[10.82188593]\n",
      "Train loss: 28.228962818345913 \n",
      "\n",
      "Epoch 548 ============================\n",
      "weight[9.76028011], bias[10.82188666]\n",
      "Train loss: 28.228962818330547 \n",
      "\n",
      "Epoch 549 ============================\n",
      "weight[9.76027997], bias[10.82188736]\n",
      "Train loss: 28.22896281831593 \n",
      "\n",
      "Epoch 550 ============================\n",
      "weight[9.76027984], bias[10.82188805]\n",
      "Train loss: 28.228962818301948 \n",
      "\n",
      "Epoch 551 ============================\n",
      "weight[9.76027971], bias[10.82188873]\n",
      "Train loss: 28.22896281828855 \n",
      "\n",
      "Epoch 552 ============================\n",
      "weight[9.76027958], bias[10.82188939]\n",
      "Train loss: 28.228962818275754 \n",
      "\n",
      "Epoch 553 ============================\n",
      "weight[9.76027945], bias[10.82189003]\n",
      "Train loss: 28.228962818263593 \n",
      "\n",
      "Epoch 554 ============================\n",
      "weight[9.76027932], bias[10.82189066]\n",
      "Train loss: 28.22896281825197 \n",
      "\n",
      "Epoch 555 ============================\n",
      "weight[9.7602792], bias[10.82189128]\n",
      "Train loss: 28.22896281824085 \n",
      "\n",
      "Epoch 556 ============================\n",
      "weight[9.76027908], bias[10.82189188]\n",
      "Train loss: 28.228962818230176 \n",
      "\n",
      "Epoch 557 ============================\n",
      "weight[9.76027897], bias[10.82189247]\n",
      "Train loss: 28.22896281822003 \n",
      "\n",
      "Epoch 558 ============================\n",
      "weight[9.76027885], bias[10.82189304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.22896281821036 \n",
      "\n",
      "Epoch 559 ============================\n",
      "weight[9.76027874], bias[10.82189361]\n",
      "Train loss: 28.228962818201108 \n",
      "\n",
      "Epoch 560 ============================\n",
      "weight[9.76027864], bias[10.82189415]\n",
      "Train loss: 28.22896281819225 \n",
      "\n",
      "Epoch 561 ============================\n",
      "weight[9.76027853], bias[10.82189469]\n",
      "Train loss: 28.228962818183827 \n",
      "\n",
      "Epoch 562 ============================\n",
      "weight[9.76027843], bias[10.82189522]\n",
      "Train loss: 28.228962818175756 \n",
      "\n",
      "Epoch 563 ============================\n",
      "weight[9.76027833], bias[10.82189573]\n",
      "Train loss: 28.228962818168043 \n",
      "\n",
      "Epoch 564 ============================\n",
      "weight[9.76027823], bias[10.82189623]\n",
      "Train loss: 28.228962818160678 \n",
      "\n",
      "Epoch 565 ============================\n",
      "weight[9.76027813], bias[10.82189672]\n",
      "Train loss: 28.22896281815366 \n",
      "\n",
      "Epoch 566 ============================\n",
      "weight[9.76027804], bias[10.8218972]\n",
      "Train loss: 28.228962818146936 \n",
      "\n",
      "Epoch 567 ============================\n",
      "weight[9.76027794], bias[10.82189766]\n",
      "Train loss: 28.22896281814054 \n",
      "\n",
      "Epoch 568 ============================\n",
      "weight[9.76027785], bias[10.82189812]\n",
      "Train loss: 28.228962818134413 \n",
      "\n",
      "Epoch 569 ============================\n",
      "weight[9.76027777], bias[10.82189857]\n",
      "Train loss: 28.228962818128508 \n",
      "\n",
      "Epoch 570 ============================\n",
      "weight[9.76027768], bias[10.821899]\n",
      "Train loss: 28.228962818122966 \n",
      "\n",
      "Epoch 571 ============================\n",
      "weight[9.7602776], bias[10.82189943]\n",
      "Train loss: 28.228962818117573 \n",
      "\n",
      "Epoch 572 ============================\n",
      "weight[9.76027751], bias[10.82189985]\n",
      "Train loss: 28.228962818112503 \n",
      "\n",
      "Epoch 573 ============================\n",
      "weight[9.76027743], bias[10.82190026]\n",
      "Train loss: 28.228962818107657 \n",
      "\n",
      "Epoch 574 ============================\n",
      "weight[9.76027735], bias[10.82190065]\n",
      "Train loss: 28.228962818102993 \n",
      "\n",
      "Epoch 575 ============================\n",
      "weight[9.76027728], bias[10.82190104]\n",
      "Train loss: 28.228962818098548 \n",
      "\n",
      "Epoch 576 ============================\n",
      "weight[9.7602772], bias[10.82190142]\n",
      "Train loss: 28.228962818094267 \n",
      "\n",
      "Epoch 577 ============================\n",
      "weight[9.76027713], bias[10.82190179]\n",
      "Train loss: 28.228962818090217 \n",
      "\n",
      "Epoch 578 ============================\n",
      "weight[9.76027706], bias[10.82190216]\n",
      "Train loss: 28.22896281808636 \n",
      "\n",
      "Epoch 579 ============================\n",
      "weight[9.76027699], bias[10.82190251]\n",
      "Train loss: 28.2289628180827 \n",
      "\n",
      "Epoch 580 ============================\n",
      "weight[9.76027692], bias[10.82190286]\n",
      "Train loss: 28.22896281807917 \n",
      "\n",
      "Epoch 581 ============================\n",
      "weight[9.76027685], bias[10.8219032]\n",
      "Train loss: 28.228962818075747 \n",
      "\n",
      "Epoch 582 ============================\n",
      "weight[9.76027679], bias[10.82190353]\n",
      "Train loss: 28.22896281807252 \n",
      "\n",
      "Epoch 583 ============================\n",
      "weight[9.76027672], bias[10.82190385]\n",
      "Train loss: 28.228962818069487 \n",
      "\n",
      "Epoch 584 ============================\n",
      "weight[9.76027666], bias[10.82190417]\n",
      "Train loss: 28.22896281806652 \n",
      "\n",
      "Epoch 585 ============================\n",
      "weight[9.7602766], bias[10.82190448]\n",
      "Train loss: 28.228962818063717 \n",
      "\n",
      "Epoch 586 ============================\n",
      "weight[9.76027654], bias[10.82190478]\n",
      "Train loss: 28.228962818061024 \n",
      "\n",
      "Epoch 587 ============================\n",
      "weight[9.76027648], bias[10.82190508]\n",
      "Train loss: 28.228962818058466 \n",
      "\n",
      "Epoch 588 ============================\n",
      "weight[9.76027643], bias[10.82190537]\n",
      "Train loss: 28.22896281805603 \n",
      "\n",
      "Epoch 589 ============================\n",
      "weight[9.76027637], bias[10.82190565]\n",
      "Train loss: 28.22896281805369 \n",
      "\n",
      "Epoch 590 ============================\n",
      "weight[9.76027632], bias[10.82190592]\n",
      "Train loss: 28.22896281805148 \n",
      "\n",
      "Epoch 591 ============================\n",
      "weight[9.76027626], bias[10.82190619]\n",
      "Train loss: 28.228962818049297 \n",
      "\n",
      "Epoch 592 ============================\n",
      "weight[9.76027621], bias[10.82190646]\n",
      "Train loss: 28.228962818047314 \n",
      "\n",
      "Epoch 593 ============================\n",
      "weight[9.76027616], bias[10.82190671]\n",
      "Train loss: 28.228962818045346 \n",
      "\n",
      "Epoch 594 ============================\n",
      "weight[9.76027611], bias[10.82190697]\n",
      "Train loss: 28.228962818043474 \n",
      "\n",
      "Epoch 595 ============================\n",
      "weight[9.76027606], bias[10.82190721]\n",
      "Train loss: 28.228962818041737 \n",
      "\n",
      "Epoch 596 ============================\n",
      "weight[9.76027601], bias[10.82190745]\n",
      "Train loss: 28.228962818040014 \n",
      "\n",
      "Epoch 597 ============================\n",
      "weight[9.76027597], bias[10.82190769]\n",
      "Train loss: 28.228962818038365 \n",
      "\n",
      "Epoch 598 ============================\n",
      "weight[9.76027592], bias[10.82190792]\n",
      "Train loss: 28.22896281803688 \n",
      "\n",
      "Epoch 599 ============================\n",
      "weight[9.76027588], bias[10.82190814]\n",
      "Train loss: 28.228962818035342 \n",
      "\n",
      "Epoch 600 ============================\n",
      "weight[9.76027584], bias[10.82190836]\n",
      "Train loss: 28.22896281803395 \n",
      "\n",
      "Epoch 601 ============================\n",
      "weight[9.76027579], bias[10.82190858]\n",
      "Train loss: 28.22896281803259 \n",
      "\n",
      "Epoch 602 ============================\n",
      "weight[9.76027575], bias[10.82190878]\n",
      "Train loss: 28.228962818031334 \n",
      "\n",
      "Epoch 603 ============================\n",
      "weight[9.76027571], bias[10.82190899]\n",
      "Train loss: 28.228962818030073 \n",
      "\n",
      "Epoch 604 ============================\n",
      "weight[9.76027567], bias[10.82190919]\n",
      "Train loss: 28.228962818028922 \n",
      "\n",
      "Epoch 605 ============================\n",
      "weight[9.76027563], bias[10.82190938]\n",
      "Train loss: 28.228962818027778 \n",
      "\n",
      "Epoch 606 ============================\n",
      "weight[9.7602756], bias[10.82190958]\n",
      "Train loss: 28.228962818026705 \n",
      "\n",
      "Epoch 607 ============================\n",
      "weight[9.76027556], bias[10.82190976]\n",
      "Train loss: 28.228962818025728 \n",
      "\n",
      "Epoch 608 ============================\n",
      "weight[9.76027552], bias[10.82190995]\n",
      "Train loss: 28.22896281802471 \n",
      "\n",
      "Epoch 609 ============================\n",
      "weight[9.76027549], bias[10.82191012]\n",
      "Train loss: 28.228962818023813 \n",
      "\n",
      "Epoch 610 ============================\n",
      "weight[9.76027545], bias[10.8219103]\n",
      "Train loss: 28.22896281802293 \n",
      "\n",
      "Epoch 611 ============================\n",
      "weight[9.76027542], bias[10.82191047]\n",
      "Train loss: 28.22896281802203 \n",
      "\n",
      "Epoch 612 ============================\n",
      "weight[9.76027539], bias[10.82191063]\n",
      "Train loss: 28.228962818021238 \n",
      "\n",
      "Epoch 613 ============================\n",
      "weight[9.76027535], bias[10.8219108]\n",
      "Train loss: 28.228962818020484 \n",
      "\n",
      "Epoch 614 ============================\n",
      "weight[9.76027532], bias[10.82191096]\n",
      "Train loss: 28.22896281801969 \n",
      "\n",
      "Epoch 615 ============================\n",
      "weight[9.76027529], bias[10.82191111]\n",
      "Train loss: 28.228962818019024 \n",
      "\n",
      "Epoch 616 ============================\n",
      "weight[9.76027526], bias[10.82191126]\n",
      "Train loss: 28.228962818018324 \n",
      "\n",
      "Epoch 617 ============================\n",
      "weight[9.76027523], bias[10.82191141]\n",
      "Train loss: 28.22896281801766 \n",
      "\n",
      "Epoch 618 ============================\n",
      "weight[9.76027521], bias[10.82191156]\n",
      "Train loss: 28.22896281801709 \n",
      "\n",
      "Epoch 619 ============================\n",
      "weight[9.76027518], bias[10.8219117]\n",
      "Train loss: 28.22896281801645 \n",
      "\n",
      "Epoch 620 ============================\n",
      "weight[9.76027515], bias[10.82191184]\n",
      "Train loss: 28.228962818015948 \n",
      "\n",
      "Epoch 621 ============================\n",
      "weight[9.76027512], bias[10.82191197]\n",
      "Train loss: 28.228962818015383 \n",
      "\n",
      "Epoch 622 ============================\n",
      "weight[9.7602751], bias[10.82191211]\n",
      "Train loss: 28.228962818014875 \n",
      "\n",
      "Epoch 623 ============================\n",
      "weight[9.76027507], bias[10.82191223]\n",
      "Train loss: 28.228962818014395 \n",
      "\n",
      "Epoch 624 ============================\n",
      "weight[9.76027505], bias[10.82191236]\n",
      "Train loss: 28.228962818013876 \n",
      "\n",
      "Epoch 625 ============================\n",
      "weight[9.76027502], bias[10.82191248]\n",
      "Train loss: 28.22896281801348 \n",
      "\n",
      "Epoch 626 ============================\n",
      "weight[9.760275], bias[10.82191261]\n",
      "Train loss: 28.228962818013024 \n",
      "\n",
      "Epoch 627 ============================\n",
      "weight[9.76027498], bias[10.82191272]\n",
      "Train loss: 28.228962818012615 \n",
      "\n",
      "Epoch 628 ============================\n",
      "weight[9.76027495], bias[10.82191284]\n",
      "Train loss: 28.22896281801224 \n",
      "\n",
      "Epoch 629 ============================\n",
      "weight[9.76027493], bias[10.82191295]\n",
      "Train loss: 28.228962818011883 \n",
      "\n",
      "Epoch 630 ============================\n",
      "weight[9.76027491], bias[10.82191306]\n",
      "Train loss: 28.228962818011524 \n",
      "\n",
      "Epoch 631 ============================\n",
      "weight[9.76027489], bias[10.82191317]\n",
      "Train loss: 28.228962818011187 \n",
      "\n",
      "Epoch 632 ============================\n",
      "weight[9.76027487], bias[10.82191327]\n",
      "Train loss: 28.22896281801082 \n",
      "\n",
      "Epoch 633 ============================\n",
      "weight[9.76027485], bias[10.82191338]\n",
      "Train loss: 28.228962818010555 \n",
      "\n",
      "Epoch 634 ============================\n",
      "weight[9.76027483], bias[10.82191348]\n",
      "Train loss: 28.228962818010217 \n",
      "\n",
      "Epoch 635 ============================\n",
      "weight[9.76027481], bias[10.82191358]\n",
      "Train loss: 28.22896281800991 \n",
      "\n",
      "Epoch 636 ============================\n",
      "weight[9.76027479], bias[10.82191367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228962818009677 \n",
      "\n",
      "Epoch 637 ============================\n",
      "weight[9.76027477], bias[10.82191377]\n",
      "Train loss: 28.228962818009425 \n",
      "\n",
      "Epoch 638 ============================\n",
      "weight[9.76027475], bias[10.82191386]\n",
      "Train loss: 28.228962818009165 \n",
      "\n",
      "Epoch 639 ============================\n",
      "weight[9.76027473], bias[10.82191395]\n",
      "Train loss: 28.228962818008995 \n",
      "\n",
      "Epoch 640 ============================\n",
      "weight[9.76027472], bias[10.82191403]\n",
      "Train loss: 28.228962818008686 \n",
      "\n",
      "Epoch 641 ============================\n",
      "weight[9.7602747], bias[10.82191412]\n",
      "Train loss: 28.228962818008483 \n",
      "\n",
      "Epoch 642 ============================\n",
      "weight[9.76027468], bias[10.8219142]\n",
      "Train loss: 28.22896281800828 \n",
      "\n",
      "Epoch 643 ============================\n",
      "weight[9.76027467], bias[10.82191429]\n",
      "Train loss: 28.22896281800811 \n",
      "\n",
      "Epoch 644 ============================\n",
      "weight[9.76027465], bias[10.82191437]\n",
      "Train loss: 28.228962818007908 \n",
      "\n",
      "Epoch 645 ============================\n",
      "weight[9.76027464], bias[10.82191444]\n",
      "Train loss: 28.22896281800775 \n",
      "\n",
      "Epoch 646 ============================\n",
      "weight[9.76027462], bias[10.82191452]\n",
      "Train loss: 28.22896281800762 \n",
      "\n",
      "Epoch 647 ============================\n",
      "weight[9.76027461], bias[10.82191459]\n",
      "Train loss: 28.22896281800741 \n",
      "\n",
      "Epoch 648 ============================\n",
      "weight[9.76027459], bias[10.82191467]\n",
      "Train loss: 28.228962818007236 \n",
      "\n",
      "Epoch 649 ============================\n",
      "weight[9.76027458], bias[10.82191474]\n",
      "Train loss: 28.22896281800707 \n",
      "\n",
      "Epoch 650 ============================\n",
      "weight[9.76027456], bias[10.82191481]\n",
      "Train loss: 28.228962818006952 \n",
      "\n",
      "Epoch 651 ============================\n",
      "weight[9.76027455], bias[10.82191488]\n",
      "Train loss: 28.22896281800683 \n",
      "\n",
      "Epoch 652 ============================\n",
      "weight[9.76027454], bias[10.82191494]\n",
      "Train loss: 28.228962818006682 \n",
      "\n",
      "Epoch 653 ============================\n",
      "weight[9.76027452], bias[10.82191501]\n",
      "Train loss: 28.228962818006533 \n",
      "\n",
      "Epoch 654 ============================\n",
      "weight[9.76027451], bias[10.82191507]\n",
      "Train loss: 28.228962818006462 \n",
      "\n",
      "Epoch 655 ============================\n",
      "weight[9.7602745], bias[10.82191513]\n",
      "Train loss: 28.2289628180063 \n",
      "\n",
      "Epoch 656 ============================\n",
      "weight[9.76027449], bias[10.82191519]\n",
      "Train loss: 28.228962818006227 \n",
      "\n",
      "Epoch 657 ============================\n",
      "weight[9.76027448], bias[10.82191525]\n",
      "Train loss: 28.228962818006107 \n",
      "\n",
      "Epoch 658 ============================\n",
      "weight[9.76027446], bias[10.82191531]\n",
      "Train loss: 28.228962818006003 \n",
      "\n",
      "Epoch 659 ============================\n",
      "weight[9.76027445], bias[10.82191537]\n",
      "Train loss: 28.228962818005918 \n",
      "\n",
      "Epoch 660 ============================\n",
      "weight[9.76027444], bias[10.82191542]\n",
      "Train loss: 28.228962818005844 \n",
      "\n",
      "Epoch 661 ============================\n",
      "weight[9.76027443], bias[10.82191548]\n",
      "Train loss: 28.228962818005762 \n",
      "\n",
      "Epoch 662 ============================\n",
      "weight[9.76027442], bias[10.82191553]\n",
      "Train loss: 28.22896281800566 \n",
      "\n",
      "Epoch 663 ============================\n",
      "weight[9.76027441], bias[10.82191558]\n",
      "Train loss: 28.228962818005577 \n",
      "\n",
      "Epoch 664 ============================\n",
      "weight[9.7602744], bias[10.82191563]\n",
      "Train loss: 28.22896281800551 \n",
      "\n",
      "Epoch 665 ============================\n",
      "weight[9.76027439], bias[10.82191568]\n",
      "Train loss: 28.22896281800543 \n",
      "\n",
      "Epoch 666 ============================\n",
      "weight[9.76027438], bias[10.82191573]\n",
      "Train loss: 28.228962818005392 \n",
      "\n",
      "Epoch 667 ============================\n",
      "weight[9.76027437], bias[10.82191578]\n",
      "Train loss: 28.2289628180053 \n",
      "\n",
      "Epoch 668 ============================\n",
      "weight[9.76027436], bias[10.82191582]\n",
      "Train loss: 28.22896281800518 \n",
      "\n",
      "Epoch 669 ============================\n",
      "weight[9.76027436], bias[10.82191587]\n",
      "Train loss: 28.228962818005193 \n",
      "\n",
      "Epoch 670 ============================\n",
      "weight[9.76027435], bias[10.82191591]\n",
      "Train loss: 28.22896281800513 \n",
      "\n",
      "Epoch 671 ============================\n",
      "weight[9.76027434], bias[10.82191596]\n",
      "Train loss: 28.228962818005073 \n",
      "\n",
      "Epoch 672 ============================\n",
      "weight[9.76027433], bias[10.821916]\n",
      "Train loss: 28.22896281800499 \n",
      "\n",
      "Epoch 673 ============================\n",
      "weight[9.76027432], bias[10.82191604]\n",
      "Train loss: 28.228962818004938 \n",
      "\n",
      "Epoch 674 ============================\n",
      "weight[9.76027431], bias[10.82191608]\n",
      "Train loss: 28.22896281800488 \n",
      "\n",
      "Epoch 675 ============================\n",
      "weight[9.76027431], bias[10.82191612]\n",
      "Train loss: 28.228962818004852 \n",
      "\n",
      "Epoch 676 ============================\n",
      "weight[9.7602743], bias[10.82191616]\n",
      "Train loss: 28.228962818004806 \n",
      "\n",
      "Epoch 677 ============================\n",
      "weight[9.76027429], bias[10.82191619]\n",
      "Train loss: 28.22896281800477 \n",
      "\n",
      "Epoch 678 ============================\n",
      "weight[9.76027428], bias[10.82191623]\n",
      "Train loss: 28.228962818004753 \n",
      "\n",
      "Epoch 679 ============================\n",
      "weight[9.76027428], bias[10.82191627]\n",
      "Train loss: 28.2289628180047 \n",
      "\n",
      "Epoch 680 ============================\n",
      "weight[9.76027427], bias[10.8219163]\n",
      "Train loss: 28.228962818004703 \n",
      "\n",
      "Epoch 681 ============================\n",
      "weight[9.76027426], bias[10.82191634]\n",
      "Train loss: 28.228962818004668 \n",
      "\n",
      "Epoch 682 ============================\n",
      "weight[9.76027426], bias[10.82191637]\n",
      "Train loss: 28.228962818004618 \n",
      "\n",
      "Epoch 683 ============================\n",
      "weight[9.76027425], bias[10.8219164]\n",
      "Train loss: 28.228962818004568 \n",
      "\n",
      "Epoch 684 ============================\n",
      "weight[9.76027424], bias[10.82191643]\n",
      "Train loss: 28.22896281800456 \n",
      "\n",
      "Epoch 685 ============================\n",
      "weight[9.76027424], bias[10.82191646]\n",
      "Train loss: 28.228962818004526 \n",
      "\n",
      "Epoch 686 ============================\n",
      "weight[9.76027423], bias[10.82191649]\n",
      "Train loss: 28.228962818004504 \n",
      "\n",
      "Epoch 687 ============================\n",
      "weight[9.76027423], bias[10.82191652]\n",
      "Train loss: 28.228962818004447 \n",
      "\n",
      "Epoch 688 ============================\n",
      "weight[9.76027422], bias[10.82191655]\n",
      "Train loss: 28.228962818004476 \n",
      "\n",
      "Epoch 689 ============================\n",
      "weight[9.76027421], bias[10.82191658]\n",
      "Train loss: 28.22896281800439 \n",
      "\n",
      "Epoch 690 ============================\n",
      "weight[9.76027421], bias[10.82191661]\n",
      "Train loss: 28.22896281800439 \n",
      "\n",
      "Epoch 691 ============================\n",
      "weight[9.7602742], bias[10.82191664]\n",
      "Train loss: 28.228962818004383 \n",
      "\n",
      "Epoch 692 ============================\n",
      "weight[9.7602742], bias[10.82191666]\n",
      "Train loss: 28.228962818004366 \n",
      "\n",
      "Epoch 693 ============================\n",
      "weight[9.76027419], bias[10.82191669]\n",
      "Train loss: 28.228962818004277 \n",
      "\n",
      "Epoch 694 ============================\n",
      "weight[9.76027419], bias[10.82191672]\n",
      "Train loss: 28.228962818004327 \n",
      "\n",
      "Epoch 695 ============================\n",
      "weight[9.76027418], bias[10.82191674]\n",
      "Train loss: 28.228962818004327 \n",
      "\n",
      "Epoch 696 ============================\n",
      "weight[9.76027418], bias[10.82191676]\n",
      "Train loss: 28.22896281800426 \n",
      "\n",
      "Epoch 697 ============================\n",
      "weight[9.76027417], bias[10.82191679]\n",
      "Train loss: 28.22896281800427 \n",
      "\n",
      "Epoch 698 ============================\n",
      "weight[9.76027417], bias[10.82191681]\n",
      "Train loss: 28.22896281800426 \n",
      "\n",
      "Epoch 699 ============================\n",
      "weight[9.76027416], bias[10.82191683]\n",
      "Train loss: 28.228962818004238 \n",
      "\n",
      "Epoch 700 ============================\n",
      "weight[9.76027416], bias[10.82191686]\n",
      "Train loss: 28.228962818004195 \n",
      "\n",
      "Epoch 701 ============================\n",
      "weight[9.76027416], bias[10.82191688]\n",
      "Train loss: 28.22896281800421 \n",
      "\n",
      "Epoch 702 ============================\n",
      "weight[9.76027415], bias[10.8219169]\n",
      "Train loss: 28.228962818004167 \n",
      "\n",
      "Epoch 703 ============================\n",
      "weight[9.76027415], bias[10.82191692]\n",
      "Train loss: 28.22896281800418 \n",
      "\n",
      "Epoch 704 ============================\n",
      "weight[9.76027414], bias[10.82191694]\n",
      "Train loss: 28.228962818004153 \n",
      "\n",
      "Epoch 705 ============================\n",
      "weight[9.76027414], bias[10.82191696]\n",
      "Train loss: 28.22896281800413 \n",
      "\n",
      "Epoch 706 ============================\n",
      "weight[9.76027414], bias[10.82191698]\n",
      "Train loss: 28.228962818004188 \n",
      "\n",
      "Epoch 707 ============================\n",
      "weight[9.76027413], bias[10.821917]\n",
      "Train loss: 28.22896281800413 \n",
      "\n",
      "Epoch 708 ============================\n",
      "weight[9.76027413], bias[10.82191702]\n",
      "Train loss: 28.228962818004117 \n",
      "\n",
      "Epoch 709 ============================\n",
      "weight[9.76027413], bias[10.82191703]\n",
      "Train loss: 28.228962818004106 \n",
      "\n",
      "Epoch 710 ============================\n",
      "weight[9.76027412], bias[10.82191705]\n",
      "Train loss: 28.228962818004103 \n",
      "\n",
      "Epoch 711 ============================\n",
      "weight[9.76027412], bias[10.82191707]\n",
      "Train loss: 28.2289628180041 \n",
      "\n",
      "Epoch 712 ============================\n",
      "weight[9.76027412], bias[10.82191708]\n",
      "Train loss: 28.228962818004074 \n",
      "\n",
      "Epoch 713 ============================\n",
      "weight[9.76027411], bias[10.8219171]\n",
      "Train loss: 28.22896281800405 \n",
      "\n",
      "Epoch 714 ============================\n",
      "weight[9.76027411], bias[10.82191712]\n",
      "Train loss: 28.22896281800408 \n",
      "\n",
      "Epoch 715 ============================\n",
      "weight[9.76027411], bias[10.82191713]\n",
      "Train loss: 28.228962818004057 \n",
      "\n",
      "Epoch 716 ============================\n",
      "weight[9.7602741], bias[10.82191715]\n",
      "Train loss: 28.22896281800403 \n",
      "\n",
      "Epoch 717 ============================\n",
      "weight[9.7602741], bias[10.82191716]\n",
      "Train loss: 28.228962818004067 \n",
      "\n",
      "Epoch 718 ============================\n",
      "weight[9.7602741], bias[10.82191718]\n",
      "Train loss: 28.22896281800403 \n",
      "\n",
      "Epoch 719 ============================\n",
      "weight[9.76027409], bias[10.82191719]\n",
      "Train loss: 28.228962818004025 \n",
      "\n",
      "Epoch 720 ============================"
     ]
    }
   ],
   "source": [
    "LinearRegression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
