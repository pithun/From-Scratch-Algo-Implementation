{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Modelling\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Metrics \n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data\n",
    "test = pd.DataFrame({'x':[0,1,2,3,5,6,7], 'y':[10,20,30,40, 60,80,70]})\n",
    "\n",
    "x, y = test[['x']], test[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Linear regression model from scratch using full gradient descent\n",
    "\n",
    "def LinearRegression(X_train, y_train, epochs = 1000, learning_rate = 0.001):\n",
    "    # Initializing random weight with a random normal distribution by default, mean is 0 and std is 1\n",
    "    w1 = abs(np.random.normal())\n",
    "    b = 5\n",
    "    \n",
    "    # Making initial prediction\n",
    "    y_pred = X_train*w1 + b\n",
    "    \n",
    "    # Creating a list that stores the mean squared error\n",
    "    mse_list = []\n",
    "    \n",
    "    # Looping through epochs and performing gradient descent\n",
    "    for x in range(epochs):\n",
    "        print('Epoch '+str(x+1)+' ============================')\n",
    "        print('weight{}, bias{}'.format(w1, b))\n",
    "\n",
    "        # Negative gradient of loss with respect to weight(w1) multiplied by learning rate \n",
    "        x1_neg_gradient = ((y_train - y_pred.values)*X_train.values)*learning_rate\n",
    "        \n",
    "        # Negative gradient of loss with respect to intercept or bias(b) multiplied by learning rate\n",
    "        c_neg_gradient = (y_train - y_pred.values)*learning_rate\n",
    "\n",
    "        # Calculating the Mean Squared Error between predicted and real for first time\n",
    "        mse = mean_squared_error(y_train, y_pred)\n",
    "        \n",
    "        # Printing the loss function(mse in my case) on screen\n",
    "        print('Train loss: {}'.format(mse), '\\n')\n",
    "        \n",
    "        # Appending the loss to the mse_list\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Updating the weight and bias with the negative of the gradient of the loss with respect to the weight and bias.\n",
    "        for a, d in zip(x1_neg_gradient.values, c_neg_gradient.values):\n",
    "            w1 += a\n",
    "            b += d \n",
    "        \n",
    "        # Calculating new Y predicted with updated weight and bias\n",
    "        y_pred = X_train*w1 + b\n",
    "        \n",
    "        #...loop continues\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ============================\n",
      "weight0.7602317610051745, bias0\n",
      "Train loss: 2248.0835302313535 \n",
      "\n",
      "Epoch 2 ============================\n",
      "weight[14.51754438], bias[2.91754438]\n",
      "Train loss: 233.7602943431973 \n",
      "\n",
      "Epoch 3 ============================\n",
      "weight[10.5155787], bias[2.32910562]\n",
      "Train loss: 66.47631190336627 \n",
      "\n",
      "Epoch 4 ============================\n",
      "weight[11.61727576], bias[2.74232934]\n",
      "Train loss: 51.71237099573381 \n",
      "\n",
      "Epoch 5 ============================\n",
      "weight[11.25369478], bias[2.8622201]\n",
      "Train loss: 49.581944025048166 \n",
      "\n",
      "Epoch 6 ============================\n",
      "weight[11.31218043], bias[3.06097795]\n",
      "Train loss: 48.53531661917487 \n",
      "\n",
      "Epoch 7 ============================\n",
      "weight[11.25044199], bias[3.23178619]\n",
      "Train loss: 47.617212629358015 \n",
      "\n",
      "Epoch 8 ============================\n",
      "weight[11.22426524], bias[3.40545508]\n",
      "Train loss: 46.74702126666487 \n",
      "\n",
      "Epoch 9 ============================\n",
      "weight[11.18886712], bias[3.57324957]\n",
      "Train loss: 45.91641494057387 \n",
      "\n",
      "Epoch 10 ============================\n",
      "weight[11.15709199], bias[3.73779399]\n",
      "Train loss: 45.123108161477326 \n",
      "\n",
      "Epoch 11 ============================\n",
      "weight[11.12522736], bias[3.89844633]\n",
      "Train loss: 44.365385895996496 \n",
      "\n",
      "Epoch 12 ============================\n",
      "weight[11.09431831], bias[4.05550052]\n",
      "Train loss: 43.64164866174163 \n",
      "\n",
      "Epoch 13 ============================\n",
      "weight[11.06404348], bias[4.20897909]\n",
      "Train loss: 42.95037190388621 \n",
      "\n",
      "Epoch 14 ============================\n",
      "weight[11.03447458], bias[4.35898012]\n",
      "Train loss: 42.29009970889801 \n",
      "\n",
      "Epoch 15 ============================\n",
      "weight[11.00557087], bias[4.50557761]\n",
      "Train loss: 41.659441484379286 \n",
      "\n",
      "Epoch 16 ============================\n",
      "weight[10.97732436], bias[4.64885017]\n",
      "Train loss: 41.0570690094008 \n",
      "\n",
      "Epoch 17 ============================\n",
      "weight[10.94971811], bias[4.78887281]\n",
      "Train loss: 40.48171363542373 \n",
      "\n",
      "Epoch 18 ============================\n",
      "weight[10.92273818], bias[4.92571936]\n",
      "Train loss: 39.93216361427322 \n",
      "\n",
      "Epoch 19 ============================\n",
      "weight[10.89637019], bias[5.05946184]\n",
      "Train loss: 39.407261546080136 \n",
      "\n",
      "Epoch 20 ============================\n",
      "weight[10.87060031], bias[5.19017067]\n",
      "Train loss: 38.905901941695866 \n",
      "\n",
      "Epoch 21 ============================\n",
      "weight[10.84541496], bias[5.31791465]\n",
      "Train loss: 38.4270288944365 \n",
      "\n",
      "Epoch 22 ============================\n",
      "weight[10.82080089], bias[5.44276103]\n",
      "Train loss: 37.96963385625208 \n",
      "\n",
      "Epoch 23 ============================\n",
      "weight[10.79674514], bias[5.56477554]\n",
      "Train loss: 37.532753513637225 \n",
      "\n",
      "Epoch 24 ============================\n",
      "weight[10.77323504], bias[5.68402242]\n",
      "Train loss: 37.115467758809366 \n",
      "\n",
      "Epoch 25 ============================\n",
      "weight[10.75025821], bias[5.80056445]\n",
      "Train loss: 36.71689775188235 \n",
      "\n",
      "Epoch 26 ============================\n",
      "weight[10.72780256], bias[5.91446296]\n",
      "Train loss: 36.33620406995381 \n",
      "\n",
      "Epoch 27 ============================\n",
      "weight[10.70585627], bias[6.02577794]\n",
      "Train loss: 35.972584939207486 \n",
      "\n",
      "Epoch 28 ============================\n",
      "weight[10.68440779], bias[6.13456798]\n",
      "Train loss: 35.62527454630875 \n",
      "\n",
      "Epoch 29 ============================\n",
      "weight[10.66344582], bias[6.24089035]\n",
      "Train loss: 35.293541425534976 \n",
      "\n",
      "Epoch 30 ============================\n",
      "weight[10.64295932], bias[6.34480103]\n",
      "Train loss: 34.976686918246 \n",
      "\n",
      "Epoch 31 ============================\n",
      "weight[10.62293752], bias[6.44635472]\n",
      "Train loss: 34.67404370144822 \n",
      "\n",
      "Epoch 32 ============================\n",
      "weight[10.60336986], bias[6.54560489]\n",
      "Train loss: 34.38497438235477 \n",
      "\n",
      "Epoch 33 ============================\n",
      "weight[10.58424606], bias[6.64260378]\n",
      "Train loss: 34.108870155981194 \n",
      "\n",
      "Epoch 34 ============================\n",
      "weight[10.56555604], bias[6.73740246]\n",
      "Train loss: 33.845149522949626 \n",
      "\n",
      "Epoch 35 ============================\n",
      "weight[10.54728996], bias[6.83005084]\n",
      "Train loss: 33.59325706480057 \n",
      "\n",
      "Epoch 36 ============================\n",
      "weight[10.52943821], bias[6.92059769]\n",
      "Train loss: 33.35266227423374 \n",
      "\n",
      "Epoch 37 ============================\n",
      "weight[10.51199138], bias[7.00909068]\n",
      "Train loss: 33.122858437813896 \n",
      "\n",
      "Epoch 38 ============================\n",
      "weight[10.4949403], bias[7.0955764]\n",
      "Train loss: 32.90336156878821 \n",
      "\n",
      "Epoch 39 ============================\n",
      "weight[10.47827599], bias[7.18010038]\n",
      "Train loss: 32.69370938776884 \n",
      "\n",
      "Epoch 40 ============================\n",
      "weight[10.46198967], bias[7.26270712]\n",
      "Train loss: 32.49346034913192 \n",
      "\n",
      "Epoch 41 ============================\n",
      "weight[10.44607277], bias[7.3434401]\n",
      "Train loss: 32.30219271108483 \n",
      "\n",
      "Epoch 42 ============================\n",
      "weight[10.43051691], bias[7.42234182]\n",
      "Train loss: 32.11950364744114 \n",
      "\n",
      "Epoch 43 ============================\n",
      "weight[10.4153139], bias[7.49945384]\n",
      "Train loss: 31.945008399234172 \n",
      "\n",
      "Epoch 44 ============================\n",
      "weight[10.40045574], bias[7.57481673]\n",
      "Train loss: 31.778339464380963 \n",
      "\n",
      "Epoch 45 ============================\n",
      "weight[10.38593461], bias[7.64847018]\n",
      "Train loss: 31.619145823691387 \n",
      "\n",
      "Epoch 46 ============================\n",
      "weight[10.37174285], bias[7.72045297]\n",
      "Train loss: 31.467092201591193 \n",
      "\n",
      "Epoch 47 ============================\n",
      "weight[10.357873], bias[7.79080297]\n",
      "Train loss: 31.321858360002828 \n",
      "\n",
      "Epoch 48 ============================\n",
      "weight[10.34431777], bias[7.85955724]\n",
      "Train loss: 31.183138423896327 \n",
      "\n",
      "Epoch 49 ============================\n",
      "weight[10.33107], bias[7.92675197]\n",
      "Train loss: 31.05064023708977 \n",
      "\n",
      "Epoch 50 ============================\n",
      "weight[10.31812273], bias[7.99242254]\n",
      "Train loss: 30.92408474694351 \n",
      "\n",
      "Epoch 51 ============================\n",
      "weight[10.30546914], bias[8.0566035]\n",
      "Train loss: 30.803205416650915 \n",
      "\n",
      "Epoch 52 ============================\n",
      "weight[10.29310257], bias[8.11932867]\n",
      "Train loss: 30.68774766388886 \n",
      "\n",
      "Epoch 53 ============================\n",
      "weight[10.2810165], bias[8.18063104]\n",
      "Train loss: 30.577468324645327 \n",
      "\n",
      "Epoch 54 ============================\n",
      "weight[10.26920459], bias[8.24054291]\n",
      "Train loss: 30.472135141095116 \n",
      "\n",
      "Epoch 55 ============================\n",
      "weight[10.2576606], bias[8.2990958]\n",
      "Train loss: 30.371526272444402 \n",
      "\n",
      "Epoch 56 ============================\n",
      "weight[10.24637846], bias[8.35632055]\n",
      "Train loss: 30.275429827715495 \n",
      "\n",
      "Epoch 57 ============================\n",
      "weight[10.23535224], bias[8.41224728]\n",
      "Train loss: 30.183643419485254 \n",
      "\n",
      "Epoch 58 ============================\n",
      "weight[10.22457612], bias[8.46690544]\n",
      "Train loss: 30.095973737640488 \n",
      "\n",
      "Epoch 59 ============================\n",
      "weight[10.21404443], bias[8.52032379]\n",
      "Train loss: 30.0122361422496 \n",
      "\n",
      "Epoch 60 ============================\n",
      "weight[10.20375163], bias[8.57253046]\n",
      "Train loss: 29.932254274695147 \n",
      "\n",
      "Epoch 61 ============================\n",
      "weight[10.1936923], bias[8.62355294]\n",
      "Train loss: 29.855859686247392 \n",
      "\n",
      "Epoch 62 ============================\n",
      "weight[10.18386114], bias[8.67341808]\n",
      "Train loss: 29.782891483296424 \n",
      "\n",
      "Epoch 63 ============================\n",
      "weight[10.17425299], bias[8.72215214]\n",
      "Train loss: 29.713195988496725 \n",
      "\n",
      "Epoch 64 ============================\n",
      "weight[10.16486277], bias[8.76978077]\n",
      "Train loss: 29.646626417108912 \n",
      "\n",
      "Epoch 65 ============================\n",
      "weight[10.15568555], bias[8.81632906]\n",
      "Train loss: 29.583042567858968 \n",
      "\n",
      "Epoch 66 ============================\n",
      "weight[10.14671649], bias[8.86182149]\n",
      "Train loss: 29.522310527661553 \n",
      "\n",
      "Epoch 67 ============================\n",
      "weight[10.13795088], bias[8.90628203]\n",
      "Train loss: 29.464302389588067 \n",
      "\n",
      "Epoch 68 ============================\n",
      "weight[10.1293841], bias[8.94973407]\n",
      "Train loss: 29.408895983483166 \n",
      "\n",
      "Epoch 69 ============================\n",
      "weight[10.12101164], bias[8.9922005]\n",
      "Train loss: 29.35597461866411 \n",
      "\n",
      "Epoch 70 ============================\n",
      "weight[10.11282909], bias[9.03370367]\n",
      "Train loss: 29.305426838160106 \n",
      "\n",
      "Epoch 71 ============================\n",
      "weight[10.10483214], bias[9.07426544]\n",
      "Train loss: 29.257146183973965 \n",
      "\n",
      "Epoch 72 ============================\n",
      "weight[10.09701658], bias[9.11390714]\n",
      "Train loss: 29.211030972872777 \n",
      "\n",
      "Epoch 73 ============================\n",
      "weight[10.08937831], bias[9.15264966]\n",
      "Train loss: 29.166984082233977 \n",
      "\n",
      "Epoch 74 ============================\n",
      "weight[10.08191329], bias[9.19051339]\n",
      "Train loss: 29.124912745496733 \n",
      "\n",
      "Epoch 75 ============================\n",
      "weight[10.0746176], bias[9.22751827]\n",
      "Train loss: 29.0847283567876 \n",
      "\n",
      "Epoch 76 ============================\n",
      "weight[10.06748739], bias[9.26368377]\n",
      "Train loss: 29.04634628430883 \n",
      "\n",
      "Epoch 77 ============================\n",
      "weight[10.06051892], bias[9.29902893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 29.009685692096564 \n",
      "\n",
      "Epoch 78 ============================\n",
      "weight[10.05370852], bias[9.33357236]\n",
      "Train loss: 28.974669369773217 \n",
      "\n",
      "Epoch 79 ============================\n",
      "weight[10.04705259], bias[9.36733225]\n",
      "Train loss: 28.941223569935858 \n",
      "\n",
      "Epoch 80 ============================\n",
      "weight[10.04054764], bias[9.40032637]\n",
      "Train loss: 28.90927785283755 \n",
      "\n",
      "Epoch 81 ============================\n",
      "weight[10.03419024], bias[9.43257209]\n",
      "Train loss: 28.8787649380356 \n",
      "\n",
      "Epoch 82 ============================\n",
      "weight[10.02797704], bias[9.46408639]\n",
      "Train loss: 28.849620562692575 \n",
      "\n",
      "Epoch 83 ============================\n",
      "weight[10.02190478], bias[9.49488585]\n",
      "Train loss: 28.821783346233765 \n",
      "\n",
      "Epoch 84 ============================\n",
      "weight[10.01597025], bias[9.5249867]\n",
      "Train loss: 28.79519466107384 \n",
      "\n",
      "Epoch 85 ============================\n",
      "weight[10.01017033], bias[9.55440477]\n",
      "Train loss: 28.769798509142564 \n",
      "\n",
      "Epoch 86 ============================\n",
      "weight[10.00450198], bias[9.58315555]\n",
      "Train loss: 28.745541403947716 \n",
      "\n",
      "Epoch 87 ============================\n",
      "weight[9.99896219], bias[9.61125419]\n",
      "Train loss: 28.7223722579281 \n",
      "\n",
      "Epoch 88 ============================\n",
      "weight[9.99354807], bias[9.63871547]\n",
      "Train loss: 28.700242274858407 \n",
      "\n",
      "Epoch 89 ============================\n",
      "weight[9.98825675], bias[9.66555385]\n",
      "Train loss: 28.679104847080662 \n",
      "\n",
      "Epoch 90 ============================\n",
      "weight[9.98308546], bias[9.69178346]\n",
      "Train loss: 28.658915457343966 \n",
      "\n",
      "Epoch 91 ============================\n",
      "weight[9.97803146], bias[9.71741811]\n",
      "Train loss: 28.639631585047532 \n",
      "\n",
      "Epoch 92 ============================\n",
      "weight[9.9730921], bias[9.74247129]\n",
      "Train loss: 28.621212616688663 \n",
      "\n",
      "Epoch 93 ============================\n",
      "weight[9.96826478], bias[9.7669562]\n",
      "Train loss: 28.603619760327252 \n",
      "\n",
      "Epoch 94 ============================\n",
      "weight[9.96354696], bias[9.79088572]\n",
      "Train loss: 28.586815963886295 \n",
      "\n",
      "Epoch 95 ============================\n",
      "weight[9.95893616], bias[9.81427244]\n",
      "Train loss: 28.570765837117246 \n",
      "\n",
      "Epoch 96 ============================\n",
      "weight[9.95442994], bias[9.8371287]\n",
      "Train loss: 28.55543557706533 \n",
      "\n",
      "Epoch 97 ============================\n",
      "weight[9.95002593], bias[9.8594665]\n",
      "Train loss: 28.540792896876958 \n",
      "\n",
      "Epoch 98 ============================\n",
      "weight[9.94572182], bias[9.88129762]\n",
      "Train loss: 28.526806957801416 \n",
      "\n",
      "Epoch 99 ============================\n",
      "weight[9.94151533], bias[9.90263355]\n",
      "Train loss: 28.513448304241553 \n",
      "\n",
      "Epoch 100 ============================\n",
      "weight[9.93740427], bias[9.92348553]\n",
      "Train loss: 28.5006888017177 \n",
      "\n",
      "Epoch 101 ============================\n",
      "weight[9.93338645], bias[9.94386451]\n",
      "Train loss: 28.48850157761415 \n",
      "\n",
      "Epoch 102 ============================\n",
      "weight[9.92945977], bias[9.96378125]\n",
      "Train loss: 28.476860964582944 \n",
      "\n",
      "Epoch 103 ============================\n",
      "weight[9.92562216], bias[9.98324622]\n",
      "Train loss: 28.465742446486445 \n",
      "\n",
      "Epoch 104 ============================\n",
      "weight[9.92187159], bias[10.00226967]\n",
      "Train loss: 28.455122606763975 \n",
      "\n",
      "Epoch 105 ============================\n",
      "weight[9.9182061], bias[10.02086161]\n",
      "Train loss: 28.44497907911462 \n",
      "\n",
      "Epoch 106 ============================\n",
      "weight[9.91462375], bias[10.03903183]\n",
      "Train loss: 28.435290500392163 \n",
      "\n",
      "Epoch 107 ============================\n",
      "weight[9.91112266], bias[10.0567899]\n",
      "Train loss: 28.42603646561168 \n",
      "\n",
      "Epoch 108 ============================\n",
      "weight[9.90770098], bias[10.07414517]\n",
      "Train loss: 28.417197484975837 \n",
      "\n",
      "Epoch 109 ============================\n",
      "weight[9.90435692], bias[10.09110677]\n",
      "Train loss: 28.40875494282698 \n",
      "\n",
      "Epoch 110 ============================\n",
      "weight[9.90108871], bias[10.10768364]\n",
      "Train loss: 28.40069105844113 \n",
      "\n",
      "Epoch 111 ============================\n",
      "weight[9.89789464], bias[10.12388449]\n",
      "Train loss: 28.39298884858064 \n",
      "\n",
      "Epoch 112 ============================\n",
      "weight[9.89477301], bias[10.13971787]\n",
      "Train loss: 28.385632091725366 \n",
      "\n",
      "Epoch 113 ============================\n",
      "weight[9.89172219], bias[10.15519209]\n",
      "Train loss: 28.37860529390906 \n",
      "\n",
      "Epoch 114 ============================\n",
      "weight[9.88874057], bias[10.17031532]\n",
      "Train loss: 28.37189365608794 \n",
      "\n",
      "Epoch 115 ============================\n",
      "weight[9.88582659], bias[10.18509551]\n",
      "Train loss: 28.365483042972375 \n",
      "\n",
      "Epoch 116 ============================\n",
      "weight[9.8829787], bias[10.19954044]\n",
      "Train loss: 28.35935995325668 \n",
      "\n",
      "Epoch 117 ============================\n",
      "weight[9.88019541], bias[10.21365773]\n",
      "Train loss: 28.3535114911845 \n",
      "\n",
      "Epoch 118 ============================\n",
      "weight[9.87747525], bias[10.22745479]\n",
      "Train loss: 28.347925339388784 \n",
      "\n",
      "Epoch 119 ============================\n",
      "weight[9.87481679], bias[10.24093889]\n",
      "Train loss: 28.3425897329509 \n",
      "\n",
      "Epoch 120 ============================\n",
      "weight[9.87221864], bias[10.25411714]\n",
      "Train loss: 28.337493434622274 \n",
      "\n",
      "Epoch 121 ============================\n",
      "weight[9.86967941], bias[10.26699647]\n",
      "Train loss: 28.33262571115778 \n",
      "\n",
      "Epoch 122 ============================\n",
      "weight[9.86719779], bias[10.27958366]\n",
      "Train loss: 28.327976310710977 \n",
      "\n",
      "Epoch 123 ============================\n",
      "weight[9.86477245], bias[10.29188533]\n",
      "Train loss: 28.32353544124238 \n",
      "\n",
      "Epoch 124 ============================\n",
      "weight[9.86240213], bias[10.30390797]\n",
      "Train loss: 28.31929374989688 \n",
      "\n",
      "Epoch 125 ============================\n",
      "weight[9.86008558], bias[10.3156579]\n",
      "Train loss: 28.31524230330563 \n",
      "\n",
      "Epoch 126 ============================\n",
      "weight[9.85782157], bias[10.32714131]\n",
      "Train loss: 28.311372568771816 \n",
      "\n",
      "Epoch 127 ============================\n",
      "weight[9.85560891], bias[10.33836424]\n",
      "Train loss: 28.307676396299822 \n",
      "\n",
      "Epoch 128 ============================\n",
      "weight[9.85344644], bias[10.34933261]\n",
      "Train loss: 28.30414600143081 \n",
      "\n",
      "Epoch 129 ============================\n",
      "weight[9.85133303], bias[10.36005218]\n",
      "Train loss: 28.30077394884785 \n",
      "\n",
      "Epoch 130 ============================\n",
      "weight[9.84926755], bias[10.3705286]\n",
      "Train loss: 28.29755313671657 \n",
      "\n",
      "Epoch 131 ============================\n",
      "weight[9.84724892], bias[10.38076738]\n",
      "Train loss: 28.294476781728115 \n",
      "\n",
      "Epoch 132 ============================\n",
      "weight[9.84527609], bias[10.39077392]\n",
      "Train loss: 28.29153840481291 \n",
      "\n",
      "Epoch 133 ============================\n",
      "weight[9.843348], bias[10.40055349]\n",
      "Train loss: 28.28873181749486 \n",
      "\n",
      "Epoch 134 ============================\n",
      "weight[9.84146364], bias[10.41011123]\n",
      "Train loss: 28.286051108858665 \n",
      "\n",
      "Epoch 135 ============================\n",
      "weight[9.83962203], bias[10.41945217]\n",
      "Train loss: 28.28349063309984 \n",
      "\n",
      "Epoch 136 ============================\n",
      "weight[9.83782219], bias[10.42858123]\n",
      "Train loss: 28.281044997635142 \n",
      "\n",
      "Epoch 137 ============================\n",
      "weight[9.83606318], bias[10.43750321]\n",
      "Train loss: 28.27870905174472 \n",
      "\n",
      "Epoch 138 ============================\n",
      "weight[9.83434407], bias[10.44622283]\n",
      "Train loss: 28.276477875724645 \n",
      "\n",
      "Epoch 139 ============================\n",
      "weight[9.83266395], bias[10.45474465]\n",
      "Train loss: 28.274346770525096 \n",
      "\n",
      "Epoch 140 ============================\n",
      "weight[9.83102194], bias[10.46307318]\n",
      "Train loss: 28.272311247854343 \n",
      "\n",
      "Epoch 141 ============================\n",
      "weight[9.82941717], bias[10.47121279]\n",
      "Train loss: 28.270367020725605 \n",
      "\n",
      "Epoch 142 ============================\n",
      "weight[9.82784881], bias[10.47916778]\n",
      "Train loss: 28.26850999442839 \n",
      "\n",
      "Epoch 143 ============================\n",
      "weight[9.82631602], bias[10.48694232]\n",
      "Train loss: 28.266736257904686 \n",
      "\n",
      "Epoch 144 ============================\n",
      "weight[9.824818], bias[10.49454051]\n",
      "Train loss: 28.265042075511992 \n",
      "\n",
      "Epoch 145 ============================\n",
      "weight[9.82335396], bias[10.50196636]\n",
      "Train loss: 28.263423879155773 \n",
      "\n",
      "Epoch 146 ============================\n",
      "weight[9.82192312], bias[10.50922376]\n",
      "Train loss: 28.26187826077446 \n",
      "\n",
      "Epoch 147 ============================\n",
      "weight[9.82052475], bias[10.51631655]\n",
      "Train loss: 28.260401965162153 \n",
      "\n",
      "Epoch 148 ============================\n",
      "weight[9.81915809], bias[10.52324845]\n",
      "Train loss: 28.258991883112543 \n",
      "\n",
      "Epoch 149 ============================\n",
      "weight[9.81782243], bias[10.53002312]\n",
      "Train loss: 28.257645044871 \n",
      "\n",
      "Epoch 150 ============================\n",
      "weight[9.81651707], bias[10.53664412]\n",
      "Train loss: 28.256358613879595 \n",
      "\n",
      "Epoch 151 ============================\n",
      "weight[9.81524132], bias[10.54311493]\n",
      "Train loss: 28.25512988080346 \n",
      "\n",
      "Epoch 152 ============================\n",
      "weight[9.8139945], bias[10.54943897]\n",
      "Train loss: 28.253956257824445 \n",
      "\n",
      "Epoch 153 ============================\n",
      "weight[9.81277597], bias[10.55561956]\n",
      "Train loss: 28.252835273191177 \n",
      "\n",
      "Epoch 154 ============================\n",
      "weight[9.81158507], bias[10.56165996]\n",
      "Train loss: 28.25176456601293 \n",
      "\n",
      "Epoch 155 ============================\n",
      "weight[9.81042119], bias[10.56756335]\n",
      "Train loss: 28.250741881287915 \n",
      "\n",
      "Epoch 156 ============================\n",
      "weight[9.80928371], bias[10.57333283]\n",
      "Train loss: 28.24976506515357 \n",
      "\n",
      "Epoch 157 ============================\n",
      "weight[9.80817203], bias[10.57897144]\n",
      "Train loss: 28.248832060350708 \n",
      "\n",
      "Epoch 158 ============================\n",
      "weight[9.80708557], bias[10.58448215]\n",
      "Train loss: 28.247940901890495 \n",
      "\n",
      "Epoch 159 ============================\n",
      "weight[9.80602375], bias[10.58986786]\n",
      "Train loss: 28.247089712916125 \n",
      "\n",
      "Epoch 160 ============================\n",
      "weight[9.80498601], bias[10.59513141]\n",
      "Train loss: 28.24627670075004 \n",
      "\n",
      "Epoch 161 ============================\n",
      "weight[9.80397182], bias[10.60027557]\n",
      "Train loss: 28.245500153118368 \n",
      "\n",
      "Epoch 162 ============================\n",
      "weight[9.80298063], bias[10.60530304]\n",
      "Train loss: 28.24475843454448 \n",
      "\n",
      "Epoch 163 ============================\n",
      "weight[9.80201192], bias[10.61021648]\n",
      "Train loss: 28.244049982905064 \n",
      "\n",
      "Epoch 164 ============================\n",
      "weight[9.80106518], bias[10.61501847]\n",
      "Train loss: 28.24337330613965 \n",
      "\n",
      "Epoch 165 ============================\n",
      "weight[9.80013992], bias[10.61971153]\n",
      "Train loss: 28.242726979108454 \n",
      "\n",
      "Epoch 166 ============================\n",
      "weight[9.79923565], bias[10.62429814]\n",
      "Train loss: 28.24210964059091 \n",
      "\n",
      "Epoch 167 ============================\n",
      "weight[9.79835189], bias[10.62878071]\n",
      "Train loss: 28.24151999041857 \n",
      "\n",
      "Epoch 168 ============================\n",
      "weight[9.79748817], bias[10.63316161]\n",
      "Train loss: 28.24095678673728 \n",
      "\n",
      "Epoch 169 ============================\n",
      "weight[9.79664405], bias[10.63744314]\n",
      "Train loss: 28.24041884339141 \n",
      "\n",
      "Epoch 170 ============================\n",
      "weight[9.79581908], bias[10.64162754]\n",
      "Train loss: 28.239905027425664 \n",
      "\n",
      "Epoch 171 ============================\n",
      "weight[9.79501281], bias[10.64571704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.239414256699312 \n",
      "\n",
      "Epoch 172 ============================\n",
      "weight[9.79422484], bias[10.64971377]\n",
      "Train loss: 28.238945497606615 \n",
      "\n",
      "Epoch 173 ============================\n",
      "weight[9.79345473], bias[10.65361985]\n",
      "Train loss: 28.23849776290044 \n",
      "\n",
      "Epoch 174 ============================\n",
      "weight[9.7927021], bias[10.65743732]\n",
      "Train loss: 28.23807010961279 \n",
      "\n",
      "Epoch 175 ============================\n",
      "weight[9.79196654], bias[10.6611682]\n",
      "Train loss: 28.237661637068843 \n",
      "\n",
      "Epoch 176 ============================\n",
      "weight[9.79124766], bias[10.66481446]\n",
      "Train loss: 28.237271484990213 \n",
      "\n",
      "Epoch 177 ============================\n",
      "weight[9.79054509], bias[10.66837801]\n",
      "Train loss: 28.23689883168274 \n",
      "\n",
      "Epoch 178 ============================\n",
      "weight[9.78985846], bias[10.67186073]\n",
      "Train loss: 28.23654289230635 \n",
      "\n",
      "Epoch 179 ============================\n",
      "weight[9.7891874], bias[10.67526445]\n",
      "Train loss: 28.23620291722195 \n",
      "\n",
      "Epoch 180 ============================\n",
      "weight[9.78853156], bias[10.67859096]\n",
      "Train loss: 28.23587819041262 \n",
      "\n",
      "Epoch 181 ============================\n",
      "weight[9.7878906], bias[10.68184202]\n",
      "Train loss: 28.23556802797561 \n",
      "\n",
      "Epoch 182 ============================\n",
      "weight[9.78726417], bias[10.68501933]\n",
      "Train loss: 28.235271776681987 \n",
      "\n",
      "Epoch 183 ============================\n",
      "weight[9.78665196], bias[10.68812458]\n",
      "Train loss: 28.23498881260104 \n",
      "\n",
      "Epoch 184 ============================\n",
      "weight[9.78605363], bias[10.69115939]\n",
      "Train loss: 28.234718539785696 \n",
      "\n",
      "Epoch 185 ============================\n",
      "weight[9.78546888], bias[10.69412536]\n",
      "Train loss: 28.234460389018295 \n",
      "\n",
      "Epoch 186 ============================\n",
      "weight[9.78489738], bias[10.69702406]\n",
      "Train loss: 28.23421381661078 \n",
      "\n",
      "Epoch 187 ============================\n",
      "weight[9.78433885], bias[10.699857]\n",
      "Train loss: 28.233978303260518 \n",
      "\n",
      "Epoch 188 ============================\n",
      "weight[9.78379299], bias[10.70262568]\n",
      "Train loss: 28.233753352955862 \n",
      "\n",
      "Epoch 189 ============================\n",
      "weight[9.78325952], bias[10.70533157]\n",
      "Train loss: 28.233538491932144 \n",
      "\n",
      "Epoch 190 ============================\n",
      "weight[9.78273814], bias[10.70797607]\n",
      "Train loss: 28.233333267673405 \n",
      "\n",
      "Epoch 191 ============================\n",
      "weight[9.78222859], bias[10.7105606]\n",
      "Train loss: 28.233137247959633 \n",
      "\n",
      "Epoch 192 ============================\n",
      "weight[9.7817306], bias[10.71308649]\n",
      "Train loss: 28.232950019956338 \n",
      "\n",
      "Epoch 193 ============================\n",
      "weight[9.7812439], bias[10.71555509]\n",
      "Train loss: 28.232771189345122 \n",
      "\n",
      "Epoch 194 ============================\n",
      "weight[9.78076824], bias[10.7179677]\n",
      "Train loss: 28.232600379493356 \n",
      "\n",
      "Epoch 195 ============================\n",
      "weight[9.78030337], bias[10.72032559]\n",
      "Train loss: 28.232437230660604 \n",
      "\n",
      "Epoch 196 ============================\n",
      "weight[9.77984905], bias[10.72262998]\n",
      "Train loss: 28.23228139924141 \n",
      "\n",
      "Epoch 197 ============================\n",
      "weight[9.77940503], bias[10.72488211]\n",
      "Train loss: 28.23213255704129 \n",
      "\n",
      "Epoch 198 ============================\n",
      "weight[9.77897109], bias[10.72708316]\n",
      "Train loss: 28.231990390585732 \n",
      "\n",
      "Epoch 199 ============================\n",
      "weight[9.77854698], bias[10.72923428]\n",
      "Train loss: 28.2318546004598 \n",
      "\n",
      "Epoch 200 ============================\n",
      "weight[9.7781325], bias[10.7313366]\n",
      "Train loss: 28.23172490067777 \n",
      "\n",
      "Epoch 201 ============================\n",
      "weight[9.77772742], bias[10.73339124]\n",
      "Train loss: 28.231601018080774 \n",
      "\n",
      "Epoch 202 ============================\n",
      "weight[9.77733152], bias[10.73539927]\n",
      "Train loss: 28.231482691761254 \n",
      "\n",
      "Epoch 203 ============================\n",
      "weight[9.77694461], bias[10.73736176]\n",
      "Train loss: 28.231369672513797 \n",
      "\n",
      "Epoch 204 ============================\n",
      "weight[9.77656647], bias[10.73927973]\n",
      "Train loss: 28.231261722310105 \n",
      "\n",
      "Epoch 205 ============================\n",
      "weight[9.77619691], bias[10.7411542]\n",
      "Train loss: 28.231158613797675 \n",
      "\n",
      "Epoch 206 ============================\n",
      "weight[9.77583573], bias[10.74298614]\n",
      "Train loss: 28.23106012982103 \n",
      "\n",
      "Epoch 207 ============================\n",
      "weight[9.77548275], bias[10.74477654]\n",
      "Train loss: 28.230966062964495 \n",
      "\n",
      "Epoch 208 ============================\n",
      "weight[9.77513777], bias[10.74652632]\n",
      "Train loss: 28.230876215115106 \n",
      "\n",
      "Epoch 209 ============================\n",
      "weight[9.77480062], bias[10.74823641]\n",
      "Train loss: 28.230790397045446 \n",
      "\n",
      "Epoch 210 ============================\n",
      "weight[9.77447111], bias[10.74990771]\n",
      "Train loss: 28.230708428015312 \n",
      "\n",
      "Epoch 211 ============================\n",
      "weight[9.77414908], bias[10.75154111]\n",
      "Train loss: 28.23063013539083 \n",
      "\n",
      "Epoch 212 ============================\n",
      "weight[9.77383435], bias[10.75313745]\n",
      "Train loss: 28.230555354280966 \n",
      "\n",
      "Epoch 213 ============================\n",
      "weight[9.77352677], bias[10.75469758]\n",
      "Train loss: 28.230483927190157 \n",
      "\n",
      "Epoch 214 ============================\n",
      "weight[9.77322616], bias[10.75622233]\n",
      "Train loss: 28.23041570368688 \n",
      "\n",
      "Epoch 215 ============================\n",
      "weight[9.77293236], bias[10.75771249]\n",
      "Train loss: 28.230350540086558 \n",
      "\n",
      "Epoch 216 ============================\n",
      "weight[9.77264524], bias[10.75916885]\n",
      "Train loss: 28.23028829914884 \n",
      "\n",
      "Epoch 217 ============================\n",
      "weight[9.77236462], bias[10.76059217]\n",
      "Train loss: 28.230228849789146 \n",
      "\n",
      "Epoch 218 ============================\n",
      "weight[9.77209037], bias[10.76198321]\n",
      "Train loss: 28.230172066801792 \n",
      "\n",
      "Epoch 219 ============================\n",
      "weight[9.77182234], bias[10.7633427]\n",
      "Train loss: 28.230117830597017 \n",
      "\n",
      "Epoch 220 ============================\n",
      "weight[9.77156039], bias[10.76467135]\n",
      "Train loss: 28.23006602694866 \n",
      "\n",
      "Epoch 221 ============================\n",
      "weight[9.77130438], bias[10.76596986]\n",
      "Train loss: 28.230016546753728 \n",
      "\n",
      "Epoch 222 ============================\n",
      "weight[9.77105418], bias[10.76723892]\n",
      "Train loss: 28.229969285802646 \n",
      "\n",
      "Epoch 223 ============================\n",
      "weight[9.77080966], bias[10.76847919]\n",
      "Train loss: 28.229924144559813 \n",
      "\n",
      "Epoch 224 ============================\n",
      "weight[9.77057068], bias[10.76969133]\n",
      "Train loss: 28.229881027953898 \n",
      "\n",
      "Epoch 225 ============================\n",
      "weight[9.77033712], bias[10.77087597]\n",
      "Train loss: 28.22983984517744 \n",
      "\n",
      "Epoch 226 ============================\n",
      "weight[9.77010886], bias[10.77203375]\n",
      "Train loss: 28.229800509496073 \n",
      "\n",
      "Epoch 227 ============================\n",
      "weight[9.76988578], bias[10.77316526]\n",
      "Train loss: 28.229762938065456 \n",
      "\n",
      "Epoch 228 ============================\n",
      "weight[9.76966775], bias[10.7742711]\n",
      "Train loss: 28.22972705175687 \n",
      "\n",
      "Epoch 229 ============================\n",
      "weight[9.76945467], bias[10.77535187]\n",
      "Train loss: 28.229692774990685 \n",
      "\n",
      "Epoch 230 ============================\n",
      "weight[9.76924643], bias[10.77640811]\n",
      "Train loss: 28.229660035576984 \n",
      "\n",
      "Epoch 231 ============================\n",
      "weight[9.76904291], bias[10.7774404]\n",
      "Train loss: 28.229628764563863 \n",
      "\n",
      "Epoch 232 ============================\n",
      "weight[9.76884401], bias[10.77844928]\n",
      "Train loss: 28.229598896091733 \n",
      "\n",
      "Epoch 233 ============================\n",
      "weight[9.76864961], bias[10.77943526]\n",
      "Train loss: 28.229570367255082 \n",
      "\n",
      "Epoch 234 ============================\n",
      "weight[9.76845963], bias[10.78039889]\n",
      "Train loss: 28.229543117969705 \n",
      "\n",
      "Epoch 235 ============================\n",
      "weight[9.76827396], bias[10.78134066]\n",
      "Train loss: 28.22951709084623 \n",
      "\n",
      "Epoch 236 ============================\n",
      "weight[9.76809249], bias[10.78226106]\n",
      "Train loss: 28.229492231069287 \n",
      "\n",
      "Epoch 237 ============================\n",
      "weight[9.76791515], bias[10.78316059]\n",
      "Train loss: 28.229468486282038 \n",
      "\n",
      "Epoch 238 ============================\n",
      "weight[9.76774182], bias[10.78403971]\n",
      "Train loss: 28.229445806475926 \n",
      "\n",
      "Epoch 239 ============================\n",
      "weight[9.76757243], bias[10.78489889]\n",
      "Train loss: 28.22942414388533 \n",
      "\n",
      "Epoch 240 ============================\n",
      "weight[9.76740688], bias[10.78573859]\n",
      "Train loss: 28.229403452886853 \n",
      "\n",
      "Epoch 241 ============================\n",
      "weight[9.76724509], bias[10.78655924]\n",
      "Train loss: 28.22938368990362 \n",
      "\n",
      "Epoch 242 ============================\n",
      "weight[9.76708696], bias[10.78736127]\n",
      "Train loss: 28.229364813312998 \n",
      "\n",
      "Epoch 243 ============================\n",
      "weight[9.76693242], bias[10.78814511]\n",
      "Train loss: 28.22934678335925 \n",
      "\n",
      "Epoch 244 ============================\n",
      "weight[9.76678139], bias[10.78891117]\n",
      "Train loss: 28.229329562069704 \n",
      "\n",
      "Epoch 245 ============================\n",
      "weight[9.76663379], bias[10.78965985]\n",
      "Train loss: 28.229313113174914 \n",
      "\n",
      "Epoch 246 ============================\n",
      "weight[9.76648953], bias[10.79039155]\n",
      "Train loss: 28.229297402032113 \n",
      "\n",
      "Epoch 247 ============================\n",
      "weight[9.76634854], bias[10.79110666]\n",
      "Train loss: 28.229282395552072 \n",
      "\n",
      "Epoch 248 ============================\n",
      "weight[9.76621075], bias[10.79180554]\n",
      "Train loss: 28.229268062130053 \n",
      "\n",
      "Epoch 249 ============================\n",
      "weight[9.76607609], bias[10.79248857]\n",
      "Train loss: 28.22925437157855 \n",
      "\n",
      "Epoch 250 ============================\n",
      "weight[9.76594448], bias[10.79315611]\n",
      "Train loss: 28.229241295064092 \n",
      "\n",
      "Epoch 251 ============================\n",
      "weight[9.76581586], bias[10.79380851]\n",
      "Train loss: 28.229228805046414 \n",
      "\n",
      "Epoch 252 ============================\n",
      "weight[9.76569015], bias[10.79444611]\n",
      "Train loss: 28.22921687522047 \n",
      "\n",
      "Epoch 253 ============================\n",
      "weight[9.7655673], bias[10.79506924]\n",
      "Train loss: 28.229205480460955 \n",
      "\n",
      "Epoch 254 ============================\n",
      "weight[9.76544723], bias[10.79567825]\n",
      "Train loss: 28.229194596769624 \n",
      "\n",
      "Epoch 255 ============================\n",
      "weight[9.76532989], bias[10.79627343]\n",
      "Train loss: 28.229184201224363 \n",
      "\n",
      "Epoch 256 ============================\n",
      "weight[9.7652152], bias[10.79685512]\n",
      "Train loss: 28.229174271931306 \n",
      "\n",
      "Epoch 257 ============================\n",
      "weight[9.76510312], bias[10.79742361]\n",
      "Train loss: 28.229164787978497 \n",
      "\n",
      "Epoch 258 ============================\n",
      "weight[9.76499358], bias[10.79797921]\n",
      "Train loss: 28.22915572939194 \n",
      "\n",
      "Epoch 259 ============================\n",
      "weight[9.76488653], bias[10.79852221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.229147077093476 \n",
      "\n",
      "Epoch 260 ============================\n",
      "weight[9.7647819], bias[10.79905288]\n",
      "Train loss: 28.229138812860562 \n",
      "\n",
      "Epoch 261 ============================\n",
      "weight[9.76467965], bias[10.79957153]\n",
      "Train loss: 28.22913091928805 \n",
      "\n",
      "Epoch 262 ============================\n",
      "weight[9.76457972], bias[10.8000784]\n",
      "Train loss: 28.229123379751382 \n",
      "\n",
      "Epoch 263 ============================\n",
      "weight[9.76448205], bias[10.80057378]\n",
      "Train loss: 28.229116178371594 \n",
      "\n",
      "Epoch 264 ============================\n",
      "weight[9.7643866], bias[10.80105793]\n",
      "Train loss: 28.22910929998205 \n",
      "\n",
      "Epoch 265 ============================\n",
      "weight[9.76429331], bias[10.80153109]\n",
      "Train loss: 28.229102730096223 \n",
      "\n",
      "Epoch 266 ============================\n",
      "weight[9.76420214], bias[10.80199352]\n",
      "Train loss: 28.229096454877315 \n",
      "\n",
      "Epoch 267 ============================\n",
      "weight[9.76411304], bias[10.80244545]\n",
      "Train loss: 28.229090461109227 \n",
      "\n",
      "Epoch 268 ============================\n",
      "weight[9.76402596], bias[10.80288714]\n",
      "Train loss: 28.229084736168602 \n",
      "\n",
      "Epoch 269 ============================\n",
      "weight[9.76394086], bias[10.80331881]\n",
      "Train loss: 28.22907926799817 \n",
      "\n",
      "Epoch 270 ============================\n",
      "weight[9.76385768], bias[10.80374069]\n",
      "Train loss: 28.229074045081454 \n",
      "\n",
      "Epoch 271 ============================\n",
      "weight[9.76377639], bias[10.804153]\n",
      "Train loss: 28.22906905641857 \n",
      "\n",
      "Epoch 272 ============================\n",
      "weight[9.76369695], bias[10.80455595]\n",
      "Train loss: 28.229064291503025 \n",
      "\n",
      "Epoch 273 ============================\n",
      "weight[9.7636193], bias[10.80494977]\n",
      "Train loss: 28.229059740299412 \n",
      "\n",
      "Epoch 274 ============================\n",
      "weight[9.76354342], bias[10.80533465]\n",
      "Train loss: 28.229055393222602 \n",
      "\n",
      "Epoch 275 ============================\n",
      "weight[9.76346926], bias[10.80571081]\n",
      "Train loss: 28.22905124111714 \n",
      "\n",
      "Epoch 276 ============================\n",
      "weight[9.76339678], bias[10.80607843]\n",
      "Train loss: 28.229047275238436 \n",
      "\n",
      "Epoch 277 ============================\n",
      "weight[9.76332595], bias[10.80643771]\n",
      "Train loss: 28.22904348723398 \n",
      "\n",
      "Epoch 278 ============================\n",
      "weight[9.76325672], bias[10.80678884]\n",
      "Train loss: 28.229039869125867 \n",
      "\n",
      "Epoch 279 ============================\n",
      "weight[9.76318906], bias[10.80713201]\n",
      "Train loss: 28.229036413294036 \n",
      "\n",
      "Epoch 280 ============================\n",
      "weight[9.76312294], bias[10.80746739]\n",
      "Train loss: 28.229033112460286 \n",
      "\n",
      "Epoch 281 ============================\n",
      "weight[9.76305832], bias[10.80779517]\n",
      "Train loss: 28.229029959672747 \n",
      "\n",
      "Epoch 282 ============================\n",
      "weight[9.76299516], bias[10.80811551]\n",
      "Train loss: 28.229026948291384 \n",
      "\n",
      "Epoch 283 ============================\n",
      "weight[9.76293344], bias[10.80842859]\n",
      "Train loss: 28.229024071973974 \n",
      "\n",
      "Epoch 284 ============================\n",
      "weight[9.76287311], bias[10.80873456]\n",
      "Train loss: 28.22902132466266 \n",
      "\n",
      "Epoch 285 ============================\n",
      "weight[9.76281416], bias[10.80903359]\n",
      "Train loss: 28.229018700571515 \n",
      "\n",
      "Epoch 286 ============================\n",
      "weight[9.76275654], bias[10.80932584]\n",
      "Train loss: 28.229016194173788 \n",
      "\n",
      "Epoch 287 ============================\n",
      "weight[9.76270023], bias[10.80961147]\n",
      "Train loss: 28.229013800190987 \n",
      "\n",
      "Epoch 288 ============================\n",
      "weight[9.76264519], bias[10.80989061]\n",
      "Train loss: 28.229011513581042 \n",
      "\n",
      "Epoch 289 ============================\n",
      "weight[9.76259141], bias[10.81016342]\n",
      "Train loss: 28.229009329528157 \n",
      "\n",
      "Epoch 290 ============================\n",
      "weight[9.76253884], bias[10.81043004]\n",
      "Train loss: 28.22900724343257 \n",
      "\n",
      "Epoch 291 ============================\n",
      "weight[9.76248747], bias[10.81069062]\n",
      "Train loss: 28.229005250900848 \n",
      "\n",
      "Epoch 292 ============================\n",
      "weight[9.76243726], bias[10.81094528]\n",
      "Train loss: 28.22900334773641 \n",
      "\n",
      "Epoch 293 ============================\n",
      "weight[9.76238819], bias[10.81119417]\n",
      "Train loss: 28.229001529931093 \n",
      "\n",
      "Epoch 294 ============================\n",
      "weight[9.76234023], bias[10.81143741]\n",
      "Train loss: 28.228999793656442 \n",
      "\n",
      "Epoch 295 ============================\n",
      "weight[9.76229336], bias[10.81167514]\n",
      "Train loss: 28.22899813525574 \n",
      "\n",
      "Epoch 296 ============================\n",
      "weight[9.76224756], bias[10.81190747]\n",
      "Train loss: 28.228996551236257 \n",
      "\n",
      "Epoch 297 ============================\n",
      "weight[9.76220279], bias[10.81213453]\n",
      "Train loss: 28.22899503826185 \n",
      "\n",
      "Epoch 298 ============================\n",
      "weight[9.76215904], bias[10.81235645]\n",
      "Train loss: 28.228993593146033 \n",
      "\n",
      "Epoch 299 ============================\n",
      "weight[9.76211628], bias[10.81257332]\n",
      "Train loss: 28.22899221284538 \n",
      "\n",
      "Epoch 300 ============================\n",
      "weight[9.76207449], bias[10.81278528]\n",
      "Train loss: 28.228990894452842 \n",
      "\n",
      "Epoch 301 ============================\n",
      "weight[9.76203365], bias[10.81299244]\n",
      "Train loss: 28.228989635191677 \n",
      "\n",
      "Epoch 302 ============================\n",
      "weight[9.76199374], bias[10.81319489]\n",
      "Train loss: 28.228988432409828 \n",
      "\n",
      "Epoch 303 ============================\n",
      "weight[9.76195473], bias[10.81339275]\n",
      "Train loss: 28.228987283574174 \n",
      "\n",
      "Epoch 304 ============================\n",
      "weight[9.76191661], bias[10.81358612]\n",
      "Train loss: 28.228986186265054 \n",
      "\n",
      "Epoch 305 ============================\n",
      "weight[9.76187935], bias[10.81377511]\n",
      "Train loss: 28.228985138171602 \n",
      "\n",
      "Epoch 306 ============================\n",
      "weight[9.76184293], bias[10.81395981]\n",
      "Train loss: 28.228984137086307 \n",
      "\n",
      "Epoch 307 ============================\n",
      "weight[9.76180734], bias[10.81414032]\n",
      "Train loss: 28.22898318090083 \n",
      "\n",
      "Epoch 308 ============================\n",
      "weight[9.76177256], bias[10.81431673]\n",
      "Train loss: 28.228982267601367 \n",
      "\n",
      "Epoch 309 ============================\n",
      "weight[9.76173857], bias[10.81448915]\n",
      "Train loss: 28.22898139526448 \n",
      "\n",
      "Epoch 310 ============================\n",
      "weight[9.76170535], bias[10.81465765]\n",
      "Train loss: 28.22898056205293 \n",
      "\n",
      "Epoch 311 ============================\n",
      "weight[9.76167288], bias[10.81482233]\n",
      "Train loss: 28.228979766211825 \n",
      "\n",
      "Epoch 312 ============================\n",
      "weight[9.76164115], bias[10.81498328]\n",
      "Train loss: 28.228979006065135 \n",
      "\n",
      "Epoch 313 ============================\n",
      "weight[9.76161014], bias[10.81514057]\n",
      "Train loss: 28.228978280011898 \n",
      "\n",
      "Epoch 314 ============================\n",
      "weight[9.76157983], bias[10.8152943]\n",
      "Train loss: 28.228977586523012 \n",
      "\n",
      "Epoch 315 ============================\n",
      "weight[9.76155021], bias[10.81544454]\n",
      "Train loss: 28.22897692413783 \n",
      "\n",
      "Epoch 316 ============================\n",
      "weight[9.76152126], bias[10.81559137]\n",
      "Train loss: 28.228976291461475 \n",
      "\n",
      "Epoch 317 ============================\n",
      "weight[9.76149297], bias[10.81573487]\n",
      "Train loss: 28.228975687161306 \n",
      "\n",
      "Epoch 318 ============================\n",
      "weight[9.76146532], bias[10.81587512]\n",
      "Train loss: 28.228975109964754 \n",
      "\n",
      "Epoch 319 ============================\n",
      "weight[9.7614383], bias[10.81601218]\n",
      "Train loss: 28.2289745586561 \n",
      "\n",
      "Epoch 320 ============================\n",
      "weight[9.76141189], bias[10.81614614]\n",
      "Train loss: 28.228974032074298 \n",
      "\n",
      "Epoch 321 ============================\n",
      "weight[9.76138607], bias[10.81627706]\n",
      "Train loss: 28.228973529110323 \n",
      "\n",
      "Epoch 322 ============================\n",
      "weight[9.76136085], bias[10.81640501]\n",
      "Train loss: 28.228973048704805 \n",
      "\n",
      "Epoch 323 ============================\n",
      "weight[9.7613362], bias[10.81653005]\n",
      "Train loss: 28.228972589846048 \n",
      "\n",
      "Epoch 324 ============================\n",
      "weight[9.7613121], bias[10.81665226]\n",
      "Train loss: 28.228972151567696 \n",
      "\n",
      "Epoch 325 ============================\n",
      "weight[9.76128855], bias[10.8167717]\n",
      "Train loss: 28.22897173294653 \n",
      "\n",
      "Epoch 326 ============================\n",
      "weight[9.76126554], bias[10.81688843]\n",
      "Train loss: 28.228971333101068 \n",
      "\n",
      "Epoch 327 ============================\n",
      "weight[9.76124305], bias[10.81700251]\n",
      "Train loss: 28.228970951189158 \n",
      "\n",
      "Epoch 328 ============================\n",
      "weight[9.76122107], bias[10.817114]\n",
      "Train loss: 28.22897058640646 \n",
      "\n",
      "Epoch 329 ============================\n",
      "weight[9.76119958], bias[10.81722296]\n",
      "Train loss: 28.22897023798461 \n",
      "\n",
      "Epoch 330 ============================\n",
      "weight[9.76117859], bias[10.81732946]\n",
      "Train loss: 28.22896990518988 \n",
      "\n",
      "Epoch 331 ============================\n",
      "weight[9.76115807], bias[10.81743353]\n",
      "Train loss: 28.22896958732142 \n",
      "\n",
      "Epoch 332 ============================\n",
      "weight[9.76113802], bias[10.81753525]\n",
      "Train loss: 28.228969283709734 \n",
      "\n",
      "Epoch 333 ============================\n",
      "weight[9.76111842], bias[10.81763466]\n",
      "Train loss: 28.22896899371531 \n",
      "\n",
      "Epoch 334 ============================\n",
      "weight[9.76109926], bias[10.81773181]\n",
      "Train loss: 28.22896871672757 \n",
      "\n",
      "Epoch 335 ============================\n",
      "weight[9.76108054], bias[10.81782676]\n",
      "Train loss: 28.22896845216295 \n",
      "\n",
      "Epoch 336 ============================\n",
      "weight[9.76106225], bias[10.81791956]\n",
      "Train loss: 28.22896819946448 \n",
      "\n",
      "Epoch 337 ============================\n",
      "weight[9.76104437], bias[10.81801025]\n",
      "Train loss: 28.22896795809973 \n",
      "\n",
      "Epoch 338 ============================\n",
      "weight[9.76102689], bias[10.81809889]\n",
      "Train loss: 28.22896772756053 \n",
      "\n",
      "Epoch 339 ============================\n",
      "weight[9.76100981], bias[10.81818551]\n",
      "Train loss: 28.22896750736118 \n",
      "\n",
      "Epoch 340 ============================\n",
      "weight[9.76099312], bias[10.81827017]\n",
      "Train loss: 28.228967297038142 \n",
      "\n",
      "Epoch 341 ============================\n",
      "weight[9.76097681], bias[10.81835291]\n",
      "Train loss: 28.228967096148263 \n",
      "\n",
      "Epoch 342 ============================\n",
      "weight[9.76096087], bias[10.81843377]\n",
      "Train loss: 28.22896690426855 \n",
      "\n",
      "Epoch 343 ============================\n",
      "weight[9.76094529], bias[10.8185128]\n",
      "Train loss: 28.22896672099486 \n",
      "\n",
      "Epoch 344 ============================\n",
      "weight[9.76093006], bias[10.81859003]\n",
      "Train loss: 28.228966545941244 \n",
      "\n",
      "Epoch 345 ============================\n",
      "weight[9.76091518], bias[10.81866552]\n",
      "Train loss: 28.228966378738924 \n",
      "\n",
      "Epoch 346 ============================\n",
      "weight[9.76090063], bias[10.81873929]\n",
      "Train loss: 28.228966219035822 \n",
      "\n",
      "Epoch 347 ============================\n",
      "weight[9.76088642], bias[10.81881138]\n",
      "Train loss: 28.228966066495612 \n",
      "\n",
      "Epoch 348 ============================\n",
      "weight[9.76087253], bias[10.81888185]\n",
      "Train loss: 28.228965920797005 \n",
      "\n",
      "Epoch 349 ============================\n",
      "weight[9.76085895], bias[10.81895071]\n",
      "Train loss: 28.228965781633235 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350 ============================\n",
      "weight[9.76084568], bias[10.81901801]\n",
      "Train loss: 28.228965648710986 \n",
      "\n",
      "Epoch 351 ============================\n",
      "weight[9.76083271], bias[10.81908379]\n",
      "Train loss: 28.228965521750503 \n",
      "\n",
      "Epoch 352 ============================\n",
      "weight[9.76082004], bias[10.81914807]\n",
      "Train loss: 28.228965400484356 \n",
      "\n",
      "Epoch 353 ============================\n",
      "weight[9.76080765], bias[10.8192109]\n",
      "Train loss: 28.228965284657143 \n",
      "\n",
      "Epoch 354 ============================\n",
      "weight[9.76079555], bias[10.8192723]\n",
      "Train loss: 28.228965174024886 \n",
      "\n",
      "Epoch 355 ============================\n",
      "weight[9.76078372], bias[10.81933231]\n",
      "Train loss: 28.228965068354633 \n",
      "\n",
      "Epoch 356 ============================\n",
      "weight[9.76077215], bias[10.81939095]\n",
      "Train loss: 28.228964967423792 \n",
      "\n",
      "Epoch 357 ============================\n",
      "weight[9.76076085], bias[10.81944827]\n",
      "Train loss: 28.228964871019823 \n",
      "\n",
      "Epoch 358 ============================\n",
      "weight[9.76074981], bias[10.81950429]\n",
      "Train loss: 28.228964778939712 \n",
      "\n",
      "Epoch 359 ============================\n",
      "weight[9.76073902], bias[10.81955903]\n",
      "Train loss: 28.228964690989454 \n",
      "\n",
      "Epoch 360 ============================\n",
      "weight[9.76072847], bias[10.81961253]\n",
      "Train loss: 28.228964606983904 \n",
      "\n",
      "Epoch 361 ============================\n",
      "weight[9.76071816], bias[10.81966482]\n",
      "Train loss: 28.228964526746047 \n",
      "\n",
      "Epoch 362 ============================\n",
      "weight[9.76070808], bias[10.81971593]\n",
      "Train loss: 28.228964450107014 \n",
      "\n",
      "Epoch 363 ============================\n",
      "weight[9.76069824], bias[10.81976587]\n",
      "Train loss: 28.228964376905278 \n",
      "\n",
      "Epoch 364 ============================\n",
      "weight[9.76068861], bias[10.81981469]\n",
      "Train loss: 28.22896430698675 \n",
      "\n",
      "Epoch 365 ============================\n",
      "weight[9.76067921], bias[10.81986239]\n",
      "Train loss: 28.22896424020419 \n",
      "\n",
      "Epoch 366 ============================\n",
      "weight[9.76067002], bias[10.81990901]\n",
      "Train loss: 28.228964176416834 \n",
      "\n",
      "Epoch 367 ============================\n",
      "weight[9.76066103], bias[10.81995458]\n",
      "Train loss: 28.22896411549049 \n",
      "\n",
      "Epoch 368 ============================\n",
      "weight[9.76065225], bias[10.81999911]\n",
      "Train loss: 28.228964057296704 \n",
      "\n",
      "Epoch 369 ============================\n",
      "weight[9.76064367], bias[10.82004263]\n",
      "Train loss: 28.22896400171296 \n",
      "\n",
      "Epoch 370 ============================\n",
      "weight[9.76063529], bias[10.82008517]\n",
      "Train loss: 28.228963948622226 \n",
      "\n",
      "Epoch 371 ============================\n",
      "weight[9.76062709], bias[10.82012674]\n",
      "Train loss: 28.228963897912738 \n",
      "\n",
      "Epoch 372 ============================\n",
      "weight[9.76061908], bias[10.82016736]\n",
      "Train loss: 28.228963849477566 \n",
      "\n",
      "Epoch 373 ============================\n",
      "weight[9.76061125], bias[10.82020707]\n",
      "Train loss: 28.228963803214786 \n",
      "\n",
      "Epoch 374 ============================\n",
      "weight[9.7606036], bias[10.82024587]\n",
      "Train loss: 28.22896375902696 \n",
      "\n",
      "Epoch 375 ============================\n",
      "weight[9.76059613], bias[10.8202838]\n",
      "Train loss: 28.228963716820992 \n",
      "\n",
      "Epoch 376 ============================\n",
      "weight[9.76058882], bias[10.82032086]\n",
      "Train loss: 28.228963676507988 \n",
      "\n",
      "Epoch 377 ============================\n",
      "weight[9.76058168], bias[10.82035708]\n",
      "Train loss: 28.22896363800309 \n",
      "\n",
      "Epoch 378 ============================\n",
      "weight[9.7605747], bias[10.82039248]\n",
      "Train loss: 28.228963601225207 \n",
      "\n",
      "Epoch 379 ============================\n",
      "weight[9.76056788], bias[10.82042708]\n",
      "Train loss: 28.22896356609681 \n",
      "\n",
      "Epoch 380 ============================\n",
      "weight[9.76056121], bias[10.8204609]\n",
      "Train loss: 28.22896353254397 \n",
      "\n",
      "Epoch 381 ============================\n",
      "weight[9.76055469], bias[10.82049394]\n",
      "Train loss: 28.228963500496043 \n",
      "\n",
      "Epoch 382 ============================\n",
      "weight[9.76054833], bias[10.82052624]\n",
      "Train loss: 28.228963469885485 \n",
      "\n",
      "Epoch 383 ============================\n",
      "weight[9.7605421], bias[10.82055781]\n",
      "Train loss: 28.228963440647835 \n",
      "\n",
      "Epoch 384 ============================\n",
      "weight[9.76053602], bias[10.82058865]\n",
      "Train loss: 28.228963412721555 \n",
      "\n",
      "Epoch 385 ============================\n",
      "weight[9.76053008], bias[10.8206188]\n",
      "Train loss: 28.22896338604773 \n",
      "\n",
      "Epoch 386 ============================\n",
      "weight[9.76052427], bias[10.82064827]\n",
      "Train loss: 28.228963360570333 \n",
      "\n",
      "Epoch 387 ============================\n",
      "weight[9.76051859], bias[10.82067707]\n",
      "Train loss: 28.228963336235616 \n",
      "\n",
      "Epoch 388 ============================\n",
      "weight[9.76051304], bias[10.82070521]\n",
      "Train loss: 28.228963312992338 \n",
      "\n",
      "Epoch 389 ============================\n",
      "weight[9.76050762], bias[10.82073271]\n",
      "Train loss: 28.228963290791487 \n",
      "\n",
      "Epoch 390 ============================\n",
      "weight[9.76050232], bias[10.8207596]\n",
      "Train loss: 28.228963269586433 \n",
      "\n",
      "Epoch 391 ============================\n",
      "weight[9.76049714], bias[10.82078587]\n",
      "Train loss: 28.228963249332427 \n",
      "\n",
      "Epoch 392 ============================\n",
      "weight[9.76049208], bias[10.82081154]\n",
      "Train loss: 28.228963229986864 \n",
      "\n",
      "Epoch 393 ============================\n",
      "weight[9.76048713], bias[10.82083664]\n",
      "Train loss: 28.228963211508926 \n",
      "\n",
      "Epoch 394 ============================\n",
      "weight[9.7604823], bias[10.82086116]\n",
      "Train loss: 28.228963193859766 \n",
      "\n",
      "Epoch 395 ============================\n",
      "weight[9.76047757], bias[10.82088513]\n",
      "Train loss: 28.228963177002242 \n",
      "\n",
      "Epoch 396 ============================\n",
      "weight[9.76047295], bias[10.82090855]\n",
      "Train loss: 28.22896316090074 \n",
      "\n",
      "Epoch 397 ============================\n",
      "weight[9.76046844], bias[10.82093144]\n",
      "Train loss: 28.228963145521444 \n",
      "\n",
      "Epoch 398 ============================\n",
      "weight[9.76046403], bias[10.82095382]\n",
      "Train loss: 28.228963130831875 \n",
      "\n",
      "Epoch 399 ============================\n",
      "weight[9.76045972], bias[10.82097568]\n",
      "Train loss: 28.228963116801214 \n",
      "\n",
      "Epoch 400 ============================\n",
      "weight[9.7604555], bias[10.82099705]\n",
      "Train loss: 28.228963103399774 \n",
      "\n",
      "Epoch 401 ============================\n",
      "weight[9.76045139], bias[10.82101794]\n",
      "Train loss: 28.228963090599425 \n",
      "\n",
      "Epoch 402 ============================\n",
      "weight[9.76044736], bias[10.82103835]\n",
      "Train loss: 28.22896307837322 \n",
      "\n",
      "Epoch 403 ============================\n",
      "weight[9.76044343], bias[10.8210583]\n",
      "Train loss: 28.22896306669534 \n",
      "\n",
      "Epoch 404 ============================\n",
      "weight[9.76043959], bias[10.8210778]\n",
      "Train loss: 28.228963055541254 \n",
      "\n",
      "Epoch 405 ============================\n",
      "weight[9.76043583], bias[10.82109685]\n",
      "Train loss: 28.228963044887443 \n",
      "\n",
      "Epoch 406 ============================\n",
      "weight[9.76043216], bias[10.82111547]\n",
      "Train loss: 28.228963034711455 \n",
      "\n",
      "Epoch 407 ============================\n",
      "weight[9.76042857], bias[10.82113367]\n",
      "Train loss: 28.228963024991895 \n",
      "\n",
      "Epoch 408 ============================\n",
      "weight[9.76042506], bias[10.82115146]\n",
      "Train loss: 28.228963015708228 \n",
      "\n",
      "Epoch 409 ============================\n",
      "weight[9.76042164], bias[10.82116884]\n",
      "Train loss: 28.22896300684094 \n",
      "\n",
      "Epoch 410 ============================\n",
      "weight[9.76041829], bias[10.82118583]\n",
      "Train loss: 28.22896299837136 \n",
      "\n",
      "Epoch 411 ============================\n",
      "weight[9.76041501], bias[10.82120243]\n",
      "Train loss: 28.22896299028168 \n",
      "\n",
      "Epoch 412 ============================\n",
      "weight[9.76041181], bias[10.82121866]\n",
      "Train loss: 28.22896298255485 \n",
      "\n",
      "Epoch 413 ============================\n",
      "weight[9.76040869], bias[10.82123452]\n",
      "Train loss: 28.22896297517455 \n",
      "\n",
      "Epoch 414 ============================\n",
      "weight[9.76040563], bias[10.82125002]\n",
      "Train loss: 28.22896296812528 \n",
      "\n",
      "Epoch 415 ============================\n",
      "weight[9.76040264], bias[10.82126516]\n",
      "Train loss: 28.228962961392142 \n",
      "\n",
      "Epoch 416 ============================\n",
      "weight[9.76039973], bias[10.82127997]\n",
      "Train loss: 28.22896295496101 \n",
      "\n",
      "Epoch 417 ============================\n",
      "weight[9.76039687], bias[10.82129444]\n",
      "Train loss: 28.22896294881836 \n",
      "\n",
      "Epoch 418 ============================\n",
      "weight[9.76039409], bias[10.82130858]\n",
      "Train loss: 28.22896294295119 \n",
      "\n",
      "Epoch 419 ============================\n",
      "weight[9.76039136], bias[10.82132239]\n",
      "Train loss: 28.22896293734713 \n",
      "\n",
      "Epoch 420 ============================\n",
      "weight[9.7603887], bias[10.8213359]\n",
      "Train loss: 28.22896293199445 \n",
      "\n",
      "Epoch 421 ============================\n",
      "weight[9.7603861], bias[10.8213491]\n",
      "Train loss: 28.22896292688186 \n",
      "\n",
      "Epoch 422 ============================\n",
      "weight[9.76038355], bias[10.821362]\n",
      "Train loss: 28.22896292199854 \n",
      "\n",
      "Epoch 423 ============================\n",
      "weight[9.76038107], bias[10.82137461]\n",
      "Train loss: 28.22896291733429 \n",
      "\n",
      "Epoch 424 ============================\n",
      "weight[9.76037864], bias[10.82138693]\n",
      "Train loss: 28.22896291287915 \n",
      "\n",
      "Epoch 425 ============================\n",
      "weight[9.76037626], bias[10.82139897]\n",
      "Train loss: 28.228962908623934 \n",
      "\n",
      "Epoch 426 ============================\n",
      "weight[9.76037394], bias[10.82141074]\n",
      "Train loss: 28.22896290455952 \n",
      "\n",
      "Epoch 427 ============================\n",
      "weight[9.76037168], bias[10.82142224]\n",
      "Train loss: 28.228962900677367 \n",
      "\n",
      "Epoch 428 ============================\n",
      "weight[9.76036946], bias[10.82143348]\n",
      "Train loss: 28.228962896969353 \n",
      "\n",
      "Epoch 429 ============================\n",
      "weight[9.76036729], bias[10.82144447]\n",
      "Train loss: 28.228962893427706 \n",
      "\n",
      "Epoch 430 ============================\n",
      "weight[9.76036518], bias[10.8214552]\n",
      "Train loss: 28.228962890044837 \n",
      "\n",
      "Epoch 431 ============================\n",
      "weight[9.76036311], bias[10.8214657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228962886813722 \n",
      "\n",
      "Epoch 432 ============================\n",
      "weight[9.76036109], bias[10.82147595]\n",
      "Train loss: 28.228962883727572 \n",
      "\n",
      "Epoch 433 ============================\n",
      "weight[9.76035911], bias[10.82148598]\n",
      "Train loss: 28.228962880779754 \n",
      "\n",
      "Epoch 434 ============================\n",
      "weight[9.76035718], bias[10.82149577]\n",
      "Train loss: 28.22896287796417 \n",
      "\n",
      "Epoch 435 ============================\n",
      "weight[9.76035529], bias[10.82150534]\n",
      "Train loss: 28.228962875274892 \n",
      "\n",
      "Epoch 436 ============================\n",
      "weight[9.76035345], bias[10.8215147]\n",
      "Train loss: 28.22896287270626 \n",
      "\n",
      "Epoch 437 ============================\n",
      "weight[9.76035164], bias[10.82152384]\n",
      "Train loss: 28.228962870252698 \n",
      "\n",
      "Epoch 438 ============================\n",
      "weight[9.76034988], bias[10.82153278]\n",
      "Train loss: 28.22896286790931 \n",
      "\n",
      "Epoch 439 ============================\n",
      "weight[9.76034816], bias[10.82154151]\n",
      "Train loss: 28.22896286567103 \n",
      "\n",
      "Epoch 440 ============================\n",
      "weight[9.76034648], bias[10.82155005]\n",
      "Train loss: 28.228962863533095 \n",
      "\n",
      "Epoch 441 ============================\n",
      "weight[9.76034483], bias[10.82155839]\n",
      "Train loss: 28.228962861491084 \n",
      "\n",
      "Epoch 442 ============================\n",
      "weight[9.76034323], bias[10.82156654]\n",
      "Train loss: 28.22896285954064 \n",
      "\n",
      "Epoch 443 ============================\n",
      "weight[9.76034166], bias[10.82157451]\n",
      "Train loss: 28.22896285767765 \n",
      "\n",
      "Epoch 444 ============================\n",
      "weight[9.76034012], bias[10.8215823]\n",
      "Train loss: 28.228962855898253 \n",
      "\n",
      "Epoch 445 ============================\n",
      "weight[9.76033862], bias[10.82158991]\n",
      "Train loss: 28.228962854198635 \n",
      "\n",
      "Epoch 446 ============================\n",
      "weight[9.76033715], bias[10.82159735]\n",
      "Train loss: 28.228962852575243 \n",
      "\n",
      "Epoch 447 ============================\n",
      "weight[9.76033572], bias[10.82160461]\n",
      "Train loss: 28.22896285102467 \n",
      "\n",
      "Epoch 448 ============================\n",
      "weight[9.76033432], bias[10.82161172]\n",
      "Train loss: 28.228962849543677 \n",
      "\n",
      "Epoch 449 ============================\n",
      "weight[9.76033295], bias[10.82161866]\n",
      "Train loss: 28.228962848129076 \n",
      "\n",
      "Epoch 450 ============================\n",
      "weight[9.76033161], bias[10.82162545]\n",
      "Train loss: 28.228962846777904 \n",
      "\n",
      "Epoch 451 ============================\n",
      "weight[9.76033031], bias[10.82163208]\n",
      "Train loss: 28.228962845487363 \n",
      "\n",
      "Epoch 452 ============================\n",
      "weight[9.76032903], bias[10.82163856]\n",
      "Train loss: 28.22896284425474 \n",
      "\n",
      "Epoch 453 ============================\n",
      "weight[9.76032778], bias[10.82164489]\n",
      "Train loss: 28.228962843077362 \n",
      "\n",
      "Epoch 454 ============================\n",
      "weight[9.76032656], bias[10.82165108]\n",
      "Train loss: 28.228962841952768 \n",
      "\n",
      "Epoch 455 ============================\n",
      "weight[9.76032537], bias[10.82165713]\n",
      "Train loss: 28.228962840878687 \n",
      "\n",
      "Epoch 456 ============================\n",
      "weight[9.7603242], bias[10.82166305]\n",
      "Train loss: 28.228962839852645 \n",
      "\n",
      "Epoch 457 ============================\n",
      "weight[9.76032306], bias[10.82166883]\n",
      "Train loss: 28.22896283887272 \n",
      "\n",
      "Epoch 458 ============================\n",
      "weight[9.76032195], bias[10.82167447]\n",
      "Train loss: 28.228962837936717 \n",
      "\n",
      "Epoch 459 ============================\n",
      "weight[9.76032086], bias[10.82167999]\n",
      "Train loss: 28.228962837042705 \n",
      "\n",
      "Epoch 460 ============================\n",
      "weight[9.7603198], bias[10.82168539]\n",
      "Train loss: 28.22896283618879 \n",
      "\n",
      "Epoch 461 ============================\n",
      "weight[9.76031876], bias[10.82169066]\n",
      "Train loss: 28.228962835373178 \n",
      "\n",
      "Epoch 462 ============================\n",
      "weight[9.76031774], bias[10.82169581]\n",
      "Train loss: 28.22896283459417 \n",
      "\n",
      "Epoch 463 ============================\n",
      "weight[9.76031675], bias[10.82170085]\n",
      "Train loss: 28.228962833850062 \n",
      "\n",
      "Epoch 464 ============================\n",
      "weight[9.76031578], bias[10.82170577]\n",
      "Train loss: 28.22896283313938 \n",
      "\n",
      "Epoch 465 ============================\n",
      "weight[9.76031483], bias[10.82171058]\n",
      "Train loss: 28.228962832460518 \n",
      "\n",
      "Epoch 466 ============================\n",
      "weight[9.7603139], bias[10.82171528]\n",
      "Train loss: 28.22896283181214 \n",
      "\n",
      "Epoch 467 ============================\n",
      "weight[9.760313], bias[10.82171987]\n",
      "Train loss: 28.228962831192813 \n",
      "\n",
      "Epoch 468 ============================\n",
      "weight[9.76031211], bias[10.82172436]\n",
      "Train loss: 28.228962830601265 \n",
      "\n",
      "Epoch 469 ============================\n",
      "weight[9.76031125], bias[10.82172875]\n",
      "Train loss: 28.22896283003625 \n",
      "\n",
      "Epoch 470 ============================\n",
      "weight[9.7603104], bias[10.82173304]\n",
      "Train loss: 28.22896282949663 \n",
      "\n",
      "Epoch 471 ============================\n",
      "weight[9.76030957], bias[10.82173723]\n",
      "Train loss: 28.228962828981135 \n",
      "\n",
      "Epoch 472 ============================\n",
      "weight[9.76030877], bias[10.82174133]\n",
      "Train loss: 28.22896282848877 \n",
      "\n",
      "Epoch 473 ============================\n",
      "weight[9.76030798], bias[10.82174533]\n",
      "Train loss: 28.22896282801857 \n",
      "\n",
      "Epoch 474 ============================\n",
      "weight[9.76030721], bias[10.82174924]\n",
      "Train loss: 28.228962827569394 \n",
      "\n",
      "Epoch 475 ============================\n",
      "weight[9.76030645], bias[10.82175306]\n",
      "Train loss: 28.22896282714038 \n",
      "\n",
      "Epoch 476 ============================\n",
      "weight[9.76030572], bias[10.8217568]\n",
      "Train loss: 28.228962826730562 \n",
      "\n",
      "Epoch 477 ============================\n",
      "weight[9.760305], bias[10.82176045]\n",
      "Train loss: 28.22896282633914 \n",
      "\n",
      "Epoch 478 ============================\n",
      "weight[9.76030429], bias[10.82176402]\n",
      "Train loss: 28.228962825965343 \n",
      "\n",
      "Epoch 479 ============================\n",
      "weight[9.7603036], bias[10.82176751]\n",
      "Train loss: 28.228962825608246 \n",
      "\n",
      "Epoch 480 ============================\n",
      "weight[9.76030293], bias[10.82177092]\n",
      "Train loss: 28.22896282526718 \n",
      "\n",
      "Epoch 481 ============================\n",
      "weight[9.76030228], bias[10.82177425]\n",
      "Train loss: 28.228962824941423 \n",
      "\n",
      "Epoch 482 ============================\n",
      "weight[9.76030163], bias[10.82177751]\n",
      "Train loss: 28.22896282463027 \n",
      "\n",
      "Epoch 483 ============================\n",
      "weight[9.76030101], bias[10.82178069]\n",
      "Train loss: 28.22896282433306 \n",
      "\n",
      "Epoch 484 ============================\n",
      "weight[9.76030039], bias[10.8217838]\n",
      "Train loss: 28.2289628240492 \n",
      "\n",
      "Epoch 485 ============================\n",
      "weight[9.76029979], bias[10.82178684]\n",
      "Train loss: 28.228962823778065 \n",
      "\n",
      "Epoch 486 ============================\n",
      "weight[9.76029921], bias[10.82178981]\n",
      "Train loss: 28.228962823519073 \n",
      "\n",
      "Epoch 487 ============================\n",
      "weight[9.76029864], bias[10.82179271]\n",
      "Train loss: 28.22896282327169 \n",
      "\n",
      "Epoch 488 ============================\n",
      "weight[9.76029808], bias[10.82179555]\n",
      "Train loss: 28.22896282303547 \n",
      "\n",
      "Epoch 489 ============================\n",
      "weight[9.76029753], bias[10.82179833]\n",
      "Train loss: 28.228962822809784 \n",
      "\n",
      "Epoch 490 ============================\n",
      "weight[9.76029699], bias[10.82180104]\n",
      "Train loss: 28.22896282259426 \n",
      "\n",
      "Epoch 491 ============================\n",
      "weight[9.76029647], bias[10.82180368]\n",
      "Train loss: 28.22896282238833 \n",
      "\n",
      "Epoch 492 ============================\n",
      "weight[9.76029596], bias[10.82180627]\n",
      "Train loss: 28.228962822191736 \n",
      "\n",
      "Epoch 493 ============================\n",
      "weight[9.76029546], bias[10.8218088]\n",
      "Train loss: 28.2289628220039 \n",
      "\n",
      "Epoch 494 ============================\n",
      "weight[9.76029498], bias[10.82181128]\n",
      "Train loss: 28.228962821824457 \n",
      "\n",
      "Epoch 495 ============================\n",
      "weight[9.7602945], bias[10.82181369]\n",
      "Train loss: 28.228962821653088 \n",
      "\n",
      "Epoch 496 ============================\n",
      "weight[9.76029403], bias[10.82181605]\n",
      "Train loss: 28.228962821489425 \n",
      "\n",
      "Epoch 497 ============================\n",
      "weight[9.76029358], bias[10.82181836]\n",
      "Train loss: 28.22896282133311 \n",
      "\n",
      "Epoch 498 ============================\n",
      "weight[9.76029313], bias[10.82182062]\n",
      "Train loss: 28.228962821183806 \n",
      "\n",
      "Epoch 499 ============================\n",
      "weight[9.7602927], bias[10.82182282]\n",
      "Train loss: 28.228962821041197 \n",
      "\n",
      "Epoch 500 ============================\n",
      "weight[9.76029227], bias[10.82182498]\n",
      "Train loss: 28.22896282090495 \n",
      "\n",
      "Epoch 501 ============================\n",
      "weight[9.76029186], bias[10.82182708]\n",
      "Train loss: 28.228962820774857 \n",
      "\n",
      "Epoch 502 ============================\n",
      "weight[9.76029145], bias[10.82182914]\n",
      "Train loss: 28.228962820650548 \n",
      "\n",
      "Epoch 503 ============================\n",
      "weight[9.76029106], bias[10.82183115]\n",
      "Train loss: 28.228962820531855 \n",
      "\n",
      "Epoch 504 ============================\n",
      "weight[9.76029067], bias[10.82183312]\n",
      "Train loss: 28.228962820418474 \n",
      "\n",
      "Epoch 505 ============================\n",
      "weight[9.76029029], bias[10.82183504]\n",
      "Train loss: 28.228962820310198 \n",
      "\n",
      "Epoch 506 ============================\n",
      "weight[9.76028992], bias[10.82183692]\n",
      "Train loss: 28.22896282020674 \n",
      "\n",
      "Epoch 507 ============================\n",
      "weight[9.76028956], bias[10.82183875]\n",
      "Train loss: 28.228962820107938 \n",
      "\n",
      "Epoch 508 ============================\n",
      "weight[9.76028921], bias[10.82184054]\n",
      "Train loss: 28.228962820013574 \n",
      "\n",
      "Epoch 509 ============================\n",
      "weight[9.76028886], bias[10.8218423]\n",
      "Train loss: 28.228962819923424 \n",
      "\n",
      "Epoch 510 ============================\n",
      "weight[9.76028852], bias[10.82184401]\n",
      "Train loss: 28.22896281983736 \n",
      "\n",
      "Epoch 511 ============================\n",
      "weight[9.76028819], bias[10.82184568]\n",
      "Train loss: 28.228962819755086 \n",
      "\n",
      "Epoch 512 ============================\n",
      "weight[9.76028787], bias[10.82184732]\n",
      "Train loss: 28.22896281967656 \n",
      "\n",
      "Epoch 513 ============================\n",
      "weight[9.76028755], bias[10.82184892]\n",
      "Train loss: 28.22896281960155 \n",
      "\n",
      "Epoch 514 ============================\n",
      "weight[9.76028725], bias[10.82185048]\n",
      "Train loss: 28.228962819529897 \n",
      "\n",
      "Epoch 515 ============================\n",
      "weight[9.76028695], bias[10.82185201]\n",
      "Train loss: 28.22896281946147 \n",
      "\n",
      "Epoch 516 ============================\n",
      "weight[9.76028665], bias[10.8218535]\n",
      "Train loss: 28.228962819396063 \n",
      "\n",
      "Epoch 517 ============================\n",
      "weight[9.76028636], bias[10.82185496]\n",
      "Train loss: 28.228962819333617 \n",
      "\n",
      "Epoch 518 ============================\n",
      "weight[9.76028608], bias[10.82185638]\n",
      "Train loss: 28.228962819274027 \n",
      "\n",
      "Epoch 519 ============================\n",
      "weight[9.76028581], bias[10.82185778]\n",
      "Train loss: 28.228962819217024 \n",
      "\n",
      "Epoch 520 ============================\n",
      "weight[9.76028554], bias[10.82185914]\n",
      "Train loss: 28.22896281916263 \n",
      "\n",
      "Epoch 521 ============================\n",
      "weight[9.76028528], bias[10.82186047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.22896281911065 \n",
      "\n",
      "Epoch 522 ============================\n",
      "weight[9.76028502], bias[10.82186177]\n",
      "Train loss: 28.228962819061017 \n",
      "\n",
      "Epoch 523 ============================\n",
      "weight[9.76028477], bias[10.82186304]\n",
      "Train loss: 28.228962819013628 \n",
      "\n",
      "Epoch 524 ============================\n",
      "weight[9.76028453], bias[10.82186428]\n",
      "Train loss: 28.22896281896833 \n",
      "\n",
      "Epoch 525 ============================\n",
      "weight[9.76028429], bias[10.8218655]\n",
      "Train loss: 28.228962818925076 \n",
      "\n",
      "Epoch 526 ============================\n",
      "weight[9.76028405], bias[10.82186668]\n",
      "Train loss: 28.228962818883733 \n",
      "\n",
      "Epoch 527 ============================\n",
      "weight[9.76028382], bias[10.82186784]\n",
      "Train loss: 28.228962818844302 \n",
      "\n",
      "Epoch 528 ============================\n",
      "weight[9.7602836], bias[10.82186898]\n",
      "Train loss: 28.22896281880662 \n",
      "\n",
      "Epoch 529 ============================\n",
      "weight[9.76028338], bias[10.82187009]\n",
      "Train loss: 28.228962818770615 \n",
      "\n",
      "Epoch 530 ============================\n",
      "weight[9.76028317], bias[10.82187117]\n",
      "Train loss: 28.2289628187362 \n",
      "\n",
      "Epoch 531 ============================\n",
      "weight[9.76028296], bias[10.82187223]\n",
      "Train loss: 28.228962818703362 \n",
      "\n",
      "Epoch 532 ============================\n",
      "weight[9.76028276], bias[10.82187326]\n",
      "Train loss: 28.22896281867199 \n",
      "\n",
      "Epoch 533 ============================\n",
      "weight[9.76028256], bias[10.82187427]\n",
      "Train loss: 28.228962818642053 \n",
      "\n",
      "Epoch 534 ============================\n",
      "weight[9.76028236], bias[10.82187526]\n",
      "Train loss: 28.228962818613432 \n",
      "\n",
      "Epoch 535 ============================\n",
      "weight[9.76028217], bias[10.82187622]\n",
      "Train loss: 28.228962818586094 \n",
      "\n",
      "Epoch 536 ============================\n",
      "weight[9.76028199], bias[10.82187717]\n",
      "Train loss: 28.228962818559943 \n",
      "\n",
      "Epoch 537 ============================\n",
      "weight[9.7602818], bias[10.82187809]\n",
      "Train loss: 28.228962818535017 \n",
      "\n",
      "Epoch 538 ============================\n",
      "weight[9.76028163], bias[10.82187899]\n",
      "Train loss: 28.228962818511206 \n",
      "\n",
      "Epoch 539 ============================\n",
      "weight[9.76028145], bias[10.82187987]\n",
      "Train loss: 28.22896281848843 \n",
      "\n",
      "Epoch 540 ============================\n",
      "weight[9.76028128], bias[10.82188073]\n",
      "Train loss: 28.228962818466744 \n",
      "\n",
      "Epoch 541 ============================\n",
      "weight[9.76028112], bias[10.82188157]\n",
      "Train loss: 28.228962818445957 \n",
      "\n",
      "Epoch 542 ============================\n",
      "weight[9.76028095], bias[10.82188239]\n",
      "Train loss: 28.228962818426105 \n",
      "\n",
      "Epoch 543 ============================\n",
      "weight[9.7602808], bias[10.8218832]\n",
      "Train loss: 28.228962818407172 \n",
      "\n",
      "Epoch 544 ============================\n",
      "weight[9.76028064], bias[10.82188398]\n",
      "Train loss: 28.228962818389117 \n",
      "\n",
      "Epoch 545 ============================\n",
      "weight[9.76028049], bias[10.82188475]\n",
      "Train loss: 28.228962818371837 \n",
      "\n",
      "Epoch 546 ============================\n",
      "weight[9.76028034], bias[10.8218855]\n",
      "Train loss: 28.228962818355324 \n",
      "\n",
      "Epoch 547 ============================\n",
      "weight[9.7602802], bias[10.82188623]\n",
      "Train loss: 28.228962818339504 \n",
      "\n",
      "Epoch 548 ============================\n",
      "weight[9.76028006], bias[10.82188695]\n",
      "Train loss: 28.228962818324515 \n",
      "\n",
      "Epoch 549 ============================\n",
      "weight[9.76027992], bias[10.82188765]\n",
      "Train loss: 28.228962818310134 \n",
      "\n",
      "Epoch 550 ============================\n",
      "weight[9.76027978], bias[10.82188833]\n",
      "Train loss: 28.22896281829642 \n",
      "\n",
      "Epoch 551 ============================\n",
      "weight[9.76027965], bias[10.821889]\n",
      "Train loss: 28.228962818283257 \n",
      "\n",
      "Epoch 552 ============================\n",
      "weight[9.76027952], bias[10.82188965]\n",
      "Train loss: 28.22896281827075 \n",
      "\n",
      "Epoch 553 ============================\n",
      "weight[9.7602794], bias[10.82189029]\n",
      "Train loss: 28.228962818258783 \n",
      "\n",
      "Epoch 554 ============================\n",
      "weight[9.76027927], bias[10.82189092]\n",
      "Train loss: 28.22896281824737 \n",
      "\n",
      "Epoch 555 ============================\n",
      "weight[9.76027915], bias[10.82189153]\n",
      "Train loss: 28.228962818236432 \n",
      "\n",
      "Epoch 556 ============================\n",
      "weight[9.76027904], bias[10.82189212]\n",
      "Train loss: 28.228962818225998 \n",
      "\n",
      "Epoch 557 ============================\n",
      "weight[9.76027892], bias[10.82189271]\n",
      "Train loss: 28.228962818216075 \n",
      "\n",
      "Epoch 558 ============================\n",
      "weight[9.76027881], bias[10.82189327]\n",
      "Train loss: 28.228962818206508 \n",
      "\n",
      "Epoch 559 ============================\n",
      "weight[9.7602787], bias[10.82189383]\n",
      "Train loss: 28.228962818197452 \n",
      "\n",
      "Epoch 560 ============================\n",
      "weight[9.76027859], bias[10.82189438]\n",
      "Train loss: 28.228962818188755 \n",
      "\n",
      "Epoch 561 ============================\n",
      "weight[9.76027849], bias[10.82189491]\n",
      "Train loss: 28.228962818180445 \n",
      "\n",
      "Epoch 562 ============================\n",
      "weight[9.76027839], bias[10.82189543]\n",
      "Train loss: 28.228962818172523 \n",
      "\n",
      "Epoch 563 ============================\n",
      "weight[9.76027829], bias[10.82189593]\n",
      "Train loss: 28.228962818164998 \n",
      "\n",
      "Epoch 564 ============================\n",
      "weight[9.76027819], bias[10.82189643]\n",
      "Train loss: 28.228962818157736 \n",
      "\n",
      "Epoch 565 ============================\n",
      "weight[9.76027809], bias[10.82189691]\n",
      "Train loss: 28.228962818150872 \n",
      "\n",
      "Epoch 566 ============================\n",
      "weight[9.760278], bias[10.82189739]\n",
      "Train loss: 28.228962818144282 \n",
      "\n",
      "Epoch 567 ============================\n",
      "weight[9.76027791], bias[10.82189785]\n",
      "Train loss: 28.22896281813798 \n",
      "\n",
      "Epoch 568 ============================\n",
      "weight[9.76027782], bias[10.8218983]\n",
      "Train loss: 28.228962818131993 \n",
      "\n",
      "Epoch 569 ============================\n",
      "weight[9.76027773], bias[10.82189875]\n",
      "Train loss: 28.228962818126245 \n",
      "\n",
      "Epoch 570 ============================\n",
      "weight[9.76027765], bias[10.82189918]\n",
      "Train loss: 28.22896281812073 \n",
      "\n",
      "Epoch 571 ============================\n",
      "weight[9.76027756], bias[10.8218996]\n",
      "Train loss: 28.228962818115527 \n",
      "\n",
      "Epoch 572 ============================\n",
      "weight[9.76027748], bias[10.82190002]\n",
      "Train loss: 28.228962818110492 \n",
      "\n",
      "Epoch 573 ============================\n",
      "weight[9.7602774], bias[10.82190042]\n",
      "Train loss: 28.228962818105703 \n",
      "\n",
      "Epoch 574 ============================\n",
      "weight[9.76027732], bias[10.82190081]\n",
      "Train loss: 28.228962818101113 \n",
      "\n",
      "Epoch 575 ============================\n",
      "weight[9.76027725], bias[10.8219012]\n",
      "Train loss: 28.228962818096782 \n",
      "\n",
      "Epoch 576 ============================\n",
      "weight[9.76027717], bias[10.82190158]\n",
      "Train loss: 28.228962818092615 \n",
      "\n",
      "Epoch 577 ============================\n",
      "weight[9.7602771], bias[10.82190194]\n",
      "Train loss: 28.228962818088622 \n",
      "\n",
      "Epoch 578 ============================\n",
      "weight[9.76027703], bias[10.8219023]\n",
      "Train loss: 28.228962818084845 \n",
      "\n",
      "Epoch 579 ============================\n",
      "weight[9.76027696], bias[10.82190266]\n",
      "Train loss: 28.22896281808121 \n",
      "\n",
      "Epoch 580 ============================\n",
      "weight[9.76027689], bias[10.821903]\n",
      "Train loss: 28.22896281807775 \n",
      "\n",
      "Epoch 581 ============================\n",
      "weight[9.76027683], bias[10.82190333]\n",
      "Train loss: 28.228962818074454 \n",
      "\n",
      "Epoch 582 ============================\n",
      "weight[9.76027676], bias[10.82190366]\n",
      "Train loss: 28.22896281807127 \n",
      "\n",
      "Epoch 583 ============================\n",
      "weight[9.7602767], bias[10.82190398]\n",
      "Train loss: 28.22896281806825 \n",
      "\n",
      "Epoch 584 ============================\n",
      "weight[9.76027664], bias[10.8219043]\n",
      "Train loss: 28.228962818065337 \n",
      "\n",
      "Epoch 585 ============================\n",
      "weight[9.76027658], bias[10.8219046]\n",
      "Train loss: 28.22896281806258 \n",
      "\n",
      "Epoch 586 ============================\n",
      "weight[9.76027652], bias[10.8219049]\n",
      "Train loss: 28.228962818059983 \n",
      "\n",
      "Epoch 587 ============================\n",
      "weight[9.76027646], bias[10.8219052]\n",
      "Train loss: 28.228962818057457 \n",
      "\n",
      "Epoch 588 ============================\n",
      "weight[9.7602764], bias[10.82190548]\n",
      "Train loss: 28.228962818055034 \n",
      "\n",
      "Epoch 589 ============================\n",
      "weight[9.76027635], bias[10.82190576]\n",
      "Train loss: 28.22896281805274 \n",
      "\n",
      "Epoch 590 ============================\n",
      "weight[9.76027629], bias[10.82190604]\n",
      "Train loss: 28.228962818050594 \n",
      "\n",
      "Epoch 591 ============================\n",
      "weight[9.76027624], bias[10.8219063]\n",
      "Train loss: 28.228962818048476 \n",
      "\n",
      "Epoch 592 ============================\n",
      "weight[9.76027619], bias[10.82190656]\n",
      "Train loss: 28.228962818046423 \n",
      "\n",
      "Epoch 593 ============================\n",
      "weight[9.76027614], bias[10.82190682]\n",
      "Train loss: 28.22896281804457 \n",
      "\n",
      "Epoch 594 ============================\n",
      "weight[9.76027609], bias[10.82190707]\n",
      "Train loss: 28.228962818042756 \n",
      "\n",
      "Epoch 595 ============================\n",
      "weight[9.76027604], bias[10.82190731]\n",
      "Train loss: 28.22896281804103 \n",
      "\n",
      "Epoch 596 ============================\n",
      "weight[9.760276], bias[10.82190755]\n",
      "Train loss: 28.228962818039342 \n",
      "\n",
      "Epoch 597 ============================\n",
      "weight[9.76027595], bias[10.82190778]\n",
      "Train loss: 28.228962818037775 \n",
      "\n",
      "Epoch 598 ============================\n",
      "weight[9.7602759], bias[10.82190801]\n",
      "Train loss: 28.228962818036244 \n",
      "\n",
      "Epoch 599 ============================\n",
      "weight[9.76027586], bias[10.82190823]\n",
      "Train loss: 28.228962818034812 \n",
      "\n",
      "Epoch 600 ============================\n",
      "weight[9.76027582], bias[10.82190845]\n",
      "Train loss: 28.2289628180334 \n",
      "\n",
      "Epoch 601 ============================\n",
      "weight[9.76027578], bias[10.82190866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.22896281803208 \n",
      "\n",
      "Epoch 602 ============================\n",
      "weight[9.76027574], bias[10.82190887]\n",
      "Train loss: 28.228962818030816 \n",
      "\n",
      "Epoch 603 ============================\n",
      "weight[9.7602757], bias[10.82190907]\n",
      "Train loss: 28.22896281802963 \n",
      "\n",
      "Epoch 604 ============================\n",
      "weight[9.76027566], bias[10.82190927]\n",
      "Train loss: 28.22896281802844 \n",
      "\n",
      "Epoch 605 ============================\n",
      "weight[9.76027562], bias[10.82190946]\n",
      "Train loss: 28.22896281802735 \n",
      "\n",
      "Epoch 606 ============================\n",
      "weight[9.76027558], bias[10.82190965]\n",
      "Train loss: 28.228962818026275 \n",
      "\n",
      "Epoch 607 ============================\n",
      "weight[9.76027554], bias[10.82190984]\n",
      "Train loss: 28.228962818025273 \n",
      "\n",
      "Epoch 608 ============================\n",
      "weight[9.76027551], bias[10.82191002]\n",
      "Train loss: 28.22896281802431 \n",
      "\n",
      "Epoch 609 ============================\n",
      "weight[9.76027547], bias[10.82191019]\n",
      "Train loss: 28.22896281802343 \n",
      "\n",
      "Epoch 610 ============================\n",
      "weight[9.76027544], bias[10.82191037]\n",
      "Train loss: 28.228962818022524 \n",
      "\n",
      "Epoch 611 ============================\n",
      "weight[9.76027541], bias[10.82191054]\n",
      "Train loss: 28.22896281802171 \n",
      "\n",
      "Epoch 612 ============================\n",
      "weight[9.76027537], bias[10.8219107]\n",
      "Train loss: 28.228962818020925 \n",
      "\n",
      "Epoch 613 ============================\n",
      "weight[9.76027534], bias[10.82191086]\n",
      "Train loss: 28.22896281802016 \n",
      "\n",
      "Epoch 614 ============================\n",
      "weight[9.76027531], bias[10.82191102]\n",
      "Train loss: 28.2289628180194 \n",
      "\n",
      "Epoch 615 ============================\n",
      "weight[9.76027528], bias[10.82191117]\n",
      "Train loss: 28.228962818018733 \n",
      "\n",
      "Epoch 616 ============================\n",
      "weight[9.76027525], bias[10.82191132]\n",
      "Train loss: 28.228962818018065 \n",
      "\n",
      "Epoch 617 ============================\n",
      "weight[9.76027522], bias[10.82191147]\n",
      "Train loss: 28.228962818017433 \n",
      "\n",
      "Epoch 618 ============================\n",
      "weight[9.76027519], bias[10.82191162]\n",
      "Train loss: 28.228962818016846 \n",
      "\n",
      "Epoch 619 ============================\n",
      "weight[9.76027517], bias[10.82191176]\n",
      "Train loss: 28.228962818016246 \n",
      "\n",
      "Epoch 620 ============================\n",
      "weight[9.76027514], bias[10.82191189]\n",
      "Train loss: 28.22896281801568 \n",
      "\n",
      "Epoch 621 ============================\n",
      "weight[9.76027511], bias[10.82191203]\n",
      "Train loss: 28.22896281801517 \n",
      "\n",
      "Epoch 622 ============================\n",
      "weight[9.76027509], bias[10.82191216]\n",
      "Train loss: 28.22896281801467 \n",
      "\n",
      "Epoch 623 ============================\n",
      "weight[9.76027506], bias[10.82191229]\n",
      "Train loss: 28.228962818014175 \n",
      "\n",
      "Epoch 624 ============================\n",
      "weight[9.76027504], bias[10.82191241]\n",
      "Train loss: 28.22896281801373 \n",
      "\n",
      "Epoch 625 ============================\n",
      "weight[9.76027501], bias[10.82191253]\n",
      "Train loss: 28.22896281801325 \n",
      "\n",
      "Epoch 626 ============================\n",
      "weight[9.76027499], bias[10.82191265]\n",
      "Train loss: 28.228962818012853 \n",
      "\n",
      "Epoch 627 ============================\n",
      "weight[9.76027497], bias[10.82191277]\n",
      "Train loss: 28.22896281801243 \n",
      "\n",
      "Epoch 628 ============================\n",
      "weight[9.76027494], bias[10.82191289]\n",
      "Train loss: 28.228962818012064 \n",
      "\n",
      "Epoch 629 ============================\n",
      "weight[9.76027492], bias[10.821913]\n",
      "Train loss: 28.228962818011723 \n",
      "\n",
      "Epoch 630 ============================\n",
      "weight[9.7602749], bias[10.82191311]\n",
      "Train loss: 28.228962818011382 \n",
      "\n",
      "Epoch 631 ============================\n",
      "weight[9.76027488], bias[10.82191321]\n",
      "Train loss: 28.228962818011045 \n",
      "\n",
      "Epoch 632 ============================\n",
      "weight[9.76027486], bias[10.82191332]\n",
      "Train loss: 28.228962818010704 \n",
      "\n",
      "Epoch 633 ============================\n",
      "weight[9.76027484], bias[10.82191342]\n",
      "Train loss: 28.228962818010384 \n",
      "\n",
      "Epoch 634 ============================\n",
      "weight[9.76027482], bias[10.82191352]\n",
      "Train loss: 28.228962818010125 \n",
      "\n",
      "Epoch 635 ============================\n",
      "weight[9.7602748], bias[10.82191362]\n",
      "Train loss: 28.228962818009858 \n",
      "\n",
      "Epoch 636 ============================\n",
      "weight[9.76027478], bias[10.82191371]\n",
      "Train loss: 28.228962818009546 \n",
      "\n",
      "Epoch 637 ============================\n",
      "weight[9.76027476], bias[10.8219138]\n",
      "Train loss: 28.228962818009308 \n",
      "\n",
      "Epoch 638 ============================\n",
      "weight[9.76027474], bias[10.82191389]\n",
      "Train loss: 28.228962818009077 \n",
      "\n",
      "Epoch 639 ============================\n",
      "weight[9.76027473], bias[10.82191398]\n",
      "Train loss: 28.228962818008817 \n",
      "\n",
      "Epoch 640 ============================\n",
      "weight[9.76027471], bias[10.82191407]\n",
      "Train loss: 28.22896281800862 \n",
      "\n",
      "Epoch 641 ============================\n",
      "weight[9.76027469], bias[10.82191415]\n",
      "Train loss: 28.228962818008387 \n",
      "\n",
      "Epoch 642 ============================\n",
      "weight[9.76027468], bias[10.82191424]\n",
      "Train loss: 28.22896281800823 \n",
      "\n",
      "Epoch 643 ============================\n",
      "weight[9.76027466], bias[10.82191432]\n",
      "Train loss: 28.22896281800799 \n",
      "\n",
      "Epoch 644 ============================\n",
      "weight[9.76027464], bias[10.8219144]\n",
      "Train loss: 28.22896281800785 \n",
      "\n",
      "Epoch 645 ============================\n",
      "weight[9.76027463], bias[10.82191448]\n",
      "Train loss: 28.228962818007655 \n",
      "\n",
      "Epoch 646 ============================\n",
      "weight[9.76027461], bias[10.82191455]\n",
      "Train loss: 28.228962818007517 \n",
      "\n",
      "Epoch 647 ============================\n",
      "weight[9.7602746], bias[10.82191462]\n",
      "Train loss: 28.228962818007354 \n",
      "\n",
      "Epoch 648 ============================\n",
      "weight[9.76027459], bias[10.8219147]\n",
      "Train loss: 28.2289628180072 \n",
      "\n",
      "Epoch 649 ============================\n",
      "weight[9.76027457], bias[10.82191477]\n",
      "Train loss: 28.228962818007 \n",
      "\n",
      "Epoch 650 ============================\n",
      "weight[9.76027456], bias[10.82191484]\n",
      "Train loss: 28.2289628180069 \n",
      "\n",
      "Epoch 651 ============================\n",
      "weight[9.76027455], bias[10.8219149]\n",
      "Train loss: 28.22896281800676 \n",
      "\n",
      "Epoch 652 ============================\n",
      "weight[9.76027453], bias[10.82191497]\n",
      "Train loss: 28.228962818006625 \n",
      "\n",
      "Epoch 653 ============================\n",
      "weight[9.76027452], bias[10.82191503]\n",
      "Train loss: 28.228962818006526 \n",
      "\n",
      "Epoch 654 ============================\n",
      "weight[9.76027451], bias[10.8219151]\n",
      "Train loss: 28.228962818006362 \n",
      "\n",
      "Epoch 655 ============================\n",
      "weight[9.7602745], bias[10.82191516]\n",
      "Train loss: 28.228962818006277 \n",
      "\n",
      "Epoch 656 ============================\n",
      "weight[9.76027448], bias[10.82191522]\n",
      "Train loss: 28.228962818006192 \n",
      "\n",
      "Epoch 657 ============================\n",
      "weight[9.76027447], bias[10.82191528]\n",
      "Train loss: 28.228962818006075 \n",
      "\n",
      "Epoch 658 ============================\n",
      "weight[9.76027446], bias[10.82191533]\n",
      "Train loss: 28.228962818005954 \n",
      "\n",
      "Epoch 659 ============================\n",
      "weight[9.76027445], bias[10.82191539]\n",
      "Train loss: 28.228962818005886 \n",
      "\n",
      "Epoch 660 ============================\n",
      "weight[9.76027444], bias[10.82191545]\n",
      "Train loss: 28.228962818005787 \n",
      "\n",
      "Epoch 661 ============================\n",
      "weight[9.76027443], bias[10.8219155]\n",
      "Train loss: 28.228962818005705 \n",
      "\n",
      "Epoch 662 ============================\n",
      "weight[9.76027442], bias[10.82191555]\n",
      "Train loss: 28.22896281800561 \n",
      "\n",
      "Epoch 663 ============================\n",
      "weight[9.76027441], bias[10.8219156]\n",
      "Train loss: 28.228962818005584 \n",
      "\n",
      "Epoch 664 ============================\n",
      "weight[9.7602744], bias[10.82191565]\n",
      "Train loss: 28.22896281800547 \n",
      "\n",
      "Epoch 665 ============================\n",
      "weight[9.76027439], bias[10.8219157]\n",
      "Train loss: 28.228962818005378 \n",
      "\n",
      "Epoch 666 ============================\n",
      "weight[9.76027438], bias[10.82191575]\n",
      "Train loss: 28.228962818005375 \n",
      "\n",
      "Epoch 667 ============================\n",
      "weight[9.76027437], bias[10.8219158]\n",
      "Train loss: 28.228962818005307 \n",
      "\n",
      "Epoch 668 ============================\n",
      "weight[9.76027436], bias[10.82191584]\n",
      "Train loss: 28.228962818005193 \n",
      "\n",
      "Epoch 669 ============================\n",
      "weight[9.76027435], bias[10.82191589]\n",
      "Train loss: 28.22896281800518 \n",
      "\n",
      "Epoch 670 ============================\n",
      "weight[9.76027434], bias[10.82191593]\n",
      "Train loss: 28.228962818005073 \n",
      "\n",
      "Epoch 671 ============================\n",
      "weight[9.76027433], bias[10.82191597]\n",
      "Train loss: 28.22896281800507 \n",
      "\n",
      "Epoch 672 ============================\n",
      "weight[9.76027433], bias[10.82191601]\n",
      "Train loss: 28.22896281800501 \n",
      "\n",
      "Epoch 673 ============================\n",
      "weight[9.76027432], bias[10.82191605]\n",
      "Train loss: 28.228962818004952 \n",
      "\n",
      "Epoch 674 ============================\n",
      "weight[9.76027431], bias[10.82191609]\n",
      "Train loss: 28.228962818004902 \n",
      "\n",
      "Epoch 675 ============================\n",
      "weight[9.7602743], bias[10.82191613]\n",
      "Train loss: 28.228962818004845 \n",
      "\n",
      "Epoch 676 ============================\n",
      "weight[9.7602743], bias[10.82191617]\n",
      "Train loss: 28.228962818004813 \n",
      "\n",
      "Epoch 677 ============================\n",
      "weight[9.76027429], bias[10.82191621]\n",
      "Train loss: 28.22896281800475 \n",
      "\n",
      "Epoch 678 ============================\n",
      "weight[9.76027428], bias[10.82191624]\n",
      "Train loss: 28.22896281800471 \n",
      "\n",
      "Epoch 679 ============================\n",
      "weight[9.76027427], bias[10.82191628]\n",
      "Train loss: 28.228962818004668 \n",
      "\n",
      "Epoch 680 ============================\n",
      "weight[9.76027427], bias[10.82191632]\n",
      "Train loss: 28.22896281800464 \n",
      "\n",
      "Epoch 681 ============================\n",
      "weight[9.76027426], bias[10.82191635]\n",
      "Train loss: 28.228962818004625 \n",
      "\n",
      "Epoch 682 ============================\n",
      "weight[9.76027425], bias[10.82191638]\n",
      "Train loss: 28.228962818004625 \n",
      "\n",
      "Epoch 683 ============================\n",
      "weight[9.76027425], bias[10.82191641]\n",
      "Train loss: 28.228962818004586 \n",
      "\n",
      "Epoch 684 ============================\n",
      "weight[9.76027424], bias[10.82191645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228962818004558 \n",
      "\n",
      "Epoch 685 ============================\n",
      "weight[9.76027424], bias[10.82191648]\n",
      "Train loss: 28.228962818004536 \n",
      "\n",
      "Epoch 686 ============================\n",
      "weight[9.76027423], bias[10.82191651]\n",
      "Train loss: 28.228962818004483 \n",
      "\n",
      "Epoch 687 ============================\n",
      "weight[9.76027422], bias[10.82191654]\n",
      "Train loss: 28.228962818004455 \n",
      "\n",
      "Epoch 688 ============================\n",
      "weight[9.76027422], bias[10.82191657]\n",
      "Train loss: 28.228962818004423 \n",
      "\n",
      "Epoch 689 ============================\n",
      "weight[9.76027421], bias[10.82191659]\n",
      "Train loss: 28.228962818004398 \n",
      "\n",
      "Epoch 690 ============================\n",
      "weight[9.76027421], bias[10.82191662]\n",
      "Train loss: 28.228962818004398 \n",
      "\n",
      "Epoch 691 ============================\n",
      "weight[9.7602742], bias[10.82191665]\n",
      "Train loss: 28.22896281800436 \n",
      "\n",
      "Epoch 692 ============================\n",
      "weight[9.7602742], bias[10.82191667]\n",
      "Train loss: 28.22896281800435 \n",
      "\n",
      "Epoch 693 ============================\n",
      "weight[9.76027419], bias[10.8219167]\n",
      "Train loss: 28.2289628180043 \n",
      "\n",
      "Epoch 694 ============================\n",
      "weight[9.76027419], bias[10.82191673]\n",
      "Train loss: 28.22896281800431 \n",
      "\n",
      "Epoch 695 ============================\n",
      "weight[9.76027418], bias[10.82191675]\n",
      "Train loss: 28.22896281800426 \n",
      "\n",
      "Epoch 696 ============================\n",
      "weight[9.76027418], bias[10.82191677]\n",
      "Train loss: 28.228962818004277 \n",
      "\n",
      "Epoch 697 ============================\n",
      "weight[9.76027417], bias[10.8219168]\n",
      "Train loss: 28.22896281800425 \n",
      "\n",
      "Epoch 698 ============================\n",
      "weight[9.76027417], bias[10.82191682]\n",
      "Train loss: 28.228962818004277 \n",
      "\n",
      "Epoch 699 ============================\n",
      "weight[9.76027416], bias[10.82191684]\n",
      "Train loss: 28.228962818004227 \n",
      "\n",
      "Epoch 700 ============================\n",
      "weight[9.76027416], bias[10.82191686]\n",
      "Train loss: 28.228962818004213 \n",
      "\n",
      "Epoch 701 ============================\n",
      "weight[9.76027415], bias[10.82191689]\n",
      "Train loss: 28.22896281800421 \n",
      "\n",
      "Epoch 702 ============================\n",
      "weight[9.76027415], bias[10.82191691]\n",
      "Train loss: 28.22896281800422 \n",
      "\n",
      "Epoch 703 ============================\n",
      "weight[9.76027415], bias[10.82191693]\n",
      "Train loss: 28.228962818004174 \n",
      "\n",
      "Epoch 704 ============================\n",
      "weight[9.76027414], bias[10.82191695]\n",
      "Train loss: 28.228962818004163 \n",
      "\n",
      "Epoch 705 ============================\n",
      "weight[9.76027414], bias[10.82191697]\n",
      "Train loss: 28.228962818004124 \n",
      "\n",
      "Epoch 706 ============================\n",
      "weight[9.76027413], bias[10.82191699]\n",
      "Train loss: 28.228962818004113 \n",
      "\n",
      "Epoch 707 ============================\n",
      "weight[9.76027413], bias[10.821917]\n",
      "Train loss: 28.228962818004124 \n",
      "\n",
      "Epoch 708 ============================\n",
      "weight[9.76027413], bias[10.82191702]\n",
      "Train loss: 28.228962818004113 \n",
      "\n",
      "Epoch 709 ============================\n",
      "weight[9.76027412], bias[10.82191704]\n",
      "Train loss: 28.22896281800413 \n",
      "\n",
      "Epoch 710 ============================\n",
      "weight[9.76027412], bias[10.82191706]\n",
      "Train loss: 28.228962818004074 \n",
      "\n",
      "Epoch 711 ============================\n",
      "weight[9.76027412], bias[10.82191708]\n",
      "Train loss: 28.22896281800408 \n",
      "\n",
      "Epoch 712 ============================\n",
      "weight[9.76027411], bias[10.82191709]\n",
      "Train loss: 28.228962818004067 \n",
      "\n",
      "Epoch 713 ============================\n",
      "weight[9.76027411], bias[10.82191711]\n",
      "Train loss: 28.228962818004074 \n",
      "\n",
      "Epoch 714 ============================\n",
      "weight[9.76027411], bias[10.82191712]\n",
      "Train loss: 28.22896281800406 \n",
      "\n",
      "Epoch 715 ============================\n",
      "weight[9.7602741], bias[10.82191714]\n",
      "Train loss: 28.228962818004074 \n",
      "\n",
      "Epoch 716 ============================\n",
      "weight[9.7602741], bias[10.82191715]\n",
      "Train loss: 28.228962818004028 \n",
      "\n",
      "Epoch 717 ============================\n",
      "weight[9.7602741], bias[10.82191717]\n",
      "Train loss: 28.228962818004057 \n",
      "\n",
      "Epoch 718 ============================\n",
      "weight[9.7602741], bias[10.82191718]\n",
      "Train loss: 28.228962818004078 \n",
      "\n",
      "Epoch 719 ============================\n",
      "weight[9.76027409], bias[10.8219172]\n",
      "Train loss: 28.22896281800401 \n",
      "\n",
      "Epoch 720 ============================\n",
      "weight[9.76027409], bias[10.82191721]\n",
      "Train loss: 28.228962818004067 \n",
      "\n",
      "Epoch 721 ============================\n",
      "weight[9.76027409], bias[10.82191723]\n",
      "Train loss: 28.228962818004057 \n",
      "\n",
      "Epoch 722 ============================\n",
      "weight[9.76027408], bias[10.82191724]\n",
      "Train loss: 28.228962818004057 \n",
      "\n",
      "Epoch 723 ============================\n",
      "weight[9.76027408], bias[10.82191725]\n",
      "Train loss: 28.228962818003993 \n",
      "\n",
      "Epoch 724 ============================\n",
      "weight[9.76027408], bias[10.82191726]\n",
      "Train loss: 28.228962818004025 \n",
      "\n",
      "Epoch 725 ============================\n",
      "weight[9.76027408], bias[10.82191728]\n",
      "Train loss: 28.228962818004018 \n",
      "\n",
      "Epoch 726 ============================\n",
      "weight[9.76027408], bias[10.82191729]\n",
      "Train loss: 28.228962818004018 \n",
      "\n",
      "Epoch 727 ============================\n",
      "weight[9.76027407], bias[10.8219173]\n",
      "Train loss: 28.22896281800397 \n",
      "\n",
      "Epoch 728 ============================\n",
      "weight[9.76027407], bias[10.82191731]\n",
      "Train loss: 28.228962818004025 \n",
      "\n",
      "Epoch 729 ============================\n",
      "weight[9.76027407], bias[10.82191732]\n",
      "Train loss: 28.228962818003986 \n",
      "\n",
      "Epoch 730 ============================\n",
      "weight[9.76027407], bias[10.82191733]\n",
      "Train loss: 28.22896281800397 \n",
      "\n",
      "Epoch 731 ============================\n",
      "weight[9.76027406], bias[10.82191734]\n",
      "Train loss: 28.228962818003975 \n",
      "\n",
      "Epoch 732 ============================\n",
      "weight[9.76027406], bias[10.82191736]\n",
      "Train loss: 28.228962818004028 \n",
      "\n",
      "Epoch 733 ============================\n",
      "weight[9.76027406], bias[10.82191737]\n",
      "Train loss: 28.22896281800401 \n",
      "\n",
      "Epoch 734 ============================\n",
      "weight[9.76027406], bias[10.82191738]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 735 ============================\n",
      "weight[9.76027406], bias[10.82191739]\n",
      "Train loss: 28.228962818003993 \n",
      "\n",
      "Epoch 736 ============================\n",
      "weight[9.76027405], bias[10.8219174]\n",
      "Train loss: 28.228962818003993 \n",
      "\n",
      "Epoch 737 ============================\n",
      "weight[9.76027405], bias[10.8219174]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 738 ============================\n",
      "weight[9.76027405], bias[10.82191741]\n",
      "Train loss: 28.228962818003993 \n",
      "\n",
      "Epoch 739 ============================\n",
      "weight[9.76027405], bias[10.82191742]\n",
      "Train loss: 28.228962818003986 \n",
      "\n",
      "Epoch 740 ============================\n",
      "weight[9.76027405], bias[10.82191743]\n",
      "Train loss: 28.228962818003986 \n",
      "\n",
      "Epoch 741 ============================\n",
      "weight[9.76027405], bias[10.82191744]\n",
      "Train loss: 28.22896281800396 \n",
      "\n",
      "Epoch 742 ============================\n",
      "weight[9.76027404], bias[10.82191745]\n",
      "Train loss: 28.228962818003982 \n",
      "\n",
      "Epoch 743 ============================\n",
      "weight[9.76027404], bias[10.82191746]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 744 ============================\n",
      "weight[9.76027404], bias[10.82191746]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 745 ============================\n",
      "weight[9.76027404], bias[10.82191747]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 746 ============================\n",
      "weight[9.76027404], bias[10.82191748]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 747 ============================\n",
      "weight[9.76027404], bias[10.82191749]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 748 ============================\n",
      "weight[9.76027403], bias[10.82191749]\n",
      "Train loss: 28.22896281800397 \n",
      "\n",
      "Epoch 749 ============================\n",
      "weight[9.76027403], bias[10.8219175]\n",
      "Train loss: 28.228962818003968 \n",
      "\n",
      "Epoch 750 ============================\n",
      "weight[9.76027403], bias[10.82191751]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 751 ============================\n",
      "weight[9.76027403], bias[10.82191752]\n",
      "Train loss: 28.22896281800397 \n",
      "\n",
      "Epoch 752 ============================\n",
      "weight[9.76027403], bias[10.82191752]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 753 ============================\n",
      "weight[9.76027403], bias[10.82191753]\n",
      "Train loss: 28.228962818003932 \n",
      "\n",
      "Epoch 754 ============================\n",
      "weight[9.76027403], bias[10.82191753]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 755 ============================\n",
      "weight[9.76027403], bias[10.82191754]\n",
      "Train loss: 28.228962818004 \n",
      "\n",
      "Epoch 756 ============================\n",
      "weight[9.76027402], bias[10.82191755]\n",
      "Train loss: 28.228962818003957 \n",
      "\n",
      "Epoch 757 ============================\n",
      "weight[9.76027402], bias[10.82191755]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 758 ============================\n",
      "weight[9.76027402], bias[10.82191756]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 759 ============================\n",
      "weight[9.76027402], bias[10.82191756]\n",
      "Train loss: 28.228962818003957 \n",
      "\n",
      "Epoch 760 ============================\n",
      "weight[9.76027402], bias[10.82191757]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 761 ============================\n",
      "weight[9.76027402], bias[10.82191758]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 762 ============================\n",
      "weight[9.76027402], bias[10.82191758]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 763 ============================\n",
      "weight[9.76027402], bias[10.82191759]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 764 ============================\n",
      "weight[9.76027402], bias[10.82191759]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 765 ============================\n",
      "weight[9.76027401], bias[10.8219176]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 766 ============================\n",
      "weight[9.76027401], bias[10.8219176]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 767 ============================\n",
      "weight[9.76027401], bias[10.82191761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228962818003993 \n",
      "\n",
      "Epoch 768 ============================\n",
      "weight[9.76027401], bias[10.82191761]\n",
      "Train loss: 28.228962818003986 \n",
      "\n",
      "Epoch 769 ============================\n",
      "weight[9.76027401], bias[10.82191761]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 770 ============================\n",
      "weight[9.76027401], bias[10.82191762]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 771 ============================\n",
      "weight[9.76027401], bias[10.82191762]\n",
      "Train loss: 28.228962818003957 \n",
      "\n",
      "Epoch 772 ============================\n",
      "weight[9.76027401], bias[10.82191763]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 773 ============================\n",
      "weight[9.76027401], bias[10.82191763]\n",
      "Train loss: 28.2289628180039 \n",
      "\n",
      "Epoch 774 ============================\n",
      "weight[9.76027401], bias[10.82191764]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 775 ============================\n",
      "weight[9.76027401], bias[10.82191764]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 776 ============================\n",
      "weight[9.76027401], bias[10.82191764]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 777 ============================\n",
      "weight[9.760274], bias[10.82191765]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 778 ============================\n",
      "weight[9.760274], bias[10.82191765]\n",
      "Train loss: 28.228962818003932 \n",
      "\n",
      "Epoch 779 ============================\n",
      "weight[9.760274], bias[10.82191765]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 780 ============================\n",
      "weight[9.760274], bias[10.82191766]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 781 ============================\n",
      "weight[9.760274], bias[10.82191766]\n",
      "Train loss: 28.228962818003982 \n",
      "\n",
      "Epoch 782 ============================\n",
      "weight[9.760274], bias[10.82191766]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 783 ============================\n",
      "weight[9.760274], bias[10.82191767]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 784 ============================\n",
      "weight[9.760274], bias[10.82191767]\n",
      "Train loss: 28.228962818003975 \n",
      "\n",
      "Epoch 785 ============================\n",
      "weight[9.760274], bias[10.82191767]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 786 ============================\n",
      "weight[9.760274], bias[10.82191768]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 787 ============================\n",
      "weight[9.760274], bias[10.82191768]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 788 ============================\n",
      "weight[9.760274], bias[10.82191768]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 789 ============================\n",
      "weight[9.760274], bias[10.82191769]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 790 ============================\n",
      "weight[9.760274], bias[10.82191769]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 791 ============================\n",
      "weight[9.760274], bias[10.82191769]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 792 ============================\n",
      "weight[9.760274], bias[10.82191769]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 793 ============================\n",
      "weight[9.76027399], bias[10.8219177]\n",
      "Train loss: 28.22896281800388 \n",
      "\n",
      "Epoch 794 ============================\n",
      "weight[9.76027399], bias[10.8219177]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 795 ============================\n",
      "weight[9.76027399], bias[10.8219177]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 796 ============================\n",
      "weight[9.76027399], bias[10.8219177]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 797 ============================\n",
      "weight[9.76027399], bias[10.82191771]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 798 ============================\n",
      "weight[9.76027399], bias[10.82191771]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 799 ============================\n",
      "weight[9.76027399], bias[10.82191771]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 800 ============================\n",
      "weight[9.76027399], bias[10.82191771]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 801 ============================\n",
      "weight[9.76027399], bias[10.82191772]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 802 ============================\n",
      "weight[9.76027399], bias[10.82191772]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 803 ============================\n",
      "weight[9.76027399], bias[10.82191772]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 804 ============================\n",
      "weight[9.76027399], bias[10.82191772]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 805 ============================\n",
      "weight[9.76027399], bias[10.82191772]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 806 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 807 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 808 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 809 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 810 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 811 ============================\n",
      "weight[9.76027399], bias[10.82191773]\n",
      "Train loss: 28.228962818003854 \n",
      "\n",
      "Epoch 812 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 813 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 814 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 815 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 816 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 817 ============================\n",
      "weight[9.76027399], bias[10.82191774]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 818 ============================\n",
      "weight[9.76027399], bias[10.82191775]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 819 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 820 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 821 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 822 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.22896281800396 \n",
      "\n",
      "Epoch 823 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 824 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 825 ============================\n",
      "weight[9.76027398], bias[10.82191775]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 826 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 827 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 828 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 829 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 830 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 831 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 832 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 833 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 834 ============================\n",
      "weight[9.76027398], bias[10.82191776]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 835 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 836 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 837 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 838 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003883 \n",
      "\n",
      "Epoch 839 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 840 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 841 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 842 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 843 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 844 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 845 ============================\n",
      "weight[9.76027398], bias[10.82191777]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 846 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 847 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 848 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 849 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 850 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003954 \n",
      "\n",
      "Epoch 851 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 852 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 853 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 854 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003914 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 855 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 856 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 857 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 858 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 859 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 860 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 861 ============================\n",
      "weight[9.76027398], bias[10.82191778]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 862 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 863 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 864 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 865 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 866 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 867 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 868 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 869 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 870 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 871 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 872 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003932 \n",
      "\n",
      "Epoch 873 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 874 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 875 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003975 \n",
      "\n",
      "Epoch 876 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 877 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 878 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 879 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 880 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 881 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 882 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 883 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 884 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 885 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 886 ============================\n",
      "weight[9.76027398], bias[10.82191779]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 887 ============================\n",
      "weight[9.76027398], bias[10.8219178]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 888 ============================\n",
      "weight[9.76027398], bias[10.8219178]\n",
      "Train loss: 28.228962818003883 \n",
      "\n",
      "Epoch 889 ============================\n",
      "weight[9.76027398], bias[10.8219178]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 890 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 891 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 892 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003932 \n",
      "\n",
      "Epoch 893 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 894 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 895 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 896 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 897 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 898 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 899 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 900 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 901 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 902 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 903 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 904 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 905 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 906 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 907 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 908 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 909 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 910 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 911 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 912 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 913 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 914 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 915 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 916 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 917 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 918 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 919 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 920 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 921 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 922 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 923 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 924 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 925 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 926 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 927 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 928 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 929 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800394 \n",
      "\n",
      "Epoch 930 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 931 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 932 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 933 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 934 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 935 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 936 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 937 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 938 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 939 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 940 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 941 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.22896281800393 \n",
      "\n",
      "Epoch 942 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 943 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 944 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 945 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 946 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 947 ============================\n",
      "weight[9.76027397], bias[10.8219178]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 948 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 949 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 950 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 951 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 952 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003883 \n",
      "\n",
      "Epoch 953 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 954 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 955 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 956 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 957 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 958 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 959 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 960 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 961 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 962 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 963 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 964 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 965 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 966 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 967 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 968 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 969 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 970 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003872 \n",
      "\n",
      "Epoch 971 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 972 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 973 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 974 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 975 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003883 \n",
      "\n",
      "Epoch 976 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 977 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 978 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003914 \n",
      "\n",
      "Epoch 979 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 980 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 981 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 982 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800389 \n",
      "\n",
      "Epoch 983 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 984 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 985 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 986 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003847 \n",
      "\n",
      "Epoch 987 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.22896281800391 \n",
      "\n",
      "Epoch 988 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003918 \n",
      "\n",
      "Epoch 989 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003907 \n",
      "\n",
      "Epoch 990 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 991 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 992 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003932 \n",
      "\n",
      "Epoch 993 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003925 \n",
      "\n",
      "Epoch 994 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003897 \n",
      "\n",
      "Epoch 995 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003904 \n",
      "\n",
      "Epoch 996 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003886 \n",
      "\n",
      "Epoch 997 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n",
      "Epoch 998 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 999 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003943 \n",
      "\n",
      "Epoch 1000 ============================\n",
      "weight[9.76027397], bias[10.82191781]\n",
      "Train loss: 28.228962818003936 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LinearRegression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing implementation on extra Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data\n",
    "test = pd.DataFrame({'x':[1,2,2,3,3,4,5,6,6,6,8,10], 'y':[-890,-1411, -1560, -2220, -2091,-2878,-3537, -3268, -3920,\n",
    "                                                         -4163, -5471, -5157]})\n",
    "\n",
    "x, y = test[['x']], test[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ============================\n",
      "weight0.5195720296603407, bias5\n",
      "Train loss: 11316741.764844388 \n",
      "\n",
      "Epoch 2 ============================\n",
      "weight[-212.34308246], bias[-31.65509603]\n",
      "Train loss: 4899022.187840232 \n",
      "\n",
      "Epoch 3 ============================\n",
      "weight[-350.77974905], bias[-55.95002226]\n",
      "Train loss: 2181511.097496031 \n",
      "\n",
      "Epoch 4 ============================\n",
      "weight[-440.78743312], bias[-72.20095605]\n",
      "Train loss: 1030720.3236895896 \n",
      "\n",
      "Epoch 5 ============================\n",
      "weight[-499.28245232], bias[-83.21644832]\n",
      "Train loss: 543301.4606197227 \n",
      "\n",
      "Epoch 6 ============================\n",
      "weight[-537.27229743], bias[-90.82403361]\n",
      "Train loss: 336764.293732003 \n",
      "\n",
      "Epoch 7 ============================\n",
      "weight[-561.91957042], bias[-96.21289655]\n",
      "Train loss: 249157.17106066478 \n",
      "\n",
      "Epoch 8 ============================\n",
      "weight[-577.88499427], bias[-100.15684585]\n",
      "Train loss: 211907.5185031326 \n",
      "\n",
      "Epoch 9 ============================\n",
      "weight[-588.20131285], bias[-103.15940402]\n",
      "Train loss: 195980.75514660403 \n",
      "\n",
      "Epoch 10 ============================\n",
      "weight[-594.84193986], bias[-105.54821765]\n",
      "Train loss: 189083.23329978762 \n",
      "\n",
      "Epoch 11 ============================\n",
      "weight[-599.09098012], bias[-107.53649041]\n",
      "Train loss: 186009.64797855736 \n",
      "\n",
      "Epoch 12 ============================\n",
      "weight[-601.78400341], bias[-109.26295764]\n",
      "Train loss: 184555.9965696512 \n",
      "\n",
      "Epoch 13 ============================\n",
      "weight[-603.46471663], bias[-110.81789796]\n",
      "Train loss: 183789.06942115608 \n",
      "\n",
      "Epoch 14 ============================\n",
      "weight[-604.48691069], bias[-112.26005905]\n",
      "Train loss: 183313.72822238342 \n",
      "\n",
      "Epoch 15 ============================\n",
      "weight[-605.08079775], bias[-113.62767134]\n",
      "Train loss: 182962.66153494257 \n",
      "\n",
      "Epoch 16 ============================\n",
      "weight[-605.39617692], bias[-114.94561461]\n",
      "Train loss: 182665.02299323797 \n",
      "\n",
      "Epoch 17 ============================\n",
      "weight[-605.53052235], bias[-116.23008133]\n",
      "Train loss: 182390.81090146667 \n",
      "\n",
      "Epoch 18 ============================\n",
      "weight[-605.54726019], bias[-117.4916111]\n",
      "Train loss: 182127.31778552916 \n",
      "\n",
      "Epoch 19 ============================\n",
      "weight[-605.48766151], bias[-118.7370652]\n",
      "Train loss: 181869.15880743752 \n",
      "\n",
      "Epoch 20 ============================\n",
      "weight[-605.37858094], bias[-119.97091137]\n",
      "Train loss: 181614.04965792448 \n",
      "\n",
      "Epoch 21 ============================\n",
      "weight[-605.23749239], bias[-121.1960599]\n",
      "Train loss: 181361.01885764967 \n",
      "\n",
      "Epoch 22 ============================\n",
      "weight[-605.07576562], bias[-122.41440761]\n",
      "Train loss: 181109.6508206773 \n",
      "\n",
      "Epoch 23 ============================\n",
      "weight[-604.90079848], bias[-123.62719184]\n",
      "Train loss: 180859.76535695916 \n",
      "\n",
      "Epoch 24 ============================\n",
      "weight[-604.71740426], bias[-124.83522083]\n",
      "Train loss: 180611.28196912855 \n",
      "\n",
      "Epoch 25 ============================\n",
      "weight[-604.52871444], bias[-126.03902354]\n",
      "Train loss: 180364.16239376343 \n",
      "\n",
      "Epoch 26 ============================\n",
      "weight[-604.33676621], bias[-127.23894725]\n",
      "Train loss: 180118.38627243848 \n",
      "\n",
      "Epoch 27 ============================\n",
      "weight[-604.14288466], bias[-128.43522097]\n",
      "Train loss: 179873.94085039268 \n",
      "\n",
      "Epoch 28 ============================\n",
      "weight[-603.9479315], bias[-129.62799678]\n",
      "Train loss: 179630.8166146792 \n",
      "\n",
      "Epoch 29 ============================\n",
      "weight[-603.75246697], bias[-130.81737665]\n",
      "Train loss: 179389.0054471785 \n",
      "\n",
      "Epoch 30 ============================\n",
      "weight[-603.55685511], bias[-132.00342998]\n",
      "Train loss: 179148.49984243858 \n",
      "\n",
      "Epoch 31 ============================\n",
      "weight[-603.36133229], bias[-133.18620494]\n",
      "Train loss: 178909.2925763783 \n",
      "\n",
      "Epoch 32 ============================\n",
      "weight[-603.16605184], bias[-134.36573587]\n",
      "Train loss: 178671.37656589356 \n",
      "\n",
      "Epoch 33 ============================\n",
      "weight[-602.971113], bias[-135.54204814]\n",
      "Train loss: 178434.74480929415 \n",
      "\n",
      "Epoch 34 ============================\n",
      "weight[-602.77657989], bias[-136.71516123]\n",
      "Train loss: 178199.3903609675 \n",
      "\n",
      "Epoch 35 ============================\n",
      "weight[-602.5824937], bias[-137.88509082]\n",
      "Train loss: 177965.30632053348 \n",
      "\n",
      "Epoch 36 ============================\n",
      "weight[-602.38888075], bias[-139.05185009]\n",
      "Train loss: 177732.4858281369 \n",
      "\n",
      "Epoch 37 ============================\n",
      "weight[-602.19575769], bias[-140.21545056]\n",
      "Train loss: 177500.92206233888 \n",
      "\n",
      "Epoch 38 ============================\n",
      "weight[-602.00313485], bias[-141.37590273]\n",
      "Train loss: 177270.60823910925 \n",
      "\n",
      "Epoch 39 ============================\n",
      "weight[-601.81101845], bias[-142.53321634]\n",
      "Train loss: 177041.5376112855 \n",
      "\n",
      "Epoch 40 ============================\n",
      "weight[-601.61941206], bias[-143.68740071]\n",
      "Train loss: 176813.70346823058 \n",
      "\n",
      "Epoch 41 ============================\n",
      "weight[-601.42831752], bias[-144.83846483]\n",
      "Train loss: 176587.0991355749 \n",
      "\n",
      "Epoch 42 ============================\n",
      "weight[-601.23773553], bias[-145.98641747]\n",
      "Train loss: 176361.71797499526 \n",
      "\n",
      "Epoch 43 ============================\n",
      "weight[-601.04766607], bias[-147.13126727]\n",
      "Train loss: 176137.5533840082 \n",
      "\n",
      "Epoch 44 ============================\n",
      "weight[-600.85810864], bias[-148.27302276]\n",
      "Train loss: 175914.5987957737 \n",
      "\n",
      "Epoch 45 ============================\n",
      "weight[-600.66906243], bias[-149.41169241]\n",
      "Train loss: 175692.84767889907 \n",
      "\n",
      "Epoch 46 ============================\n",
      "weight[-600.48052643], bias[-150.5472846]\n",
      "Train loss: 175472.2935372482 \n",
      "\n",
      "Epoch 47 ============================\n",
      "weight[-600.2924995], bias[-151.67980771]\n",
      "Train loss: 175252.92990974974 \n",
      "\n",
      "Epoch 48 ============================\n",
      "weight[-600.10498044], bias[-152.80927004]\n",
      "Train loss: 175034.75037020867 \n",
      "\n",
      "Epoch 49 ============================\n",
      "weight[-599.91796797], bias[-153.9356799]\n",
      "Train loss: 174817.74852711754 \n",
      "\n",
      "Epoch 50 ============================\n",
      "weight[-599.73146079], bias[-155.05904553]\n",
      "Train loss: 174601.91802346855 \n",
      "\n",
      "Epoch 51 ============================\n",
      "weight[-599.54545757], bias[-156.17937518]\n",
      "Train loss: 174387.2525365689 \n",
      "\n",
      "Epoch 52 ============================\n",
      "weight[-599.35995699], bias[-157.29667705]\n",
      "Train loss: 174173.74577785385 \n",
      "\n",
      "Epoch 53 ============================\n",
      "weight[-599.1749577], bias[-158.41095934]\n",
      "Train loss: 173961.3914927038 \n",
      "\n",
      "Epoch 54 ============================\n",
      "weight[-598.99045836], bias[-159.5222302]\n",
      "Train loss: 173750.18346026028 \n",
      "\n",
      "Epoch 55 ============================\n",
      "weight[-598.80645762], bias[-160.63049777]\n",
      "Train loss: 173540.11549324394 \n",
      "\n",
      "Epoch 56 ============================\n",
      "weight[-598.62295416], bias[-161.73577017]\n",
      "Train loss: 173331.18143777348 \n",
      "\n",
      "Epoch 57 ============================\n",
      "weight[-598.43994661], bias[-162.83805549]\n",
      "Train loss: 173123.3751731851 \n",
      "\n",
      "Epoch 58 ============================\n",
      "weight[-598.25743366], bias[-163.93736181]\n",
      "Train loss: 172916.69061185332 \n",
      "\n",
      "Epoch 59 ============================\n",
      "weight[-598.07541395], bias[-165.03369719]\n",
      "Train loss: 172711.1216990126 \n",
      "\n",
      "Epoch 60 ============================\n",
      "weight[-597.89388617], bias[-166.12706964]\n",
      "Train loss: 172506.6624125803 \n",
      "\n",
      "Epoch 61 ============================\n",
      "weight[-597.71284897], bias[-167.21748718]\n",
      "Train loss: 172303.30676297963 \n",
      "\n",
      "Epoch 62 ============================\n",
      "weight[-597.53230104], bias[-168.30495779]\n",
      "Train loss: 172101.04879296455 \n",
      "\n",
      "Epoch 63 ============================\n",
      "weight[-597.35224105], bias[-169.38948944]\n",
      "Train loss: 171899.88257744556 \n",
      "\n",
      "Epoch 64 ============================\n",
      "weight[-597.17266768], bias[-170.47109007]\n",
      "Train loss: 171699.8022233151 \n",
      "\n",
      "Epoch 65 ============================\n",
      "weight[-596.99357963], bias[-171.5497676]\n",
      "Train loss: 171500.80186927653 \n",
      "\n",
      "Epoch 66 ============================\n",
      "weight[-596.81497557], bias[-172.62552993]\n",
      "Train loss: 171302.87568567085 \n",
      "\n",
      "Epoch 67 ============================\n",
      "weight[-596.6368542], bias[-173.69838493]\n",
      "Train loss: 171106.0178743068 \n",
      "\n",
      "Epoch 68 ============================\n",
      "weight[-596.45921422], bias[-174.76834048]\n",
      "Train loss: 170910.2226682908 \n",
      "\n",
      "Epoch 69 ============================\n",
      "weight[-596.28205432], bias[-175.8354044]\n",
      "Train loss: 170715.48433185797 \n",
      "\n",
      "Epoch 70 ============================\n",
      "weight[-596.1053732], bias[-176.8995845]\n",
      "Train loss: 170521.7971602039 \n",
      "\n",
      "Epoch 71 ============================\n",
      "weight[-595.92916958], bias[-177.96088859]\n",
      "Train loss: 170329.15547931808 \n",
      "\n",
      "Epoch 72 ============================\n",
      "weight[-595.75344216], bias[-179.01932443]\n",
      "Train loss: 170137.5536458173 \n",
      "\n",
      "Epoch 73 ============================\n",
      "weight[-595.57818966], bias[-180.07489978]\n",
      "Train loss: 169946.98604678005 \n",
      "\n",
      "Epoch 74 ============================\n",
      "weight[-595.40341079], bias[-181.12762236]\n",
      "Train loss: 169757.44709958314 \n",
      "\n",
      "Epoch 75 ============================\n",
      "weight[-595.22910427], bias[-182.17749989]\n",
      "Train loss: 169568.93125173653 \n",
      "\n",
      "Epoch 76 ============================\n",
      "weight[-595.05526882], bias[-183.22454005]\n",
      "Train loss: 169381.4329807225 \n",
      "\n",
      "Epoch 77 ============================\n",
      "weight[-594.88190318], bias[-184.26875051]\n",
      "Train loss: 169194.94679383186 \n",
      "\n",
      "Epoch 78 ============================\n",
      "weight[-594.70900607], bias[-185.31013893]\n",
      "Train loss: 169009.4672280055 \n",
      "\n",
      "Epoch 79 ============================\n",
      "weight[-594.53657623], bias[-186.34871292]\n",
      "Train loss: 168824.9888496717 \n",
      "\n",
      "Epoch 80 ============================\n",
      "weight[-594.36461239], bias[-187.3844801]\n",
      "Train loss: 168641.50625458907 \n",
      "\n",
      "Epoch 81 ============================\n",
      "weight[-594.19311329], bias[-188.41744804]\n",
      "Train loss: 168459.01406768718 \n",
      "\n",
      "Epoch 82 ============================\n",
      "weight[-594.02207768], bias[-189.44762432]\n",
      "Train loss: 168277.50694290907 \n",
      "\n",
      "Epoch 83 ============================\n",
      "weight[-593.85150431], bias[-190.47501648]\n",
      "Train loss: 168096.9795630553 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 ============================\n",
      "weight[-593.68139192], bias[-191.49963204]\n",
      "Train loss: 167917.42663962767 \n",
      "\n",
      "Epoch 85 ============================\n",
      "weight[-593.51173927], bias[-192.52147851]\n",
      "Train loss: 167738.84291267407 \n",
      "\n",
      "Epoch 86 ============================\n",
      "weight[-593.34254512], bias[-193.54056337]\n",
      "Train loss: 167561.22315063505 \n",
      "\n",
      "Epoch 87 ============================\n",
      "weight[-593.17380823], bias[-194.55689408]\n",
      "Train loss: 167384.56215019035 \n",
      "\n",
      "Epoch 88 ============================\n",
      "weight[-593.00552737], bias[-195.57047809]\n",
      "Train loss: 167208.85473610592 \n",
      "\n",
      "Epoch 89 ============================\n",
      "weight[-592.83770129], bias[-196.58132282]\n",
      "Train loss: 167034.0957610832 \n",
      "\n",
      "Epoch 90 ============================\n",
      "weight[-592.67032877], bias[-197.58943568]\n",
      "Train loss: 166860.2801056076 \n",
      "\n",
      "Epoch 91 ============================\n",
      "weight[-592.50340859], bias[-198.59482404]\n",
      "Train loss: 166687.40267779908 \n",
      "\n",
      "Epoch 92 ============================\n",
      "weight[-592.33693952], bias[-199.59749527]\n",
      "Train loss: 166515.45841326233 \n",
      "\n",
      "Epoch 93 ============================\n",
      "weight[-592.17092035], bias[-200.59745671]\n",
      "Train loss: 166344.442274939 \n",
      "\n",
      "Epoch 94 ============================\n",
      "weight[-592.00534986], bias[-201.59471569]\n",
      "Train loss: 166174.34925296003 \n",
      "\n",
      "Epoch 95 ============================\n",
      "weight[-591.84022683], bias[-202.58927951]\n",
      "Train loss: 166005.17436449867 \n",
      "\n",
      "Epoch 96 ============================\n",
      "weight[-591.67555005], bias[-203.58115545]\n",
      "Train loss: 165836.91265362463 \n",
      "\n",
      "Epoch 97 ============================\n",
      "weight[-591.51131833], bias[-204.57035079]\n",
      "Train loss: 165669.55919115897 \n",
      "\n",
      "Epoch 98 ============================\n",
      "weight[-591.34753045], bias[-205.55687275]\n",
      "Train loss: 165503.1090745294 \n",
      "\n",
      "Epoch 99 ============================\n",
      "weight[-591.18418523], bias[-206.54072857]\n",
      "Train loss: 165337.55742762733 \n",
      "\n",
      "Epoch 100 ============================\n",
      "weight[-591.02128145], bias[-207.52192546]\n",
      "Train loss: 165172.89940066403 \n",
      "\n",
      "Epoch 101 ============================\n",
      "weight[-590.85881793], bias[-208.50047059]\n",
      "Train loss: 165009.13017002968 \n",
      "\n",
      "Epoch 102 ============================\n",
      "weight[-590.69679348], bias[-209.47637114]\n",
      "Train loss: 164846.24493815113 \n",
      "\n",
      "Epoch 103 ============================\n",
      "weight[-590.53520691], bias[-210.44963425]\n",
      "Train loss: 164684.2389333517 \n",
      "\n",
      "Epoch 104 ============================\n",
      "weight[-590.37405705], bias[-211.42026705]\n",
      "Train loss: 164523.1074097116 \n",
      "\n",
      "Epoch 105 ============================\n",
      "weight[-590.2133427], bias[-212.38827665]\n",
      "Train loss: 164362.84564692885 \n",
      "\n",
      "Epoch 106 ============================\n",
      "weight[-590.05306269], bias[-213.35367014]\n",
      "Train loss: 164203.44895018038 \n",
      "\n",
      "Epoch 107 ============================\n",
      "weight[-589.89321584], bias[-214.31645459]\n",
      "Train loss: 164044.91264998532 \n",
      "\n",
      "Epoch 108 ============================\n",
      "weight[-589.733801], bias[-215.27663705]\n",
      "Train loss: 163887.23210206802 \n",
      "\n",
      "Epoch 109 ============================\n",
      "weight[-589.57481699], bias[-216.23422455]\n",
      "Train loss: 163730.4026872213 \n",
      "\n",
      "Epoch 110 ============================\n",
      "weight[-589.41626264], bias[-217.1892241]\n",
      "Train loss: 163574.4198111722 \n",
      "\n",
      "Epoch 111 ============================\n",
      "weight[-589.25813679], bias[-218.1416427]\n",
      "Train loss: 163419.278904447 \n",
      "\n",
      "Epoch 112 ============================\n",
      "weight[-589.10043829], bias[-219.09148733]\n",
      "Train loss: 163264.97542223646 \n",
      "\n",
      "Epoch 113 ============================\n",
      "weight[-588.94316598], bias[-220.03876494]\n",
      "Train loss: 163111.50484426421 \n",
      "\n",
      "Epoch 114 ============================\n",
      "weight[-588.78631871], bias[-220.98348246]\n",
      "Train loss: 162958.86267465318 \n",
      "\n",
      "Epoch 115 ============================\n",
      "weight[-588.62989533], bias[-221.92564683]\n",
      "Train loss: 162807.0444417946 \n",
      "\n",
      "Epoch 116 ============================\n",
      "weight[-588.4738947], bias[-222.86526493]\n",
      "Train loss: 162656.0456982161 \n",
      "\n",
      "Epoch 117 ============================\n",
      "weight[-588.31831566], bias[-223.80234364]\n",
      "Train loss: 162505.86202045277 \n",
      "\n",
      "Epoch 118 ============================\n",
      "weight[-588.16315709], bias[-224.73688984]\n",
      "Train loss: 162356.48900891645 \n",
      "\n",
      "Epoch 119 ============================\n",
      "weight[-588.00841785], bias[-225.66891037]\n",
      "Train loss: 162207.92228776694 \n",
      "\n",
      "Epoch 120 ============================\n",
      "weight[-587.8540968], bias[-226.59841204]\n",
      "Train loss: 162060.1575047846 \n",
      "\n",
      "Epoch 121 ============================\n",
      "weight[-587.70019281], bias[-227.52540168]\n",
      "Train loss: 161913.19033124225 \n",
      "\n",
      "Epoch 122 ============================\n",
      "weight[-587.54670476], bias[-228.44988606]\n",
      "Train loss: 161767.01646177788 \n",
      "\n",
      "Epoch 123 ============================\n",
      "weight[-587.39363152], bias[-229.37187196]\n",
      "Train loss: 161621.63161426992 \n",
      "\n",
      "Epoch 124 ============================\n",
      "weight[-587.24097198], bias[-230.29136613]\n",
      "Train loss: 161477.0315297104 \n",
      "\n",
      "Epoch 125 ============================\n",
      "weight[-587.088725], bias[-231.20837531]\n",
      "Train loss: 161333.21197208107 \n",
      "\n",
      "Epoch 126 ============================\n",
      "weight[-586.93688948], bias[-232.1229062]\n",
      "Train loss: 161190.16872822875 \n",
      "\n",
      "Epoch 127 ============================\n",
      "weight[-586.78546431], bias[-233.03496552]\n",
      "Train loss: 161047.8976077423 \n",
      "\n",
      "Epoch 128 ============================\n",
      "weight[-586.63444838], bias[-233.94455993]\n",
      "Train loss: 160906.3944428298 \n",
      "\n",
      "Epoch 129 ============================\n",
      "weight[-586.48384057], bias[-234.8516961]\n",
      "Train loss: 160765.65508819613 \n",
      "\n",
      "Epoch 130 ============================\n",
      "weight[-586.3336398], bias[-235.75638068]\n",
      "Train loss: 160625.67542092197 \n",
      "\n",
      "Epoch 131 ============================\n",
      "weight[-586.18384495], bias[-236.65862028]\n",
      "Train loss: 160486.4513403426 \n",
      "\n",
      "Epoch 132 ============================\n",
      "weight[-586.03445493], bias[-237.55842152]\n",
      "Train loss: 160347.97876792832 \n",
      "\n",
      "Epoch 133 ============================\n",
      "weight[-585.88546865], bias[-238.45579099]\n",
      "Train loss: 160210.25364716462 \n",
      "\n",
      "Epoch 134 ============================\n",
      "weight[-585.73688501], bias[-239.35073525]\n",
      "Train loss: 160073.27194343283 \n",
      "\n",
      "Epoch 135 ============================\n",
      "weight[-585.58870293], bias[-240.24326087]\n",
      "Train loss: 159937.02964389336 \n",
      "\n",
      "Epoch 136 ============================\n",
      "weight[-585.44092133], bias[-241.13337437]\n",
      "Train loss: 159801.5227573662 \n",
      "\n",
      "Epoch 137 ============================\n",
      "weight[-585.29353911], bias[-242.02108228]\n",
      "Train loss: 159666.74731421625 \n",
      "\n",
      "Epoch 138 ============================\n",
      "weight[-585.14655521], bias[-242.90639111]\n",
      "Train loss: 159532.69936623468 \n",
      "\n",
      "Epoch 139 ============================\n",
      "weight[-584.99996853], bias[-243.78930732]\n",
      "Train loss: 159399.37498652542 \n",
      "\n",
      "Epoch 140 ============================\n",
      "weight[-584.85377802], bias[-244.6698374]\n",
      "Train loss: 159266.7702693882 \n",
      "\n",
      "Epoch 141 ============================\n",
      "weight[-584.7079826], bias[-245.54798778]\n",
      "Train loss: 159134.88133020594 \n",
      "\n",
      "Epoch 142 ============================\n",
      "weight[-584.5625812], bias[-246.4237649]\n",
      "Train loss: 159003.70430532933 \n",
      "\n",
      "Epoch 143 ============================\n",
      "weight[-584.41757276], bias[-247.29717517]\n",
      "Train loss: 158873.23535196495 \n",
      "\n",
      "Epoch 144 ============================\n",
      "weight[-584.27295621], bias[-248.168225]\n",
      "Train loss: 158743.47064806186 \n",
      "\n",
      "Epoch 145 ============================\n",
      "weight[-584.1287305], bias[-249.03692075]\n",
      "Train loss: 158614.4063921997 \n",
      "\n",
      "Epoch 146 ============================\n",
      "weight[-583.98489457], bias[-249.90326879]\n",
      "Train loss: 158486.0388034782 \n",
      "\n",
      "Epoch 147 ============================\n",
      "weight[-583.84144736], bias[-250.76727547]\n",
      "Train loss: 158358.36412140494 \n",
      "\n",
      "Epoch 148 ============================\n",
      "weight[-583.69838783], bias[-251.62894711]\n",
      "Train loss: 158231.37860578662 \n",
      "\n",
      "Epoch 149 ============================\n",
      "weight[-583.55571493], bias[-252.48829003]\n",
      "Train loss: 158105.07853661865 \n",
      "\n",
      "Epoch 150 ============================\n",
      "weight[-583.41342761], bias[-253.34531051]\n",
      "Train loss: 157979.46021397642 \n",
      "\n",
      "Epoch 151 ============================\n",
      "weight[-583.27152484], bias[-254.20001484]\n",
      "Train loss: 157854.5199579068 \n",
      "\n",
      "Epoch 152 ============================\n",
      "weight[-583.13000556], bias[-255.05240927]\n",
      "Train loss: 157730.25410832037 \n",
      "\n",
      "Epoch 153 ============================\n",
      "weight[-582.98886875], bias[-255.90250005]\n",
      "Train loss: 157606.6590248847 \n",
      "\n",
      "Epoch 154 ============================\n",
      "weight[-582.84811337], bias[-256.7502934]\n",
      "Train loss: 157483.73108691664 \n",
      "\n",
      "Epoch 155 ============================\n",
      "weight[-582.7077384], bias[-257.59579553]\n",
      "Train loss: 157361.4666932775 \n",
      "\n",
      "Epoch 156 ============================\n",
      "weight[-582.56774279], bias[-258.43901263]\n",
      "Train loss: 157239.8622622665 \n",
      "\n",
      "Epoch 157 ============================\n",
      "weight[-582.42812554], bias[-259.27995088]\n",
      "Train loss: 157118.91423151622 \n",
      "\n",
      "Epoch 158 ============================\n",
      "weight[-582.2888856], bias[-260.11861644]\n",
      "Train loss: 156998.61905788866 \n",
      "\n",
      "Epoch 159 ============================\n",
      "weight[-582.15002198], bias[-260.95501545]\n",
      "Train loss: 156878.97321737095 \n",
      "\n",
      "Epoch 160 ============================\n",
      "weight[-582.01153364], bias[-261.78915404]\n",
      "Train loss: 156759.9732049721 \n",
      "\n",
      "Epoch 161 ============================\n",
      "weight[-581.87341958], bias[-262.6210383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 156641.61553462048 \n",
      "\n",
      "Epoch 162 ============================\n",
      "weight[-581.73567878], bias[-263.45067435]\n",
      "Train loss: 156523.896739062 \n",
      "\n",
      "Epoch 163 ============================\n",
      "weight[-581.59831023], bias[-264.27806824]\n",
      "Train loss: 156406.81336975793 \n",
      "\n",
      "Epoch 164 ============================\n",
      "weight[-581.46131293], bias[-265.10322605]\n",
      "Train loss: 156290.3619967841 \n",
      "\n",
      "Epoch 165 ============================\n",
      "weight[-581.32468587], bias[-265.92615382]\n",
      "Train loss: 156174.53920873074 \n",
      "\n",
      "Epoch 166 ============================\n",
      "weight[-581.18842806], bias[-266.74685756]\n",
      "Train loss: 156059.34161260238 \n",
      "\n",
      "Epoch 167 ============================\n",
      "weight[-581.0525385], bias[-267.5653433]\n",
      "Train loss: 155944.76583371792 \n",
      "\n",
      "Epoch 168 ============================\n",
      "weight[-580.91701618], bias[-268.38161702]\n",
      "Train loss: 155830.80851561276 \n",
      "\n",
      "Epoch 169 ============================\n",
      "weight[-580.78186013], bias[-269.19568471]\n",
      "Train loss: 155717.46631993935 \n",
      "\n",
      "Epoch 170 ============================\n",
      "weight[-580.64706934], bias[-270.00755233]\n",
      "Train loss: 155604.73592637034 \n",
      "\n",
      "Epoch 171 ============================\n",
      "weight[-580.51264283], bias[-270.81722582]\n",
      "Train loss: 155492.61403250106 \n",
      "\n",
      "Epoch 172 ============================\n",
      "weight[-580.37857962], bias[-271.62471111]\n",
      "Train loss: 155381.0973537525 \n",
      "\n",
      "Epoch 173 ============================\n",
      "weight[-580.24487873], bias[-272.43001412]\n",
      "Train loss: 155270.18262327512 \n",
      "\n",
      "Epoch 174 ============================\n",
      "weight[-580.11153917], bias[-273.23314074]\n",
      "Train loss: 155159.86659185347 \n",
      "\n",
      "Epoch 175 ============================\n",
      "weight[-579.97855997], bias[-274.03409686]\n",
      "Train loss: 155050.14602781078 \n",
      "\n",
      "Epoch 176 ============================\n",
      "weight[-579.84594016], bias[-274.83288834]\n",
      "Train loss: 154941.01771691433 \n",
      "\n",
      "Epoch 177 ============================\n",
      "weight[-579.71367876], bias[-275.62952103]\n",
      "Train loss: 154832.4784622812 \n",
      "\n",
      "Epoch 178 ============================\n",
      "weight[-579.5817748], bias[-276.42400076]\n",
      "Train loss: 154724.52508428483 \n",
      "\n",
      "Epoch 179 ============================\n",
      "weight[-579.45022733], bias[-277.21633337]\n",
      "Train loss: 154617.1544204617 \n",
      "\n",
      "Epoch 180 ============================\n",
      "weight[-579.31903537], bias[-278.00652464]\n",
      "Train loss: 154510.3633254187 \n",
      "\n",
      "Epoch 181 ============================\n",
      "weight[-579.18819796], bias[-278.79458036]\n",
      "Train loss: 154404.1486707412 \n",
      "\n",
      "Epoch 182 ============================\n",
      "weight[-579.05771416], bias[-279.58050631]\n",
      "Train loss: 154298.50734490086 \n",
      "\n",
      "Epoch 183 ============================\n",
      "weight[-578.92758299], bias[-280.36430824]\n",
      "Train loss: 154193.43625316516 \n",
      "\n",
      "Epoch 184 ============================\n",
      "weight[-578.79780351], bias[-281.14599189]\n",
      "Train loss: 154088.93231750652 \n",
      "\n",
      "Epoch 185 ============================\n",
      "weight[-578.66837477], bias[-281.92556299]\n",
      "Train loss: 153984.99247651148 \n",
      "\n",
      "Epoch 186 ============================\n",
      "weight[-578.53929582], bias[-282.70302725]\n",
      "Train loss: 153881.6136852923 \n",
      "\n",
      "Epoch 187 ============================\n",
      "weight[-578.41056572], bias[-283.47839036]\n",
      "Train loss: 153778.79291539642 \n",
      "\n",
      "Epoch 188 ============================\n",
      "weight[-578.28218351], bias[-284.25165799]\n",
      "Train loss: 153676.52715471885 \n",
      "\n",
      "Epoch 189 ============================\n",
      "weight[-578.15414827], bias[-285.02283582]\n",
      "Train loss: 153574.81340741293 \n",
      "\n",
      "Epoch 190 ============================\n",
      "weight[-578.02645905], bias[-285.79192949]\n",
      "Train loss: 153473.64869380358 \n",
      "\n",
      "Epoch 191 ============================\n",
      "weight[-577.89911492], bias[-286.55894463]\n",
      "Train loss: 153373.03005029904 \n",
      "\n",
      "Epoch 192 ============================\n",
      "weight[-577.77211495], bias[-287.32388686]\n",
      "Train loss: 153272.95452930496 \n",
      "\n",
      "Epoch 193 ============================\n",
      "weight[-577.6454582], bias[-288.08676178]\n",
      "Train loss: 153173.41919913734 \n",
      "\n",
      "Epoch 194 ============================\n",
      "weight[-577.51914375], bias[-288.84757498]\n",
      "Train loss: 153074.42114393695 \n",
      "\n",
      "Epoch 195 ============================\n",
      "weight[-577.39317068], bias[-289.60633203]\n",
      "Train loss: 152975.95746358394 \n",
      "\n",
      "Epoch 196 ============================\n",
      "weight[-577.26753805], bias[-290.36303848]\n",
      "Train loss: 152878.02527361276 \n",
      "\n",
      "Epoch 197 ============================\n",
      "weight[-577.14224496], bias[-291.11769989]\n",
      "Train loss: 152780.62170512765 \n",
      "\n",
      "Epoch 198 ============================\n",
      "weight[-577.01729048], bias[-291.87032177]\n",
      "Train loss: 152683.74390471895 \n",
      "\n",
      "Epoch 199 ============================\n",
      "weight[-576.8926737], bias[-292.62090965]\n",
      "Train loss: 152587.38903437884 \n",
      "\n",
      "Epoch 200 ============================\n",
      "weight[-576.7683937], bias[-293.369469]\n",
      "Train loss: 152491.55427141895 \n",
      "\n",
      "Epoch 201 ============================\n",
      "weight[-576.64444958], bias[-294.11600533]\n",
      "Train loss: 152396.23680838698 \n",
      "\n",
      "Epoch 202 ============================\n",
      "weight[-576.52084042], bias[-294.86052409]\n",
      "Train loss: 152301.43385298504 \n",
      "\n",
      "Epoch 203 ============================\n",
      "weight[-576.39756533], bias[-295.60303074]\n",
      "Train loss: 152207.14262798757 \n",
      "\n",
      "Epoch 204 ============================\n",
      "weight[-576.2746234], bias[-296.34353071]\n",
      "Train loss: 152113.36037115988 \n",
      "\n",
      "Epoch 205 ============================\n",
      "weight[-576.15201372], bias[-297.08202943]\n",
      "Train loss: 152020.08433517735 \n",
      "\n",
      "Epoch 206 ============================\n",
      "weight[-576.02973541], bias[-297.81853231]\n",
      "Train loss: 151927.31178754495 \n",
      "\n",
      "Epoch 207 ============================\n",
      "weight[-575.90778756], bias[-298.55304474]\n",
      "Train loss: 151835.04001051738 \n",
      "\n",
      "Epoch 208 ============================\n",
      "weight[-575.78616928], bias[-299.2855721]\n",
      "Train loss: 151743.26630101944 \n",
      "\n",
      "Epoch 209 ============================\n",
      "weight[-575.66487969], bias[-300.01611975]\n",
      "Train loss: 151651.9879705659 \n",
      "\n",
      "Epoch 210 ============================\n",
      "weight[-575.54391789], bias[-300.74469305]\n",
      "Train loss: 151561.2023451842 \n",
      "\n",
      "Epoch 211 ============================\n",
      "weight[-575.423283], bias[-301.47129733]\n",
      "Train loss: 151470.90676533544 \n",
      "\n",
      "Epoch 212 ============================\n",
      "weight[-575.30297413], bias[-302.19593792]\n",
      "Train loss: 151381.09858583592 \n",
      "\n",
      "Epoch 213 ============================\n",
      "weight[-575.1829904], bias[-302.91862011]\n",
      "Train loss: 151291.77517578084 \n",
      "\n",
      "Epoch 214 ============================\n",
      "weight[-575.06333094], bias[-303.63934921]\n",
      "Train loss: 151202.9339184663 \n",
      "\n",
      "Epoch 215 ============================\n",
      "weight[-574.94399486], bias[-304.35813049]\n",
      "Train loss: 151114.57221131262 \n",
      "\n",
      "Epoch 216 ============================\n",
      "weight[-574.8249813], bias[-305.07496921]\n",
      "Train loss: 151026.68746578912 \n",
      "\n",
      "Epoch 217 ============================\n",
      "weight[-574.70628938], bias[-305.78987062]\n",
      "Train loss: 150939.27710733682 \n",
      "\n",
      "Epoch 218 ============================\n",
      "weight[-574.58791824], bias[-306.50283997]\n",
      "Train loss: 150852.33857529436 \n",
      "\n",
      "Epoch 219 ============================\n",
      "weight[-574.469867], bias[-307.21388247]\n",
      "Train loss: 150765.86932282196 \n",
      "\n",
      "Epoch 220 ============================\n",
      "weight[-574.3521348], bias[-307.92300333]\n",
      "Train loss: 150679.86681682753 \n",
      "\n",
      "Epoch 221 ============================\n",
      "weight[-574.23472078], bias[-308.63020774]\n",
      "Train loss: 150594.32853789214 \n",
      "\n",
      "Epoch 222 ============================\n",
      "weight[-574.11762408], bias[-309.33550088]\n",
      "Train loss: 150509.25198019607 \n",
      "\n",
      "Epoch 223 ============================\n",
      "weight[-574.00084385], bias[-310.03888792]\n",
      "Train loss: 150424.6346514459 \n",
      "\n",
      "Epoch 224 ============================\n",
      "weight[-573.88437921], bias[-310.74037401]\n",
      "Train loss: 150340.47407280104 \n",
      "\n",
      "Epoch 225 ============================\n",
      "weight[-573.76822934], bias[-311.43996429]\n",
      "Train loss: 150256.76777880127 \n",
      "\n",
      "Epoch 226 ============================\n",
      "weight[-573.65239336], bias[-312.13766387]\n",
      "Train loss: 150173.51331729442 \n",
      "\n",
      "Epoch 227 ============================\n",
      "weight[-573.53687044], bias[-312.83347788]\n",
      "Train loss: 150090.70824936483 \n",
      "\n",
      "Epoch 228 ============================\n",
      "weight[-573.42165973], bias[-313.5274114]\n",
      "Train loss: 150008.3501492613 \n",
      "\n",
      "Epoch 229 ============================\n",
      "weight[-573.30676038], bias[-314.21946952]\n",
      "Train loss: 149926.4366043269 \n",
      "\n",
      "Epoch 230 ============================\n",
      "weight[-573.19217156], bias[-314.9096573]\n",
      "Train loss: 149844.96521492768 \n",
      "\n",
      "Epoch 231 ============================\n",
      "weight[-573.07789242], bias[-315.59797981]\n",
      "Train loss: 149763.93359438222 \n",
      "\n",
      "Epoch 232 ============================\n",
      "weight[-572.96392213], bias[-316.28444207]\n",
      "Train loss: 149683.3393688925 \n",
      "\n",
      "Epoch 233 ============================\n",
      "weight[-572.85025985], bias[-316.96904913]\n",
      "Train loss: 149603.1801774736 \n",
      "\n",
      "Epoch 234 ============================\n",
      "weight[-572.73690475], bias[-317.65180599]\n",
      "Train loss: 149523.45367188434 \n",
      "\n",
      "Epoch 235 ============================\n",
      "weight[-572.623856], bias[-318.33271765]\n",
      "Train loss: 149444.15751655996 \n",
      "\n",
      "Epoch 236 ============================\n",
      "weight[-572.51111277], bias[-319.0117891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 149365.2893885418 \n",
      "\n",
      "Epoch 237 ============================\n",
      "weight[-572.39867424], bias[-319.68902532]\n",
      "Train loss: 149286.84697741058 \n",
      "\n",
      "Epoch 238 ============================\n",
      "weight[-572.28653958], bias[-320.36443126]\n",
      "Train loss: 149208.82798521811 \n",
      "\n",
      "Epoch 239 ============================\n",
      "weight[-572.17470797], bias[-321.03801187]\n",
      "Train loss: 149131.23012642007 \n",
      "\n",
      "Epoch 240 ============================\n",
      "weight[-572.0631786], bias[-321.70977208]\n",
      "Train loss: 149054.05112780968 \n",
      "\n",
      "Epoch 241 ============================\n",
      "weight[-571.95195064], bias[-322.37971681]\n",
      "Train loss: 148977.2887284498 \n",
      "\n",
      "Epoch 242 ============================\n",
      "weight[-571.84102328], bias[-323.04785097]\n",
      "Train loss: 148900.94067960765 \n",
      "\n",
      "Epoch 243 ============================\n",
      "weight[-571.73039571], bias[-323.71417946]\n",
      "Train loss: 148825.00474468895 \n",
      "\n",
      "Epoch 244 ============================\n",
      "weight[-571.62006712], bias[-324.37870715]\n",
      "Train loss: 148749.47869917218 \n",
      "\n",
      "Epoch 245 ============================\n",
      "weight[-571.5100367], bias[-325.0414389]\n",
      "Train loss: 148674.36033054322 \n",
      "\n",
      "Epoch 246 ============================\n",
      "weight[-571.40030364], bias[-325.70237958]\n",
      "Train loss: 148599.64743823087 \n",
      "\n",
      "Epoch 247 ============================\n",
      "weight[-571.29086715], bias[-326.36153402]\n",
      "Train loss: 148525.33783354214 \n",
      "\n",
      "Epoch 248 ============================\n",
      "weight[-571.18172641], bias[-327.01890705]\n",
      "Train loss: 148451.42933959825 \n",
      "\n",
      "Epoch 249 ============================\n",
      "weight[-571.07288064], bias[-327.67450349]\n",
      "Train loss: 148377.91979127095 \n",
      "\n",
      "Epoch 250 ============================\n",
      "weight[-570.96432903], bias[-328.32832813]\n",
      "Train loss: 148304.80703511872 \n",
      "\n",
      "Epoch 251 ============================\n",
      "weight[-570.85607078], bias[-328.98038577]\n",
      "Train loss: 148232.08892932456 \n",
      "\n",
      "Epoch 252 ============================\n",
      "weight[-570.74810511], bias[-329.63068117]\n",
      "Train loss: 148159.76334363184 \n",
      "\n",
      "Epoch 253 ============================\n",
      "weight[-570.64043123], bias[-330.27921911]\n",
      "Train loss: 148087.82815928318 \n",
      "\n",
      "Epoch 254 ============================\n",
      "weight[-570.53304834], bias[-330.92600434]\n",
      "Train loss: 148016.281268958 \n",
      "\n",
      "Epoch 255 ============================\n",
      "weight[-570.42595566], bias[-331.57104158]\n",
      "Train loss: 147945.12057671018 \n",
      "\n",
      "Epoch 256 ============================\n",
      "weight[-570.31915241], bias[-332.21433556]\n",
      "Train loss: 147874.34399790762 \n",
      "\n",
      "Epoch 257 ============================\n",
      "weight[-570.2126378], bias[-332.855891]\n",
      "Train loss: 147803.9494591707 \n",
      "\n",
      "Epoch 258 ============================\n",
      "weight[-570.10641105], bias[-333.49571259]\n",
      "Train loss: 147733.93489831118 \n",
      "\n",
      "Epoch 259 ============================\n",
      "weight[-570.00047139], bias[-334.13380502]\n",
      "Train loss: 147664.29826427248 \n",
      "\n",
      "Epoch 260 ============================\n",
      "weight[-569.89481804], bias[-334.77017296]\n",
      "Train loss: 147595.03751706943 \n",
      "\n",
      "Epoch 261 ============================\n",
      "weight[-569.78945022], bias[-335.40482108]\n",
      "Train loss: 147526.15062772765 \n",
      "\n",
      "Epoch 262 ============================\n",
      "weight[-569.68436716], bias[-336.03775401]\n",
      "Train loss: 147457.63557822583 \n",
      "\n",
      "Epoch 263 ============================\n",
      "weight[-569.5795681], bias[-336.6689764]\n",
      "Train loss: 147389.49036143455 \n",
      "\n",
      "Epoch 264 ============================\n",
      "weight[-569.47505227], bias[-337.29849287]\n",
      "Train loss: 147321.71298105965 \n",
      "\n",
      "Epoch 265 ============================\n",
      "weight[-569.3708189], bias[-337.92630803]\n",
      "Train loss: 147254.30145158144 \n",
      "\n",
      "Epoch 266 ============================\n",
      "weight[-569.26686722], bias[-338.55242648]\n",
      "Train loss: 147187.25379819865 \n",
      "\n",
      "Epoch 267 ============================\n",
      "weight[-569.16319648], bias[-339.17685279]\n",
      "Train loss: 147120.56805676932 \n",
      "\n",
      "Epoch 268 ============================\n",
      "weight[-569.05980592], bias[-339.79959156]\n",
      "Train loss: 147054.2422737535 \n",
      "\n",
      "Epoch 269 ============================\n",
      "weight[-568.95669478], bias[-340.42064733]\n",
      "Train loss: 146988.27450615633 \n",
      "\n",
      "Epoch 270 ============================\n",
      "weight[-568.85386231], bias[-341.04002465]\n",
      "Train loss: 146922.6628214708 \n",
      "\n",
      "Epoch 271 ============================\n",
      "weight[-568.75130774], bias[-341.65772807]\n",
      "Train loss: 146857.40529762118 \n",
      "\n",
      "Epoch 272 ============================\n",
      "weight[-568.64903034], bias[-342.2737621]\n",
      "Train loss: 146792.500022907 \n",
      "\n",
      "Epoch 273 ============================\n",
      "weight[-568.54702935], bias[-342.88813125]\n",
      "Train loss: 146727.94509594643 \n",
      "\n",
      "Epoch 274 ============================\n",
      "weight[-568.44530402], bias[-343.50084003]\n",
      "Train loss: 146663.73862562157 \n",
      "\n",
      "Epoch 275 ============================\n",
      "weight[-568.34385361], bias[-344.11189293]\n",
      "Train loss: 146599.8787310219 \n",
      "\n",
      "Epoch 276 ============================\n",
      "weight[-568.24267738], bias[-344.72129441]\n",
      "Train loss: 146536.3635413899 \n",
      "\n",
      "Epoch 277 ============================\n",
      "weight[-568.14177458], bias[-345.32904894]\n",
      "Train loss: 146473.19119606641 \n",
      "\n",
      "Epoch 278 ============================\n",
      "weight[-568.04114448], bias[-345.93516098]\n",
      "Train loss: 146410.35984443538 \n",
      "\n",
      "Epoch 279 ============================\n",
      "weight[-567.94078634], bias[-346.53963496]\n",
      "Train loss: 146347.86764587008 \n",
      "\n",
      "Epoch 280 ============================\n",
      "weight[-567.84069943], bias[-347.1424753]\n",
      "Train loss: 146285.7127696796 \n",
      "\n",
      "Epoch 281 ============================\n",
      "weight[-567.74088301], bias[-347.74368643]\n",
      "Train loss: 146223.89339505442 \n",
      "\n",
      "Epoch 282 ============================\n",
      "weight[-567.64133634], bias[-348.34327275]\n",
      "Train loss: 146162.40771101337 \n",
      "\n",
      "Epoch 283 ============================\n",
      "weight[-567.54205871], bias[-348.94123864]\n",
      "Train loss: 146101.25391635098 \n",
      "\n",
      "Epoch 284 ============================\n",
      "weight[-567.44304939], bias[-349.53758849]\n",
      "Train loss: 146040.43021958412 \n",
      "\n",
      "Epoch 285 ============================\n",
      "weight[-567.34430764], bias[-350.13232666]\n",
      "Train loss: 145979.93483889994 \n",
      "\n",
      "Epoch 286 ============================\n",
      "weight[-567.24583275], bias[-350.72545751]\n",
      "Train loss: 145919.76600210348 \n",
      "\n",
      "Epoch 287 ============================\n",
      "weight[-567.14762399], bias[-351.31698539]\n",
      "Train loss: 145859.92194656565 \n",
      "\n",
      "Epoch 288 ============================\n",
      "weight[-567.04968065], bias[-351.90691462]\n",
      "Train loss: 145800.40091917207 \n",
      "\n",
      "Epoch 289 ============================\n",
      "weight[-566.95200201], bias[-352.49524953]\n",
      "Train loss: 145741.20117627093 \n",
      "\n",
      "Epoch 290 ============================\n",
      "weight[-566.85458736], bias[-353.08199442]\n",
      "Train loss: 145682.32098362283 \n",
      "\n",
      "Epoch 291 ============================\n",
      "weight[-566.75743597], bias[-353.66715359]\n",
      "Train loss: 145623.75861634893 \n",
      "\n",
      "Epoch 292 ============================\n",
      "weight[-566.66054714], bias[-354.25073134]\n",
      "Train loss: 145565.51235888165 \n",
      "\n",
      "Epoch 293 ============================\n",
      "weight[-566.56392016], bias[-354.83273192]\n",
      "Train loss: 145507.58050491323 \n",
      "\n",
      "Epoch 294 ============================\n",
      "weight[-566.46755432], bias[-355.41315961]\n",
      "Train loss: 145449.96135734662 \n",
      "\n",
      "Epoch 295 ============================\n",
      "weight[-566.37144891], bias[-355.99201865]\n",
      "Train loss: 145392.6532282452 \n",
      "\n",
      "Epoch 296 ============================\n",
      "weight[-566.27560324], bias[-356.56931329]\n",
      "Train loss: 145335.65443878356 \n",
      "\n",
      "Epoch 297 ============================\n",
      "weight[-566.18001659], bias[-357.14504775]\n",
      "Train loss: 145278.9633191985 \n",
      "\n",
      "Epoch 298 ============================\n",
      "weight[-566.08468828], bias[-357.71922625]\n",
      "Train loss: 145222.57820873972 \n",
      "\n",
      "Epoch 299 ============================\n",
      "weight[-565.98961759], bias[-358.29185299]\n",
      "Train loss: 145166.49745562137 \n",
      "\n",
      "Epoch 300 ============================\n",
      "weight[-565.89480384], bias[-358.86293217]\n",
      "Train loss: 145110.71941697388 \n",
      "\n",
      "Epoch 301 ============================\n",
      "weight[-565.80024634], bias[-359.43246797]\n",
      "Train loss: 145055.2424587954 \n",
      "\n",
      "Epoch 302 ============================\n",
      "weight[-565.70594438], bias[-360.00046456]\n",
      "Train loss: 145000.06495590394 \n",
      "\n",
      "Epoch 303 ============================\n",
      "weight[-565.61189727], bias[-360.5669261]\n",
      "Train loss: 144945.18529189067 \n",
      "\n",
      "Epoch 304 ============================\n",
      "weight[-565.51810434], bias[-361.13185674]\n",
      "Train loss: 144890.60185907126 \n",
      "\n",
      "Epoch 305 ============================\n",
      "weight[-565.42456489], bias[-361.69526061]\n",
      "Train loss: 144836.31305843938 \n",
      "\n",
      "Epoch 306 ============================\n",
      "weight[-565.33127823], bias[-362.25714185]\n",
      "Train loss: 144782.31729962028 \n",
      "\n",
      "Epoch 307 ============================\n",
      "weight[-565.23824369], bias[-362.81750457]\n",
      "Train loss: 144728.6130008235 \n",
      "\n",
      "Epoch 308 ============================\n",
      "weight[-565.14546058], bias[-363.37635287]\n",
      "Train loss: 144675.19858879692 \n",
      "\n",
      "Epoch 309 ============================\n",
      "weight[-565.05292822], bias[-363.93369084]\n",
      "Train loss: 144622.07249878044 \n",
      "\n",
      "Epoch 310 ============================\n",
      "weight[-564.96064594], bias[-364.48952257]\n",
      "Train loss: 144569.23317446036 \n",
      "\n",
      "Epoch 311 ============================\n",
      "weight[-564.86861306], bias[-365.04385213]\n",
      "Train loss: 144516.67906792366 \n",
      "\n",
      "Epoch 312 ============================\n",
      "weight[-564.7768289], bias[-365.59668357]\n",
      "Train loss: 144464.4086396128 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313 ============================\n",
      "weight[-564.68529279], bias[-366.14802095]\n",
      "Train loss: 144412.4203582805 \n",
      "\n",
      "Epoch 314 ============================\n",
      "weight[-564.59400407], bias[-366.6978683]\n",
      "Train loss: 144360.71270094448 \n",
      "\n",
      "Epoch 315 ============================\n",
      "weight[-564.50296206], bias[-367.24622965]\n",
      "Train loss: 144309.28415284416 \n",
      "\n",
      "Epoch 316 ============================\n",
      "weight[-564.4121661], bias[-367.79310902]\n",
      "Train loss: 144258.13320739448 \n",
      "\n",
      "Epoch 317 ============================\n",
      "weight[-564.32161552], bias[-368.33851041]\n",
      "Train loss: 144207.25836614316 \n",
      "\n",
      "Epoch 318 ============================\n",
      "weight[-564.23130966], bias[-368.88243782]\n",
      "Train loss: 144156.658138726 \n",
      "\n",
      "Epoch 319 ============================\n",
      "weight[-564.14124786], bias[-369.42489522]\n",
      "Train loss: 144106.33104282402 \n",
      "\n",
      "Epoch 320 ============================\n",
      "weight[-564.05142945], bias[-369.9658866]\n",
      "Train loss: 144056.27560411906 \n",
      "\n",
      "Epoch 321 ============================\n",
      "weight[-563.96185379], bias[-370.50541591]\n",
      "Train loss: 144006.4903562514 \n",
      "\n",
      "Epoch 322 ============================\n",
      "weight[-563.87252021], bias[-371.04348711]\n",
      "Train loss: 143956.97384077613 \n",
      "\n",
      "Epoch 323 ============================\n",
      "weight[-563.78342806], bias[-371.58010413]\n",
      "Train loss: 143907.72460712123 \n",
      "\n",
      "Epoch 324 ============================\n",
      "weight[-563.69457669], bias[-372.11527091]\n",
      "Train loss: 143858.74121254424 \n",
      "\n",
      "Epoch 325 ============================\n",
      "weight[-563.60596544], bias[-372.64899136]\n",
      "Train loss: 143810.02222209066 \n",
      "\n",
      "Epoch 326 ============================\n",
      "weight[-563.51759368], bias[-373.1812694]\n",
      "Train loss: 143761.56620855152 \n",
      "\n",
      "Epoch 327 ============================\n",
      "weight[-563.42946074], bias[-373.71210892]\n",
      "Train loss: 143713.37175242152 \n",
      "\n",
      "Epoch 328 ============================\n",
      "weight[-563.34156599], bias[-374.24151382]\n",
      "Train loss: 143665.43744185797 \n",
      "\n",
      "Epoch 329 ============================\n",
      "weight[-563.25390878], bias[-374.76948795]\n",
      "Train loss: 143617.76187263863 \n",
      "\n",
      "Epoch 330 ============================\n",
      "weight[-563.16648847], bias[-375.29603521]\n",
      "Train loss: 143570.34364812155 \n",
      "\n",
      "Epoch 331 ============================\n",
      "weight[-563.07930442], bias[-375.82115943]\n",
      "Train loss: 143523.181379203 \n",
      "\n",
      "Epoch 332 ============================\n",
      "weight[-562.99235599], bias[-376.34486447]\n",
      "Train loss: 143476.27368427799 \n",
      "\n",
      "Epoch 333 ============================\n",
      "weight[-562.90564254], bias[-376.86715416]\n",
      "Train loss: 143429.61918919883 \n",
      "\n",
      "Epoch 334 ============================\n",
      "weight[-562.81916344], bias[-377.38803233]\n",
      "Train loss: 143383.21652723523 \n",
      "\n",
      "Epoch 335 ============================\n",
      "weight[-562.73291806], bias[-377.90750279]\n",
      "Train loss: 143337.0643390347 \n",
      "\n",
      "Epoch 336 ============================\n",
      "weight[-562.64690577], bias[-378.42556934]\n",
      "Train loss: 143291.16127258167 \n",
      "\n",
      "Epoch 337 ============================\n",
      "weight[-562.56112592], bias[-378.94223579]\n",
      "Train loss: 143245.50598315898 \n",
      "\n",
      "Epoch 338 ============================\n",
      "weight[-562.4755779], bias[-379.45750591]\n",
      "Train loss: 143200.09713330804 \n",
      "\n",
      "Epoch 339 ============================\n",
      "weight[-562.39026109], bias[-379.97138347]\n",
      "Train loss: 143154.93339278924 \n",
      "\n",
      "Epoch 340 ============================\n",
      "weight[-562.30517484], bias[-380.48387225]\n",
      "Train loss: 143110.0134385438 \n",
      "\n",
      "Epoch 341 ============================\n",
      "weight[-562.22031855], bias[-380.99497599]\n",
      "Train loss: 143065.33595465458 \n",
      "\n",
      "Epoch 342 ============================\n",
      "weight[-562.13569159], bias[-381.50469844]\n",
      "Train loss: 143020.89963230715 \n",
      "\n",
      "Epoch 343 ============================\n",
      "weight[-562.05129333], bias[-382.01304333]\n",
      "Train loss: 142976.7031697521 \n",
      "\n",
      "Epoch 344 ============================\n",
      "weight[-561.96712317], bias[-382.52001439]\n",
      "Train loss: 142932.7452722667 \n",
      "\n",
      "Epoch 345 ============================\n",
      "weight[-561.88318049], bias[-383.02561532]\n",
      "Train loss: 142889.02465211667 \n",
      "\n",
      "Epoch 346 ============================\n",
      "weight[-561.79946467], bias[-383.52984982]\n",
      "Train loss: 142845.54002851908 \n",
      "\n",
      "Epoch 347 ============================\n",
      "weight[-561.71597509], bias[-384.0327216]\n",
      "Train loss: 142802.2901276044 \n",
      "\n",
      "Epoch 348 ============================\n",
      "weight[-561.63271115], bias[-384.53423434]\n",
      "Train loss: 142759.2736823787 \n",
      "\n",
      "Epoch 349 ============================\n",
      "weight[-561.54967224], bias[-385.0343917]\n",
      "Train loss: 142716.48943268755 \n",
      "\n",
      "Epoch 350 ============================\n",
      "weight[-561.46685774], bias[-385.53319736]\n",
      "Train loss: 142673.93612517856 \n",
      "\n",
      "Epoch 351 ============================\n",
      "weight[-561.38426706], bias[-386.03065496]\n",
      "Train loss: 142631.61251326473 \n",
      "\n",
      "Epoch 352 ============================\n",
      "weight[-561.30189958], bias[-386.52676814]\n",
      "Train loss: 142589.51735708778 \n",
      "\n",
      "Epoch 353 ============================\n",
      "weight[-561.21975471], bias[-387.02154055]\n",
      "Train loss: 142547.64942348216 \n",
      "\n",
      "Epoch 354 ============================\n",
      "weight[-561.13783184], bias[-387.5149758]\n",
      "Train loss: 142506.00748593872 \n",
      "\n",
      "Epoch 355 ============================\n",
      "weight[-561.05613037], bias[-388.00707751]\n",
      "Train loss: 142464.59032456853 \n",
      "\n",
      "Epoch 356 ============================\n",
      "weight[-560.9746497], bias[-388.49784927]\n",
      "Train loss: 142423.39672606759 \n",
      "\n",
      "Epoch 357 ============================\n",
      "weight[-560.89338924], bias[-388.9872947]\n",
      "Train loss: 142382.42548368155 \n",
      "\n",
      "Epoch 358 ============================\n",
      "weight[-560.8123484], bias[-389.47541737]\n",
      "Train loss: 142341.6753971692 \n",
      "\n",
      "Epoch 359 ============================\n",
      "weight[-560.73152657], bias[-389.96222085]\n",
      "Train loss: 142301.14527276836 \n",
      "\n",
      "Epoch 360 ============================\n",
      "weight[-560.65092317], bias[-390.44770871]\n",
      "Train loss: 142260.83392316054 \n",
      "\n",
      "Epoch 361 ============================\n",
      "weight[-560.5705376], bias[-390.93188451]\n",
      "Train loss: 142220.74016743622 \n",
      "\n",
      "Epoch 362 ============================\n",
      "weight[-560.49036929], bias[-391.41475179]\n",
      "Train loss: 142180.86283106028 \n",
      "\n",
      "Epoch 363 ============================\n",
      "weight[-560.41041763], bias[-391.89631409]\n",
      "Train loss: 142141.20074583762 \n",
      "\n",
      "Epoch 364 ============================\n",
      "weight[-560.33068205], bias[-392.37657493]\n",
      "Train loss: 142101.75274987842 \n",
      "\n",
      "Epoch 365 ============================\n",
      "weight[-560.25116195], bias[-392.85553784]\n",
      "Train loss: 142062.51768756504 \n",
      "\n",
      "Epoch 366 ============================\n",
      "weight[-560.17185677], bias[-393.33320631]\n",
      "Train loss: 142023.49440951736 \n",
      "\n",
      "Epoch 367 ============================\n",
      "weight[-560.09276592], bias[-393.80958386]\n",
      "Train loss: 141984.68177255965 \n",
      "\n",
      "Epoch 368 ============================\n",
      "weight[-560.01388881], bias[-394.28467396]\n",
      "Train loss: 141946.07863968675 \n",
      "\n",
      "Epoch 369 ============================\n",
      "weight[-559.93522487], bias[-394.7584801]\n",
      "Train loss: 141907.6838800308 \n",
      "\n",
      "Epoch 370 ============================\n",
      "weight[-559.85677353], bias[-395.23100575]\n",
      "Train loss: 141869.49636882835 \n",
      "\n",
      "Epoch 371 ============================\n",
      "weight[-559.77853421], bias[-395.70225436]\n",
      "Train loss: 141831.514987387 \n",
      "\n",
      "Epoch 372 ============================\n",
      "weight[-559.70050633], bias[-396.17222939]\n",
      "Train loss: 141793.73862305295 \n",
      "\n",
      "Epoch 373 ============================\n",
      "weight[-559.62268933], bias[-396.64093428]\n",
      "Train loss: 141756.16616917832 \n",
      "\n",
      "Epoch 374 ============================\n",
      "weight[-559.54508264], bias[-397.10837247]\n",
      "Train loss: 141718.79652508887 \n",
      "\n",
      "Epoch 375 ============================\n",
      "weight[-559.46768568], bias[-397.57454737]\n",
      "Train loss: 141681.6285960515 \n",
      "\n",
      "Epoch 376 ============================\n",
      "weight[-559.3904979], bias[-398.03946241]\n",
      "Train loss: 141644.6612932422 \n",
      "\n",
      "Epoch 377 ============================\n",
      "weight[-559.31351872], bias[-398.50312097]\n",
      "Train loss: 141607.89353371446 \n",
      "\n",
      "Epoch 378 ============================\n",
      "weight[-559.23674758], bias[-398.96552647]\n",
      "Train loss: 141571.32424036707 \n",
      "\n",
      "Epoch 379 ============================\n",
      "weight[-559.16018392], bias[-399.42668229]\n",
      "Train loss: 141534.9523419131 \n",
      "\n",
      "Epoch 380 ============================\n",
      "weight[-559.08382718], bias[-399.8865918]\n",
      "Train loss: 141498.7767728481 \n",
      "\n",
      "Epoch 381 ============================\n",
      "weight[-559.0076768], bias[-400.34525838]\n",
      "Train loss: 141462.79647341897 \n",
      "\n",
      "Epoch 382 ============================\n",
      "weight[-558.93173222], bias[-400.80268538]\n",
      "Train loss: 141427.01038959308 \n",
      "\n",
      "Epoch 383 ============================\n",
      "weight[-558.85599288], bias[-401.25887615]\n",
      "Train loss: 141391.4174730274 \n",
      "\n",
      "Epoch 384 ============================\n",
      "weight[-558.78045824], bias[-401.71383404]\n",
      "Train loss: 141356.0166810375 \n",
      "\n",
      "Epoch 385 ============================\n",
      "weight[-558.70512773], bias[-402.16756237]\n",
      "Train loss: 141320.80697656726 \n",
      "\n",
      "Epoch 386 ============================\n",
      "weight[-558.63000081], bias[-402.62006446]\n",
      "Train loss: 141285.78732815816 \n",
      "\n",
      "Epoch 387 ============================\n",
      "weight[-558.55507692], bias[-403.07134365]\n",
      "Train loss: 141250.95670991996 \n",
      "\n",
      "Epoch 388 ============================\n",
      "weight[-558.48035553], bias[-403.52140321]\n",
      "Train loss: 141216.31410149936 \n",
      "\n",
      "Epoch 389 ============================\n",
      "weight[-558.40583607], bias[-403.97024647]\n",
      "Train loss: 141181.85848805096 \n",
      "\n",
      "Epoch 390 ============================\n",
      "weight[-558.331518], bias[-404.41787669]\n",
      "Train loss: 141147.58886020744 \n",
      "\n",
      "Epoch 391 ============================\n",
      "weight[-558.25740079], bias[-404.86429716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 141113.50421404978 \n",
      "\n",
      "Epoch 392 ============================\n",
      "weight[-558.18348388], bias[-405.30951115]\n",
      "Train loss: 141079.603551078 \n",
      "\n",
      "Epoch 393 ============================\n",
      "weight[-558.10976674], bias[-405.75352192]\n",
      "Train loss: 141045.88587818178 \n",
      "\n",
      "Epoch 394 ============================\n",
      "weight[-558.03624882], bias[-406.19633272]\n",
      "Train loss: 141012.35020761137 \n",
      "\n",
      "Epoch 395 ============================\n",
      "weight[-557.96292959], bias[-406.63794679]\n",
      "Train loss: 140978.99555694906 \n",
      "\n",
      "Epoch 396 ============================\n",
      "weight[-557.88980851], bias[-407.07836737]\n",
      "Train loss: 140945.82094907967 \n",
      "\n",
      "Epoch 397 ============================\n",
      "weight[-557.81688504], bias[-407.51759769]\n",
      "Train loss: 140912.82541216246 \n",
      "\n",
      "Epoch 398 ============================\n",
      "weight[-557.74415866], bias[-407.95564096]\n",
      "Train loss: 140880.00797960276 \n",
      "\n",
      "Epoch 399 ============================\n",
      "weight[-557.67162882], bias[-408.39250038]\n",
      "Train loss: 140847.36769002295 \n",
      "\n",
      "Epoch 400 ============================\n",
      "weight[-557.599295], bias[-408.82817916]\n",
      "Train loss: 140814.90358723525 \n",
      "\n",
      "Epoch 401 ============================\n",
      "weight[-557.52715667], bias[-409.26268049]\n",
      "Train loss: 140782.61472021297 \n",
      "\n",
      "Epoch 402 ============================\n",
      "weight[-557.45521329], bias[-409.69600755]\n",
      "Train loss: 140750.50014306264 \n",
      "\n",
      "Epoch 403 ============================\n",
      "weight[-557.38346435], bias[-410.12816352]\n",
      "Train loss: 140718.5589149973 \n",
      "\n",
      "Epoch 404 ============================\n",
      "weight[-557.31190931], bias[-410.55915155]\n",
      "Train loss: 140686.7901003074 \n",
      "\n",
      "Epoch 405 ============================\n",
      "weight[-557.24054766], bias[-410.98897481]\n",
      "Train loss: 140655.19276833464 \n",
      "\n",
      "Epoch 406 ============================\n",
      "weight[-557.16937887], bias[-411.41763644]\n",
      "Train loss: 140623.7659934443 \n",
      "\n",
      "Epoch 407 ============================\n",
      "weight[-557.09840241], bias[-411.84513959]\n",
      "Train loss: 140592.50885499778 \n",
      "\n",
      "Epoch 408 ============================\n",
      "weight[-557.02761777], bias[-412.27148738]\n",
      "Train loss: 140561.42043732616 \n",
      "\n",
      "Epoch 409 ============================\n",
      "weight[-556.95702444], bias[-412.69668294]\n",
      "Train loss: 140530.49982970316 \n",
      "\n",
      "Epoch 410 ============================\n",
      "weight[-556.88662188], bias[-413.12072937]\n",
      "Train loss: 140499.74612631812 \n",
      "\n",
      "Epoch 411 ============================\n",
      "weight[-556.8164096], bias[-413.54362979]\n",
      "Train loss: 140469.15842625033 \n",
      "\n",
      "Epoch 412 ============================\n",
      "weight[-556.74638707], bias[-413.9653873]\n",
      "Train loss: 140438.73583344155 \n",
      "\n",
      "Epoch 413 ============================\n",
      "weight[-556.67655378], bias[-414.38600498]\n",
      "Train loss: 140408.47745667087 \n",
      "\n",
      "Epoch 414 ============================\n",
      "weight[-556.60690921], bias[-414.8054859]\n",
      "Train loss: 140378.38240952746 \n",
      "\n",
      "Epoch 415 ============================\n",
      "weight[-556.53745287], bias[-415.22383316]\n",
      "Train loss: 140348.44981038567 \n",
      "\n",
      "Epoch 416 ============================\n",
      "weight[-556.46818424], bias[-415.6410498]\n",
      "Train loss: 140318.6787823783 \n",
      "\n",
      "Epoch 417 ============================\n",
      "weight[-556.39910281], bias[-416.05713888]\n",
      "Train loss: 140289.0684533718 \n",
      "\n",
      "Epoch 418 ============================\n",
      "weight[-556.33020808], bias[-416.47210346]\n",
      "Train loss: 140259.61795593987 \n",
      "\n",
      "Epoch 419 ============================\n",
      "weight[-556.26149954], bias[-416.88594657]\n",
      "Train loss: 140230.32642733853 \n",
      "\n",
      "Epoch 420 ============================\n",
      "weight[-556.19297669], bias[-417.29867123]\n",
      "Train loss: 140201.19300948083 \n",
      "\n",
      "Epoch 421 ============================\n",
      "weight[-556.12463902], bias[-417.71028048]\n",
      "Train loss: 140172.21684891154 \n",
      "\n",
      "Epoch 422 ============================\n",
      "weight[-556.05648605], bias[-418.12077733]\n",
      "Train loss: 140143.3970967823 \n",
      "\n",
      "Epoch 423 ============================\n",
      "weight[-555.98851726], bias[-418.53016479]\n",
      "Train loss: 140114.7329088267 \n",
      "\n",
      "Epoch 424 ============================\n",
      "weight[-555.92073216], bias[-418.93844584]\n",
      "Train loss: 140086.22344533537 \n",
      "\n",
      "Epoch 425 ============================\n",
      "weight[-555.85313026], bias[-419.34562349]\n",
      "Train loss: 140057.86787113186 \n",
      "\n",
      "Epoch 426 ============================\n",
      "weight[-555.78571106], bias[-419.75170071]\n",
      "Train loss: 140029.66535554756 \n",
      "\n",
      "Epoch 427 ============================\n",
      "weight[-555.71847406], bias[-420.15668049]\n",
      "Train loss: 140001.61507239784 \n",
      "\n",
      "Epoch 428 ============================\n",
      "weight[-555.65141877], bias[-420.56056577]\n",
      "Train loss: 139973.7161999575 \n",
      "\n",
      "Epoch 429 ============================\n",
      "weight[-555.58454471], bias[-420.96335953]\n",
      "Train loss: 139945.96792093726 \n",
      "\n",
      "Epoch 430 ============================\n",
      "weight[-555.51785137], bias[-421.36506472]\n",
      "Train loss: 139918.36942245893 \n",
      "\n",
      "Epoch 431 ============================\n",
      "weight[-555.45133828], bias[-421.76568426]\n",
      "Train loss: 139890.91989603257 \n",
      "\n",
      "Epoch 432 ============================\n",
      "weight[-555.38500495], bias[-422.16522111]\n",
      "Train loss: 139863.6185375318 \n",
      "\n",
      "Epoch 433 ============================\n",
      "weight[-555.31885088], bias[-422.56367818]\n",
      "Train loss: 139836.4645471714 \n",
      "\n",
      "Epoch 434 ============================\n",
      "weight[-555.2528756], bias[-422.96105839]\n",
      "Train loss: 139809.45712948273 \n",
      "\n",
      "Epoch 435 ============================\n",
      "weight[-555.18707863], bias[-423.35736465]\n",
      "Train loss: 139782.5954932912 \n",
      "\n",
      "Epoch 436 ============================\n",
      "weight[-555.12145947], bias[-423.75259988]\n",
      "Train loss: 139755.878851693 \n",
      "\n",
      "Epoch 437 ============================\n",
      "weight[-555.05601766], bias[-424.14676695]\n",
      "Train loss: 139729.30642203143 \n",
      "\n",
      "Epoch 438 ============================\n",
      "weight[-554.99075271], bias[-424.53986875]\n",
      "Train loss: 139702.87742587496 \n",
      "\n",
      "Epoch 439 ============================\n",
      "weight[-554.92566414], bias[-424.93190818]\n",
      "Train loss: 139676.59108899342 \n",
      "\n",
      "Epoch 440 ============================\n",
      "weight[-554.86075147], bias[-425.32288809]\n",
      "Train loss: 139650.44664133617 \n",
      "\n",
      "Epoch 441 ============================\n",
      "weight[-554.79601424], bias[-425.71281135]\n",
      "Train loss: 139624.44331700917 \n",
      "\n",
      "Epoch 442 ============================\n",
      "weight[-554.73145196], bias[-426.10168081]\n",
      "Train loss: 139598.58035425225 \n",
      "\n",
      "Epoch 443 ============================\n",
      "weight[-554.66706417], bias[-426.48949934]\n",
      "Train loss: 139572.8569954172 \n",
      "\n",
      "Epoch 444 ============================\n",
      "weight[-554.60285039], bias[-426.87626975]\n",
      "Train loss: 139547.27248694576 \n",
      "\n",
      "Epoch 445 ============================\n",
      "weight[-554.53881015], bias[-427.26199489]\n",
      "Train loss: 139521.82607934694 \n",
      "\n",
      "Epoch 446 ============================\n",
      "weight[-554.47494299], bias[-427.64667758]\n",
      "Train loss: 139496.5170271755 \n",
      "\n",
      "Epoch 447 ============================\n",
      "weight[-554.41124843], bias[-428.03032065]\n",
      "Train loss: 139471.34458900976 \n",
      "\n",
      "Epoch 448 ============================\n",
      "weight[-554.347726], bias[-428.41292689]\n",
      "Train loss: 139446.3080274305 \n",
      "\n",
      "Epoch 449 ============================\n",
      "weight[-554.28437526], bias[-428.79449911]\n",
      "Train loss: 139421.40660899875 \n",
      "\n",
      "Epoch 450 ============================\n",
      "weight[-554.22119572], bias[-429.1750401]\n",
      "Train loss: 139396.63960423434 \n",
      "\n",
      "Epoch 451 ============================\n",
      "weight[-554.15818693], bias[-429.55455266]\n",
      "Train loss: 139372.00628759517 \n",
      "\n",
      "Epoch 452 ============================\n",
      "weight[-554.09534842], bias[-429.93303956]\n",
      "Train loss: 139347.50593745502 \n",
      "\n",
      "Epoch 453 ============================\n",
      "weight[-554.03267974], bias[-430.31050358]\n",
      "Train loss: 139323.13783608328 \n",
      "\n",
      "Epoch 454 ============================\n",
      "weight[-553.97018043], bias[-430.68694747]\n",
      "Train loss: 139298.90126962328 \n",
      "\n",
      "Epoch 455 ============================\n",
      "weight[-553.90785003], bias[-431.06237399]\n",
      "Train loss: 139274.79552807158 \n",
      "\n",
      "Epoch 456 ============================\n",
      "weight[-553.84568807], bias[-431.4367859]\n",
      "Train loss: 139250.81990525752 \n",
      "\n",
      "Epoch 457 ============================\n",
      "weight[-553.78369412], bias[-431.81018594]\n",
      "Train loss: 139226.97369882182 \n",
      "\n",
      "Epoch 458 ============================\n",
      "weight[-553.72186771], bias[-432.18257684]\n",
      "Train loss: 139203.25621019685 \n",
      "\n",
      "Epoch 459 ============================\n",
      "weight[-553.66020838], bias[-432.55396133]\n",
      "Train loss: 139179.6667445854 \n",
      "\n",
      "Epoch 460 ============================\n",
      "weight[-553.5987157], bias[-432.92434212]\n",
      "Train loss: 139156.20461094088 \n",
      "\n",
      "Epoch 461 ============================\n",
      "weight[-553.5373892], bias[-433.29372194]\n",
      "Train loss: 139132.86912194663 \n",
      "\n",
      "Epoch 462 ============================\n",
      "weight[-553.47622845], bias[-433.66210348]\n",
      "Train loss: 139109.65959399633 \n",
      "\n",
      "Epoch 463 ============================\n",
      "weight[-553.41523298], bias[-434.02948944]\n",
      "Train loss: 139086.57534717314 \n",
      "\n",
      "Epoch 464 ============================\n",
      "weight[-553.35440236], bias[-434.39588252]\n",
      "Train loss: 139063.61570523097 \n",
      "\n",
      "Epoch 465 ============================\n",
      "weight[-553.29373613], bias[-434.7612854]\n",
      "Train loss: 139040.7799955733 \n",
      "\n",
      "Epoch 466 ============================\n",
      "weight[-553.23323387], bias[-435.12570075]\n",
      "Train loss: 139018.06754923484 \n",
      "\n",
      "Epoch 467 ============================\n",
      "weight[-553.17289511], bias[-435.48913125]\n",
      "Train loss: 138995.47770086068 \n",
      "\n",
      "Epoch 468 ============================\n",
      "weight[-553.11271942], bias[-435.85157954]\n",
      "Train loss: 138973.0097886877 \n",
      "\n",
      "Epoch 469 ============================\n",
      "weight[-553.05270636], bias[-436.2130483]\n",
      "Train loss: 138950.66315452478 \n",
      "\n",
      "Epoch 470 ============================\n",
      "weight[-552.9928555], bias[-436.57354017]\n",
      "Train loss: 138928.4371437337 \n",
      "\n",
      "Epoch 471 ============================\n",
      "weight[-552.93316638], bias[-436.93305778]\n",
      "Train loss: 138906.33110520974 \n",
      "\n",
      "Epoch 472 ============================\n",
      "weight[-552.87363857], bias[-437.29160377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 138884.34439136277 \n",
      "\n",
      "Epoch 473 ============================\n",
      "weight[-552.81427165], bias[-437.64918076]\n",
      "Train loss: 138862.47635809836 \n",
      "\n",
      "Epoch 474 ============================\n",
      "weight[-552.75506516], bias[-438.00579138]\n",
      "Train loss: 138840.7263647986 \n",
      "\n",
      "Epoch 475 ============================\n",
      "weight[-552.69601869], bias[-438.36143823]\n",
      "Train loss: 138819.09377430353 \n",
      "\n",
      "Epoch 476 ============================\n",
      "weight[-552.6371318], bias[-438.71612393]\n",
      "Train loss: 138797.57795289278 \n",
      "\n",
      "Epoch 477 ============================\n",
      "weight[-552.57840404], bias[-439.06985106]\n",
      "Train loss: 138776.17827026636 \n",
      "\n",
      "Epoch 478 ============================\n",
      "weight[-552.51983501], bias[-439.42262222]\n",
      "Train loss: 138754.89409952657 \n",
      "\n",
      "Epoch 479 ============================\n",
      "weight[-552.46142426], bias[-439.77443999]\n",
      "Train loss: 138733.7248171598 \n",
      "\n",
      "Epoch 480 ============================\n",
      "weight[-552.40317137], bias[-440.12530696]\n",
      "Train loss: 138712.66980301784 \n",
      "\n",
      "Epoch 481 ============================\n",
      "weight[-552.34507592], bias[-440.47522567]\n",
      "Train loss: 138691.72844030012 \n",
      "\n",
      "Epoch 482 ============================\n",
      "weight[-552.28713747], bias[-440.82419872]\n",
      "Train loss: 138670.90011553545 \n",
      "\n",
      "Epoch 483 ============================\n",
      "weight[-552.2293556], bias[-441.17222863]\n",
      "Train loss: 138650.1842185638 \n",
      "\n",
      "Epoch 484 ============================\n",
      "weight[-552.17172989], bias[-441.51931798]\n",
      "Train loss: 138629.58014251894 \n",
      "\n",
      "Epoch 485 ============================\n",
      "weight[-552.11425992], bias[-441.86546929]\n",
      "Train loss: 138609.0872838103 \n",
      "\n",
      "Epoch 486 ============================\n",
      "weight[-552.05694527], bias[-442.2106851]\n",
      "Train loss: 138588.7050421054 \n",
      "\n",
      "Epoch 487 ============================\n",
      "weight[-551.99978551], bias[-442.55496794]\n",
      "Train loss: 138568.43282031224 \n",
      "\n",
      "Epoch 488 ============================\n",
      "weight[-551.94278023], bias[-442.89832034]\n",
      "Train loss: 138548.27002456202 \n",
      "\n",
      "Epoch 489 ============================\n",
      "weight[-551.88592901], bias[-443.2407448]\n",
      "Train loss: 138528.21606419107 \n",
      "\n",
      "Epoch 490 ============================\n",
      "weight[-551.82923144], bias[-443.58224384]\n",
      "Train loss: 138508.27035172464 \n",
      "\n",
      "Epoch 491 ============================\n",
      "weight[-551.7726871], bias[-443.92281995]\n",
      "Train loss: 138488.4323028586 \n",
      "\n",
      "Epoch 492 ============================\n",
      "weight[-551.71629557], bias[-444.26247564]\n",
      "Train loss: 138468.70133644287 \n",
      "\n",
      "Epoch 493 ============================\n",
      "weight[-551.66005644], bias[-444.60121338]\n",
      "Train loss: 138449.07687446455 \n",
      "\n",
      "Epoch 494 ============================\n",
      "weight[-551.6039693], bias[-444.93903565]\n",
      "Train loss: 138429.55834203045 \n",
      "\n",
      "Epoch 495 ============================\n",
      "weight[-551.54803374], bias[-445.27594495]\n",
      "Train loss: 138410.14516735086 \n",
      "\n",
      "Epoch 496 ============================\n",
      "weight[-551.49224935], bias[-445.61194372]\n",
      "Train loss: 138390.83678172217 \n",
      "\n",
      "Epoch 497 ============================\n",
      "weight[-551.43661572], bias[-445.94703443]\n",
      "Train loss: 138371.63261951093 \n",
      "\n",
      "Epoch 498 ============================\n",
      "weight[-551.38113245], bias[-446.28121954]\n",
      "Train loss: 138352.53211813653 \n",
      "\n",
      "Epoch 499 ============================\n",
      "weight[-551.32579912], bias[-446.61450148]\n",
      "Train loss: 138333.53471805513 \n",
      "\n",
      "Epoch 500 ============================\n",
      "weight[-551.27061534], bias[-446.94688272]\n",
      "Train loss: 138314.63986274356 \n",
      "\n",
      "Epoch 501 ============================\n",
      "weight[-551.21558069], bias[-447.27836566]\n",
      "Train loss: 138295.84699868225 \n",
      "\n",
      "Epoch 502 ============================\n",
      "weight[-551.16069478], bias[-447.60895276]\n",
      "Train loss: 138277.15557533948 \n",
      "\n",
      "Epoch 503 ============================\n",
      "weight[-551.1059572], bias[-447.93864642]\n",
      "Train loss: 138258.56504515547 \n",
      "\n",
      "Epoch 504 ============================\n",
      "weight[-551.05136755], bias[-448.26744906]\n",
      "Train loss: 138240.07486352586 \n",
      "\n",
      "Epoch 505 ============================\n",
      "weight[-550.99692544], bias[-448.59536308]\n",
      "Train loss: 138221.68448878606 \n",
      "\n",
      "Epoch 506 ============================\n",
      "weight[-550.94263046], bias[-448.9223909]\n",
      "Train loss: 138203.3933821953 \n",
      "\n",
      "Epoch 507 ============================\n",
      "weight[-550.88848221], bias[-449.24853491]\n",
      "Train loss: 138185.2010079207 \n",
      "\n",
      "Epoch 508 ============================\n",
      "weight[-550.8344803], bias[-449.57379748]\n",
      "Train loss: 138167.1068330219 \n",
      "\n",
      "Epoch 509 ============================\n",
      "weight[-550.78062434], bias[-449.89818102]\n",
      "Train loss: 138149.1103274352 \n",
      "\n",
      "Epoch 510 ============================\n",
      "weight[-550.72691393], bias[-450.22168788]\n",
      "Train loss: 138131.21096395818 \n",
      "\n",
      "Epoch 511 ============================\n",
      "weight[-550.67334867], bias[-450.54432045]\n",
      "Train loss: 138113.40821823388 \n",
      "\n",
      "Epoch 512 ============================\n",
      "weight[-550.61992818], bias[-450.86608108]\n",
      "Train loss: 138095.70156873597 \n",
      "\n",
      "Epoch 513 ============================\n",
      "weight[-550.56665206], bias[-451.18697213]\n",
      "Train loss: 138078.09049675334 \n",
      "\n",
      "Epoch 514 ============================\n",
      "weight[-550.51351992], bias[-451.50699594]\n",
      "Train loss: 138060.57448637448 \n",
      "\n",
      "Epoch 515 ============================\n",
      "weight[-550.46053137], bias[-451.82615488]\n",
      "Train loss: 138043.15302447285 \n",
      "\n",
      "Epoch 516 ============================\n",
      "weight[-550.40768603], bias[-452.14445126]\n",
      "Train loss: 138025.82560069166 \n",
      "\n",
      "Epoch 517 ============================\n",
      "weight[-550.35498351], bias[-452.46188743]\n",
      "Train loss: 138008.59170742912 \n",
      "\n",
      "Epoch 518 ============================\n",
      "weight[-550.30242342], bias[-452.7784657]\n",
      "Train loss: 137991.45083982314 \n",
      "\n",
      "Epoch 519 ============================\n",
      "weight[-550.25000538], bias[-453.0941884]\n",
      "Train loss: 137974.40249573687 \n",
      "\n",
      "Epoch 520 ============================\n",
      "weight[-550.197729], bias[-453.40905784]\n",
      "Train loss: 137957.44617574374 \n",
      "\n",
      "Epoch 521 ============================\n",
      "weight[-550.1455939], bias[-453.72307632]\n",
      "Train loss: 137940.5813831135 \n",
      "\n",
      "Epoch 522 ============================\n",
      "weight[-550.0935997], bias[-454.03624615]\n",
      "Train loss: 137923.80762379663 \n",
      "\n",
      "Epoch 523 ============================\n",
      "weight[-550.04174602], bias[-454.34856961]\n",
      "Train loss: 137907.12440641082 \n",
      "\n",
      "Epoch 524 ============================\n",
      "weight[-549.99003247], bias[-454.660049]\n",
      "Train loss: 137890.53124222564 \n",
      "\n",
      "Epoch 525 ============================\n",
      "weight[-549.93845869], bias[-454.97068659]\n",
      "Train loss: 137874.02764514924 \n",
      "\n",
      "Epoch 526 ============================\n",
      "weight[-549.88702429], bias[-455.28048467]\n",
      "Train loss: 137857.61313171344 \n",
      "\n",
      "Epoch 527 ============================\n",
      "weight[-549.83572889], bias[-455.58944549]\n",
      "Train loss: 137841.2872210597 \n",
      "\n",
      "Epoch 528 ============================\n",
      "weight[-549.78457212], bias[-455.89757133]\n",
      "Train loss: 137825.04943492528 \n",
      "\n",
      "Epoch 529 ============================\n",
      "weight[-549.7335536], bias[-456.20486443]\n",
      "Train loss: 137808.89929762858 \n",
      "\n",
      "Epoch 530 ============================\n",
      "weight[-549.68267297], bias[-456.51132706]\n",
      "Train loss: 137792.83633605606 \n",
      "\n",
      "Epoch 531 ============================\n",
      "weight[-549.63192984], bias[-456.81696145]\n",
      "Train loss: 137776.86007964777 \n",
      "\n",
      "Epoch 532 ============================\n",
      "weight[-549.58132386], bias[-457.12176984]\n",
      "Train loss: 137760.9700603839 \n",
      "\n",
      "Epoch 533 ============================\n",
      "weight[-549.53085463], bias[-457.42575446]\n",
      "Train loss: 137745.16581277087 \n",
      "\n",
      "Epoch 534 ============================\n",
      "weight[-549.48052181], bias[-457.72891755]\n",
      "Train loss: 137729.44687382763 \n",
      "\n",
      "Epoch 535 ============================\n",
      "weight[-549.43032501], bias[-458.03126132]\n",
      "Train loss: 137713.81278307232 \n",
      "\n",
      "Epoch 536 ============================\n",
      "weight[-549.38026387], bias[-458.33278798]\n",
      "Train loss: 137698.26308250878 \n",
      "\n",
      "Epoch 537 ============================\n",
      "weight[-549.33033803], bias[-458.63349975]\n",
      "Train loss: 137682.79731661294 \n",
      "\n",
      "Epoch 538 ============================\n",
      "weight[-549.28054711], bias[-458.93339882]\n",
      "Train loss: 137667.4150323196 \n",
      "\n",
      "Epoch 539 ============================\n",
      "weight[-549.23089076], bias[-459.2324874]\n",
      "Train loss: 137652.1157790091 \n",
      "\n",
      "Epoch 540 ============================\n",
      "weight[-549.18136861], bias[-459.53076767]\n",
      "Train loss: 137636.89910849408 \n",
      "\n",
      "Epoch 541 ============================\n",
      "weight[-549.13198029], bias[-459.82824181]\n",
      "Train loss: 137621.7645750066 \n",
      "\n",
      "Epoch 542 ============================\n",
      "weight[-549.08272545], bias[-460.12491202]\n",
      "Train loss: 137606.7117351848 \n",
      "\n",
      "Epoch 543 ============================\n",
      "weight[-549.03360372], bias[-460.42078045]\n",
      "Train loss: 137591.74014806 \n",
      "\n",
      "Epoch 544 ============================\n",
      "weight[-548.98461475], bias[-460.71584927]\n",
      "Train loss: 137576.8493750438 \n",
      "\n",
      "Epoch 545 ============================\n",
      "weight[-548.93575818], bias[-461.01012066]\n",
      "Train loss: 137562.0389799152 \n",
      "\n",
      "Epoch 546 ============================\n",
      "weight[-548.88703364], bias[-461.30359675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 137547.308528808 \n",
      "\n",
      "Epoch 547 ============================\n",
      "weight[-548.83844078], bias[-461.5962797]\n",
      "Train loss: 137532.6575901977 \n",
      "\n",
      "Epoch 548 ============================\n",
      "weight[-548.78997925], bias[-461.88817166]\n",
      "Train loss: 137518.08573488926 \n",
      "\n",
      "Epoch 549 ============================\n",
      "weight[-548.74164869], bias[-462.17927477]\n",
      "Train loss: 137503.59253600435 \n",
      "\n",
      "Epoch 550 ============================\n",
      "weight[-548.69344875], bias[-462.46959114]\n",
      "Train loss: 137489.1775689688 \n",
      "\n",
      "Epoch 551 ============================\n",
      "weight[-548.64537907], bias[-462.75912292]\n",
      "Train loss: 137474.8404115002 \n",
      "\n",
      "Epoch 552 ============================\n",
      "weight[-548.5974393], bias[-463.04787221]\n",
      "Train loss: 137460.58064359563 \n",
      "\n",
      "Epoch 553 ============================\n",
      "weight[-548.5496291], bias[-463.33584115]\n",
      "Train loss: 137446.39784751934 \n",
      "\n",
      "Epoch 554 ============================\n",
      "weight[-548.5019481], bias[-463.62303182]\n",
      "Train loss: 137432.2916077903 \n",
      "\n",
      "Epoch 555 ============================\n",
      "weight[-548.45439596], bias[-463.90944635]\n",
      "Train loss: 137418.26151117007 \n",
      "\n",
      "Epoch 556 ============================\n",
      "weight[-548.40697234], bias[-464.19508682]\n",
      "Train loss: 137404.30714665123 \n",
      "\n",
      "Epoch 557 ============================\n",
      "weight[-548.35967688], bias[-464.47995533]\n",
      "Train loss: 137390.42810544439 \n",
      "\n",
      "Epoch 558 ============================\n",
      "weight[-548.31250924], bias[-464.76405396]\n",
      "Train loss: 137376.62398096718 \n",
      "\n",
      "Epoch 559 ============================\n",
      "weight[-548.26546908], bias[-465.04738479]\n",
      "Train loss: 137362.8943688315 \n",
      "\n",
      "Epoch 560 ============================\n",
      "weight[-548.21855604], bias[-465.32994991]\n",
      "Train loss: 137349.2388668325 \n",
      "\n",
      "Epoch 561 ============================\n",
      "weight[-548.17176979], bias[-465.61175137]\n",
      "Train loss: 137335.65707493588 \n",
      "\n",
      "Epoch 562 ============================\n",
      "weight[-548.12510999], bias[-465.89279124]\n",
      "Train loss: 137322.148595267 \n",
      "\n",
      "Epoch 563 ============================\n",
      "weight[-548.07857628], bias[-466.17307159]\n",
      "Train loss: 137308.71303209898 \n",
      "\n",
      "Epoch 564 ============================\n",
      "weight[-548.03216834], bias[-466.45259446]\n",
      "Train loss: 137295.34999184054 \n",
      "\n",
      "Epoch 565 ============================\n",
      "weight[-547.98588581], bias[-466.7313619]\n",
      "Train loss: 137282.05908302532 \n",
      "\n",
      "Epoch 566 ============================\n",
      "weight[-547.93972837], bias[-467.00937595]\n",
      "Train loss: 137268.83991630006 \n",
      "\n",
      "Epoch 567 ============================\n",
      "weight[-547.89369567], bias[-467.28663865]\n",
      "Train loss: 137255.69210441282 \n",
      "\n",
      "Epoch 568 ============================\n",
      "weight[-547.84778738], bias[-467.56315203]\n",
      "Train loss: 137242.6152622022 \n",
      "\n",
      "Epoch 569 ============================\n",
      "weight[-547.80200316], bias[-467.83891811]\n",
      "Train loss: 137229.60900658593 \n",
      "\n",
      "Epoch 570 ============================\n",
      "weight[-547.75634267], bias[-468.11393892]\n",
      "Train loss: 137216.67295654942 \n",
      "\n",
      "Epoch 571 ============================\n",
      "weight[-547.71080558], bias[-468.38821646]\n",
      "Train loss: 137203.80673313476 \n",
      "\n",
      "Epoch 572 ============================\n",
      "weight[-547.66539156], bias[-468.66175275]\n",
      "Train loss: 137191.00995942953 \n",
      "\n",
      "Epoch 573 ============================\n",
      "weight[-547.62010028], bias[-468.93454979]\n",
      "Train loss: 137178.28226055604 \n",
      "\n",
      "Epoch 574 ============================\n",
      "weight[-547.57493139], bias[-469.20660958]\n",
      "Train loss: 137165.62326365986 \n",
      "\n",
      "Epoch 575 ============================\n",
      "weight[-547.52988458], bias[-469.4779341]\n",
      "Train loss: 137153.0325978992 \n",
      "\n",
      "Epoch 576 ============================\n",
      "weight[-547.48495952], bias[-469.74852536]\n",
      "Train loss: 137140.50989443416 \n",
      "\n",
      "Epoch 577 ============================\n",
      "weight[-547.44015586], bias[-470.01838532]\n",
      "Train loss: 137128.05478641577 \n",
      "\n",
      "Epoch 578 ============================\n",
      "weight[-547.39547329], bias[-470.28751597]\n",
      "Train loss: 137115.6669089751 \n",
      "\n",
      "Epoch 579 ============================\n",
      "weight[-547.35091148], bias[-470.55591927]\n",
      "Train loss: 137103.3458992128 \n",
      "\n",
      "Epoch 580 ============================\n",
      "weight[-547.3064701], bias[-470.8235972]\n",
      "Train loss: 137091.09139618845 \n",
      "\n",
      "Epoch 581 ============================\n",
      "weight[-547.26214882], bias[-471.09055171]\n",
      "Train loss: 137078.9030409098 \n",
      "\n",
      "Epoch 582 ============================\n",
      "weight[-547.21794733], bias[-471.35678475]\n",
      "Train loss: 137066.7804763225 \n",
      "\n",
      "Epoch 583 ============================\n",
      "weight[-547.17386529], bias[-471.62229828]\n",
      "Train loss: 137054.7233472994 \n",
      "\n",
      "Epoch 584 ============================\n",
      "weight[-547.12990239], bias[-471.88709425]\n",
      "Train loss: 137042.7313006304 \n",
      "\n",
      "Epoch 585 ============================\n",
      "weight[-547.0860583], bias[-472.15117458]\n",
      "Train loss: 137030.80398501188 \n",
      "\n",
      "Epoch 586 ============================\n",
      "weight[-547.0423327], bias[-472.41454122]\n",
      "Train loss: 137018.9410510365 \n",
      "\n",
      "Epoch 587 ============================\n",
      "weight[-546.99872527], bias[-472.6771961]\n",
      "Train loss: 137007.14215118307 \n",
      "\n",
      "Epoch 588 ============================\n",
      "weight[-546.9552357], bias[-472.93914113]\n",
      "Train loss: 136995.40693980598 \n",
      "\n",
      "Epoch 589 ============================\n",
      "weight[-546.91186366], bias[-473.20037824]\n",
      "Train loss: 136983.73507312572 \n",
      "\n",
      "Epoch 590 ============================\n",
      "weight[-546.86860883], bias[-473.46090933]\n",
      "Train loss: 136972.1262092183 \n",
      "\n",
      "Epoch 591 ============================\n",
      "weight[-546.82547091], bias[-473.72073633]\n",
      "Train loss: 136960.58000800517 \n",
      "\n",
      "Epoch 592 ============================\n",
      "weight[-546.78244956], bias[-473.97986112]\n",
      "Train loss: 136949.09613124383 \n",
      "\n",
      "Epoch 593 ============================\n",
      "weight[-546.73954449], bias[-474.23828561]\n",
      "Train loss: 136937.67424251724 \n",
      "\n",
      "Epoch 594 ============================\n",
      "weight[-546.69675537], bias[-474.49601169]\n",
      "Train loss: 136926.31400722443 \n",
      "\n",
      "Epoch 595 ============================\n",
      "weight[-546.65408189], bias[-474.75304125]\n",
      "Train loss: 136915.01509257057 \n",
      "\n",
      "Epoch 596 ============================\n",
      "weight[-546.61152374], bias[-475.00937617]\n",
      "Train loss: 136903.7771675573 \n",
      "\n",
      "Epoch 597 ============================\n",
      "weight[-546.5690806], bias[-475.26501833]\n",
      "Train loss: 136892.59990297255 \n",
      "\n",
      "Epoch 598 ============================\n",
      "weight[-546.52675217], bias[-475.51996959]\n",
      "Train loss: 136881.48297138172 \n",
      "\n",
      "Epoch 599 ============================\n",
      "weight[-546.48453813], bias[-475.77423184]\n",
      "Train loss: 136870.42604711736 \n",
      "\n",
      "Epoch 600 ============================\n",
      "weight[-546.44243819], bias[-476.02780692]\n",
      "Train loss: 136859.42880627015 \n",
      "\n",
      "Epoch 601 ============================\n",
      "weight[-546.40045202], bias[-476.2806967]\n",
      "Train loss: 136848.49092667885 \n",
      "\n",
      "Epoch 602 ============================\n",
      "weight[-546.35857932], bias[-476.53290303]\n",
      "Train loss: 136837.6120879215 \n",
      "\n",
      "Epoch 603 ============================\n",
      "weight[-546.31681978], bias[-476.78442775]\n",
      "Train loss: 136826.7919713057 \n",
      "\n",
      "Epoch 604 ============================\n",
      "weight[-546.2751731], bias[-477.03527271]\n",
      "Train loss: 136816.03025985896 \n",
      "\n",
      "Epoch 605 ============================\n",
      "weight[-546.23363897], bias[-477.28543974]\n",
      "Train loss: 136805.3266383202 \n",
      "\n",
      "Epoch 606 ============================\n",
      "weight[-546.1922171], bias[-477.53493068]\n",
      "Train loss: 136794.68079312978 \n",
      "\n",
      "Epoch 607 ============================\n",
      "weight[-546.15090717], bias[-477.78374736]\n",
      "Train loss: 136784.09241242078 \n",
      "\n",
      "Epoch 608 ============================\n",
      "weight[-546.10970888], bias[-478.03189159]\n",
      "Train loss: 136773.56118600947 \n",
      "\n",
      "Epoch 609 ============================\n",
      "weight[-546.06862193], bias[-478.27936519]\n",
      "Train loss: 136763.0868053868 \n",
      "\n",
      "Epoch 610 ============================\n",
      "weight[-546.02764602], bias[-478.52616998]\n",
      "Train loss: 136752.66896370854 \n",
      "\n",
      "Epoch 611 ============================\n",
      "weight[-545.98678086], bias[-478.77230776]\n",
      "Train loss: 136742.30735578694 \n",
      "\n",
      "Epoch 612 ============================\n",
      "weight[-545.94602613], bias[-479.01778034]\n",
      "Train loss: 136732.00167808187 \n",
      "\n",
      "Epoch 613 ============================\n",
      "weight[-545.90538155], bias[-479.26258951]\n",
      "Train loss: 136721.7516286912 \n",
      "\n",
      "Epoch 614 ============================\n",
      "weight[-545.86484681], bias[-479.50673707]\n",
      "Train loss: 136711.5569073429 \n",
      "\n",
      "Epoch 615 ============================\n",
      "weight[-545.82442162], bias[-479.75022481]\n",
      "Train loss: 136701.41721538533 \n",
      "\n",
      "Epoch 616 ============================\n",
      "weight[-545.78410568], bias[-479.9930545]\n",
      "Train loss: 136691.3322557791 \n",
      "\n",
      "Epoch 617 ============================\n",
      "weight[-545.7438987], bias[-480.23522793]\n",
      "Train loss: 136681.30173308807 \n",
      "\n",
      "Epoch 618 ============================\n",
      "weight[-545.70380038], bias[-480.47674686]\n",
      "Train loss: 136671.3253534712 \n",
      "\n",
      "Epoch 619 ============================\n",
      "weight[-545.66381042], bias[-480.71761308]\n",
      "Train loss: 136661.40282467325 \n",
      "\n",
      "Epoch 620 ============================\n",
      "weight[-545.62392855], bias[-480.95782834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 136651.53385601632 \n",
      "\n",
      "Epoch 621 ============================\n",
      "weight[-545.58415445], bias[-481.1973944]\n",
      "Train loss: 136641.718158392 \n",
      "\n",
      "Epoch 622 ============================\n",
      "weight[-545.54448785], bias[-481.43631302]\n",
      "Train loss: 136631.9554442521 \n",
      "\n",
      "Epoch 623 ============================\n",
      "weight[-545.50492845], bias[-481.67458594]\n",
      "Train loss: 136622.2454276008 \n",
      "\n",
      "Epoch 624 ============================\n",
      "weight[-545.46547597], bias[-481.91221492]\n",
      "Train loss: 136612.58782398578 \n",
      "\n",
      "Epoch 625 ============================\n",
      "weight[-545.4261301], bias[-482.14920169]\n",
      "Train loss: 136602.98235049038 \n",
      "\n",
      "Epoch 626 ============================\n",
      "weight[-545.38689057], bias[-482.38554798]\n",
      "Train loss: 136593.428725725 \n",
      "\n",
      "Epoch 627 ============================\n",
      "weight[-545.34775709], bias[-482.62125553]\n",
      "Train loss: 136583.92666981876 \n",
      "\n",
      "Epoch 628 ============================\n",
      "weight[-545.30872937], bias[-482.85632607]\n",
      "Train loss: 136574.47590441196 \n",
      "\n",
      "Epoch 629 ============================\n",
      "weight[-545.26980712], bias[-483.09076131]\n",
      "Train loss: 136565.07615264668 \n",
      "\n",
      "Epoch 630 ============================\n",
      "weight[-545.23099007], bias[-483.32456298]\n",
      "Train loss: 136555.72713916027 \n",
      "\n",
      "Epoch 631 ============================\n",
      "weight[-545.19227792], bias[-483.55773278]\n",
      "Train loss: 136546.4285900758 \n",
      "\n",
      "Epoch 632 ============================\n",
      "weight[-545.15367039], bias[-483.79027242]\n",
      "Train loss: 136537.18023299496 \n",
      "\n",
      "Epoch 633 ============================\n",
      "weight[-545.1151672], bias[-484.02218361]\n",
      "Train loss: 136527.9817969897 \n",
      "\n",
      "Epoch 634 ============================\n",
      "weight[-545.07676807], bias[-484.25346804]\n",
      "Train loss: 136518.83301259464 \n",
      "\n",
      "Epoch 635 ============================\n",
      "weight[-545.03847272], bias[-484.48412741]\n",
      "Train loss: 136509.7336117987 \n",
      "\n",
      "Epoch 636 ============================\n",
      "weight[-545.00028086], bias[-484.71416341]\n",
      "Train loss: 136500.68332803724 \n",
      "\n",
      "Epoch 637 ============================\n",
      "weight[-544.96219222], bias[-484.94357772]\n",
      "Train loss: 136491.68189618507 \n",
      "\n",
      "Epoch 638 ============================\n",
      "weight[-544.92420651], bias[-485.17237203]\n",
      "Train loss: 136482.7290525477 \n",
      "\n",
      "Epoch 639 ============================\n",
      "weight[-544.88632346], bias[-485.400548]\n",
      "Train loss: 136473.82453485418 \n",
      "\n",
      "Epoch 640 ============================\n",
      "weight[-544.8485428], bias[-485.62810731]\n",
      "Train loss: 136464.96808224916 \n",
      "\n",
      "Epoch 641 ============================\n",
      "weight[-544.81086424], bias[-485.85505162]\n",
      "Train loss: 136456.15943528537 \n",
      "\n",
      "Epoch 642 ============================\n",
      "weight[-544.77328751], bias[-486.08138261]\n",
      "Train loss: 136447.3983359159 \n",
      "\n",
      "Epoch 643 ============================\n",
      "weight[-544.73581233], bias[-486.30710192]\n",
      "Train loss: 136438.6845274869 \n",
      "\n",
      "Epoch 644 ============================\n",
      "weight[-544.69843843], bias[-486.5322112]\n",
      "Train loss: 136430.01775473004 \n",
      "\n",
      "Epoch 645 ============================\n",
      "weight[-544.66116554], bias[-486.75671212]\n",
      "Train loss: 136421.39776375433 \n",
      "\n",
      "Epoch 646 ============================\n",
      "weight[-544.62399338], bias[-486.9806063]\n",
      "Train loss: 136412.8243020399 \n",
      "\n",
      "Epoch 647 ============================\n",
      "weight[-544.58692167], bias[-487.2038954]\n",
      "Train loss: 136404.29711842933 \n",
      "\n",
      "Epoch 648 ============================\n",
      "weight[-544.54995016], bias[-487.42658104]\n",
      "Train loss: 136395.81596312145 \n",
      "\n",
      "Epoch 649 ============================\n",
      "weight[-544.51307857], bias[-487.64866486]\n",
      "Train loss: 136387.38058766315 \n",
      "\n",
      "Epoch 650 ============================\n",
      "weight[-544.47630662], bias[-487.87014848]\n",
      "Train loss: 136378.9907449423 \n",
      "\n",
      "Epoch 651 ============================\n",
      "weight[-544.43963406], bias[-488.09103352]\n",
      "Train loss: 136370.6461891811 \n",
      "\n",
      "Epoch 652 ============================\n",
      "weight[-544.4030606], bias[-488.31132161]\n",
      "Train loss: 136362.34667592813 \n",
      "\n",
      "Epoch 653 ============================\n",
      "weight[-544.36658599], bias[-488.53101436]\n",
      "Train loss: 136354.0919620515 \n",
      "\n",
      "Epoch 654 ============================\n",
      "weight[-544.33020995], bias[-488.75011337]\n",
      "Train loss: 136345.88180573157 \n",
      "\n",
      "Epoch 655 ============================\n",
      "weight[-544.29393222], bias[-488.96862026]\n",
      "Train loss: 136337.71596645433 \n",
      "\n",
      "Epoch 656 ============================\n",
      "weight[-544.25775253], bias[-489.18653661]\n",
      "Train loss: 136329.5942050037 \n",
      "\n",
      "Epoch 657 ============================\n",
      "weight[-544.22167062], bias[-489.40386403]\n",
      "Train loss: 136321.51628345498 \n",
      "\n",
      "Epoch 658 ============================\n",
      "weight[-544.18568622], bias[-489.62060411]\n",
      "Train loss: 136313.48196516788 \n",
      "\n",
      "Epoch 659 ============================\n",
      "weight[-544.14979908], bias[-489.83675843]\n",
      "Train loss: 136305.4910147793 \n",
      "\n",
      "Epoch 660 ============================\n",
      "weight[-544.11400892], bias[-490.05232858]\n",
      "Train loss: 136297.54319819662 \n",
      "\n",
      "Epoch 661 ============================\n",
      "weight[-544.07831549], bias[-490.26731614]\n",
      "Train loss: 136289.63828259066 \n",
      "\n",
      "Epoch 662 ============================\n",
      "weight[-544.04271852], bias[-490.48172268]\n",
      "Train loss: 136281.77603638938 \n",
      "\n",
      "Epoch 663 ============================\n",
      "weight[-544.00721775], bias[-490.69554977]\n",
      "Train loss: 136273.9562292704 \n",
      "\n",
      "Epoch 664 ============================\n",
      "weight[-543.97181293], bias[-490.90879898]\n",
      "Train loss: 136266.17863215457 \n",
      "\n",
      "Epoch 665 ============================\n",
      "weight[-543.93650379], bias[-491.12147186]\n",
      "Train loss: 136258.44301719972 \n",
      "\n",
      "Epoch 666 ============================\n",
      "weight[-543.90129008], bias[-491.33356999]\n",
      "Train loss: 136250.74915779292 \n",
      "\n",
      "Epoch 667 ============================\n",
      "weight[-543.86617153], bias[-491.5450949]\n",
      "Train loss: 136243.0968285448 \n",
      "\n",
      "Epoch 668 ============================\n",
      "weight[-543.8311479], bias[-491.75604816]\n",
      "Train loss: 136235.48580528278 \n",
      "\n",
      "Epoch 669 ============================\n",
      "weight[-543.79621891], bias[-491.9664313]\n",
      "Train loss: 136227.91586504376 \n",
      "\n",
      "Epoch 670 ============================\n",
      "weight[-543.76138433], bias[-492.17624587]\n",
      "Train loss: 136220.3867860687 \n",
      "\n",
      "Epoch 671 ============================\n",
      "weight[-543.72664389], bias[-492.38549339]\n",
      "Train loss: 136212.89834779524 \n",
      "\n",
      "Epoch 672 ============================\n",
      "weight[-543.69199734], bias[-492.59417541]\n",
      "Train loss: 136205.45033085192 \n",
      "\n",
      "Epoch 673 ============================\n",
      "weight[-543.65744442], bias[-492.80229346]\n",
      "Train loss: 136198.04251705096 \n",
      "\n",
      "Epoch 674 ============================\n",
      "weight[-543.62298488], bias[-493.00984905]\n",
      "Train loss: 136190.67468938275 \n",
      "\n",
      "Epoch 675 ============================\n",
      "weight[-543.58861848], bias[-493.21684371]\n",
      "Train loss: 136183.3466320087 \n",
      "\n",
      "Epoch 676 ============================\n",
      "weight[-543.55434495], bias[-493.42327895]\n",
      "Train loss: 136176.05813025575 \n",
      "\n",
      "Epoch 677 ============================\n",
      "weight[-543.52016404], bias[-493.62915628]\n",
      "Train loss: 136168.80897060878 \n",
      "\n",
      "Epoch 678 ============================\n",
      "weight[-543.48607552], bias[-493.83447722]\n",
      "Train loss: 136161.59894070626 \n",
      "\n",
      "Epoch 679 ============================\n",
      "weight[-543.45207912], bias[-494.03924327]\n",
      "Train loss: 136154.42782933215 \n",
      "\n",
      "Epoch 680 ============================\n",
      "weight[-543.41817459], bias[-494.24345592]\n",
      "Train loss: 136147.2954264108 \n",
      "\n",
      "Epoch 681 ============================\n",
      "weight[-543.3843617], bias[-494.44711667]\n",
      "Train loss: 136140.20152300052 \n",
      "\n",
      "Epoch 682 ============================\n",
      "weight[-543.35064019], bias[-494.65022701]\n",
      "Train loss: 136133.1459112875 \n",
      "\n",
      "Epoch 683 ============================\n",
      "weight[-543.31700981], bias[-494.85278844]\n",
      "Train loss: 136126.1283845794 \n",
      "\n",
      "Epoch 684 ============================\n",
      "weight[-543.28347032], bias[-495.05480243]\n",
      "Train loss: 136119.14873729998 \n",
      "\n",
      "Epoch 685 ============================\n",
      "weight[-543.25002148], bias[-495.25627046]\n",
      "Train loss: 136112.20676498237 \n",
      "\n",
      "Epoch 686 ============================\n",
      "weight[-543.21666303], bias[-495.45719401]\n",
      "Train loss: 136105.3022642636 \n",
      "\n",
      "Epoch 687 ============================\n",
      "weight[-543.18339473], bias[-495.65757455]\n",
      "Train loss: 136098.43503287795 \n",
      "\n",
      "Epoch 688 ============================\n",
      "weight[-543.15021635], bias[-495.85741355]\n",
      "Train loss: 136091.6048696522 \n",
      "\n",
      "Epoch 689 ============================\n",
      "weight[-543.11712763], bias[-496.05671248]\n",
      "Train loss: 136084.8115744983 \n",
      "\n",
      "Epoch 690 ============================\n",
      "weight[-543.08412834], bias[-496.25547278]\n",
      "Train loss: 136078.05494840894 \n",
      "\n",
      "Epoch 691 ============================\n",
      "weight[-543.05121823], bias[-496.45369592]\n",
      "Train loss: 136071.33479345043 \n",
      "\n",
      "Epoch 692 ============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight[-543.01839706], bias[-496.65138335]\n",
      "Train loss: 136064.65091275802 \n",
      "\n",
      "Epoch 693 ============================\n",
      "weight[-542.98566459], bias[-496.84853651]\n",
      "Train loss: 136058.00311052904 \n",
      "\n",
      "Epoch 694 ============================\n",
      "weight[-542.95302059], bias[-497.04515686]\n",
      "Train loss: 136051.39119201808 \n",
      "\n",
      "Epoch 695 ============================\n",
      "weight[-542.9204648], bias[-497.24124582]\n",
      "Train loss: 136044.81496353095 \n",
      "\n",
      "Epoch 696 ============================\n",
      "weight[-542.887997], bias[-497.43680484]\n",
      "Train loss: 136038.27423241883 \n",
      "\n",
      "Epoch 697 ============================\n",
      "weight[-542.85561695], bias[-497.63183535]\n",
      "Train loss: 136031.7688070729 \n",
      "\n",
      "Epoch 698 ============================\n",
      "weight[-542.82332441], bias[-497.82633878]\n",
      "Train loss: 136025.29849691852 \n",
      "\n",
      "Epoch 699 ============================\n",
      "weight[-542.79111914], bias[-498.02031655]\n",
      "Train loss: 136018.86311240998 \n",
      "\n",
      "Epoch 700 ============================\n",
      "weight[-542.7590009], bias[-498.21377008]\n",
      "Train loss: 136012.46246502423 \n",
      "\n",
      "Epoch 701 ============================\n",
      "weight[-542.72696947], bias[-498.40670078]\n",
      "Train loss: 136006.09636725642 \n",
      "\n",
      "Epoch 702 ============================\n",
      "weight[-542.69502461], bias[-498.59911008]\n",
      "Train loss: 135999.76463261325 \n",
      "\n",
      "Epoch 703 ============================\n",
      "weight[-542.66316608], bias[-498.79099939]\n",
      "Train loss: 135993.46707560858 \n",
      "\n",
      "Epoch 704 ============================\n",
      "weight[-542.63139364], bias[-498.98237009]\n",
      "Train loss: 135987.20351175722 \n",
      "\n",
      "Epoch 705 ============================\n",
      "weight[-542.59970708], bias[-499.17322361]\n",
      "Train loss: 135980.97375756953 \n",
      "\n",
      "Epoch 706 ============================\n",
      "weight[-542.56810615], bias[-499.36356133]\n",
      "Train loss: 135974.77763054727 \n",
      "\n",
      "Epoch 707 ============================\n",
      "weight[-542.53659063], bias[-499.55338465]\n",
      "Train loss: 135968.6149491761 \n",
      "\n",
      "Epoch 708 ============================\n",
      "weight[-542.50516027], bias[-499.74269496]\n",
      "Train loss: 135962.48553292212 \n",
      "\n",
      "Epoch 709 ============================\n",
      "weight[-542.47381486], bias[-499.93149364]\n",
      "Train loss: 135956.38920222604 \n",
      "\n",
      "Epoch 710 ============================\n",
      "weight[-542.44255417], bias[-500.11978209]\n",
      "Train loss: 135950.32577849753 \n",
      "\n",
      "Epoch 711 ============================\n",
      "weight[-542.41137795], bias[-500.30756167]\n",
      "Train loss: 135944.29508411014 \n",
      "\n",
      "Epoch 712 ============================\n",
      "weight[-542.380286], bias[-500.49483376]\n",
      "Train loss: 135938.29694239647 \n",
      "\n",
      "Epoch 713 ============================\n",
      "weight[-542.34927807], bias[-500.68159974]\n",
      "Train loss: 135932.33117764277 \n",
      "\n",
      "Epoch 714 ============================\n",
      "weight[-542.31835394], bias[-500.86786097]\n",
      "Train loss: 135926.39761508352 \n",
      "\n",
      "Epoch 715 ============================\n",
      "weight[-542.28751338], bias[-501.05361882]\n",
      "Train loss: 135920.4960808968 \n",
      "\n",
      "Epoch 716 ============================\n",
      "weight[-542.25675618], bias[-501.23887464]\n",
      "Train loss: 135914.62640219872 \n",
      "\n",
      "Epoch 717 ============================\n",
      "weight[-542.2260821], bias[-501.4236298]\n",
      "Train loss: 135908.78840703893 \n",
      "\n",
      "Epoch 718 ============================\n",
      "weight[-542.19549092], bias[-501.60788565]\n",
      "Train loss: 135902.9819243946 \n",
      "\n",
      "Epoch 719 ============================\n",
      "weight[-542.16498241], bias[-501.79164353]\n",
      "Train loss: 135897.20678416712 \n",
      "\n",
      "Epoch 720 ============================\n",
      "weight[-542.13455635], bias[-501.97490479]\n",
      "Train loss: 135891.46281717485 \n",
      "\n",
      "Epoch 721 ============================\n",
      "weight[-542.10421252], bias[-502.15767078]\n",
      "Train loss: 135885.74985515 \n",
      "\n",
      "Epoch 722 ============================\n",
      "weight[-542.0739507], bias[-502.33994283]\n",
      "Train loss: 135880.06773073316 \n",
      "\n",
      "Epoch 723 ============================\n",
      "weight[-542.04377067], bias[-502.52172227]\n",
      "Train loss: 135874.416277468 \n",
      "\n",
      "Epoch 724 ============================\n",
      "weight[-542.01367219], bias[-502.70301045]\n",
      "Train loss: 135868.79532979682 \n",
      "\n",
      "Epoch 725 ============================\n",
      "weight[-541.98365506], bias[-502.88380868]\n",
      "Train loss: 135863.20472305547 \n",
      "\n",
      "Epoch 726 ============================\n",
      "weight[-541.95371905], bias[-503.06411829]\n",
      "Train loss: 135857.64429346888 \n",
      "\n",
      "Epoch 727 ============================\n",
      "weight[-541.92386395], bias[-503.24394061]\n",
      "Train loss: 135852.1138781457 \n",
      "\n",
      "Epoch 728 ============================\n",
      "weight[-541.89408953], bias[-503.42327694]\n",
      "Train loss: 135846.61331507415 \n",
      "\n",
      "Epoch 729 ============================\n",
      "weight[-541.86439558], bias[-503.6021286]\n",
      "Train loss: 135841.14244311652 \n",
      "\n",
      "Epoch 730 ============================\n",
      "weight[-541.83478188], bias[-503.78049691]\n",
      "Train loss: 135835.70110200543 \n",
      "\n",
      "Epoch 731 ============================\n",
      "weight[-541.80524822], bias[-503.95838316]\n",
      "Train loss: 135830.28913233825 \n",
      "\n",
      "Epoch 732 ============================\n",
      "weight[-541.77579437], bias[-504.13578866]\n",
      "Train loss: 135824.90637557287 \n",
      "\n",
      "Epoch 733 ============================\n",
      "weight[-541.74642012], bias[-504.31271471]\n",
      "Train loss: 135819.55267402297 \n",
      "\n",
      "Epoch 734 ============================\n",
      "weight[-541.71712525], bias[-504.48916261]\n",
      "Train loss: 135814.22787085333 \n",
      "\n",
      "Epoch 735 ============================\n",
      "weight[-541.68790956], bias[-504.66513364]\n",
      "Train loss: 135808.9318100755 \n",
      "\n",
      "Epoch 736 ============================\n",
      "weight[-541.65877283], bias[-504.8406291]\n",
      "Train loss: 135803.6643365427 \n",
      "\n",
      "Epoch 737 ============================\n",
      "weight[-541.62971484], bias[-505.01565028]\n",
      "Train loss: 135798.42529594584 \n",
      "\n",
      "Epoch 738 ============================\n",
      "weight[-541.60073538], bias[-505.19019844]\n",
      "Train loss: 135793.21453480877 \n",
      "\n",
      "Epoch 739 ============================\n",
      "weight[-541.57183424], bias[-505.36427488]\n",
      "Train loss: 135788.03190048374 \n",
      "\n",
      "Epoch 740 ============================\n",
      "weight[-541.5430112], bias[-505.53788086]\n",
      "Train loss: 135782.87724114687 \n",
      "\n",
      "Epoch 741 ============================\n",
      "weight[-541.51426607], bias[-505.71101767]\n",
      "Train loss: 135777.750405794 \n",
      "\n",
      "Epoch 742 ============================\n",
      "weight[-541.48559861], bias[-505.88368655]\n",
      "Train loss: 135772.65124423584 \n",
      "\n",
      "Epoch 743 ============================\n",
      "weight[-541.45700864], bias[-506.05588879]\n",
      "Train loss: 135767.57960709385 \n",
      "\n",
      "Epoch 744 ============================\n",
      "weight[-541.42849593], bias[-506.22762564]\n",
      "Train loss: 135762.53534579606 \n",
      "\n",
      "Epoch 745 ============================\n",
      "weight[-541.40006028], bias[-506.39889836]\n",
      "Train loss: 135757.51831257204 \n",
      "\n",
      "Epoch 746 ============================\n",
      "weight[-541.37170147], bias[-506.56970821]\n",
      "Train loss: 135752.52836044945 \n",
      "\n",
      "Epoch 747 ============================\n",
      "weight[-541.34341931], bias[-506.74005643]\n",
      "Train loss: 135747.56534324886 \n",
      "\n",
      "Epoch 748 ============================\n",
      "weight[-541.31521359], bias[-506.90994427]\n",
      "Train loss: 135742.62911558023 \n",
      "\n",
      "Epoch 749 ============================\n",
      "weight[-541.28708409], bias[-507.07937298]\n",
      "Train loss: 135737.7195328381 \n",
      "\n",
      "Epoch 750 ============================\n",
      "weight[-541.25903061], bias[-507.24834379]\n",
      "Train loss: 135732.83645119765 \n",
      "\n",
      "Epoch 751 ============================\n",
      "weight[-541.23105295], bias[-507.41685795]\n",
      "Train loss: 135727.97972761042 \n",
      "\n",
      "Epoch 752 ============================\n",
      "weight[-541.2031509], bias[-507.58491669]\n",
      "Train loss: 135723.14921980014 \n",
      "\n",
      "Epoch 753 ============================\n",
      "weight[-541.17532426], bias[-507.75252124]\n",
      "Train loss: 135718.34478625824 \n",
      "\n",
      "Epoch 754 ============================\n",
      "weight[-541.14757282], bias[-507.91967283]\n",
      "Train loss: 135713.56628624038 \n",
      "\n",
      "Epoch 755 ============================\n",
      "weight[-541.11989638], bias[-508.08637268]\n",
      "Train loss: 135708.8135797619 \n",
      "\n",
      "Epoch 756 ============================\n",
      "weight[-541.09229474], bias[-508.25262201]\n",
      "Train loss: 135704.08652759317 \n",
      "\n",
      "Epoch 757 ============================\n",
      "weight[-541.0647677], bias[-508.41842204]\n",
      "Train loss: 135699.38499125684 \n",
      "\n",
      "Epoch 758 ============================\n",
      "weight[-541.03731505], bias[-508.58377398]\n",
      "Train loss: 135694.70883302286 \n",
      "\n",
      "Epoch 759 ============================\n",
      "weight[-541.00993659], bias[-508.74867905]\n",
      "Train loss: 135690.05791590424 \n",
      "\n",
      "Epoch 760 ============================\n",
      "weight[-540.98263212], bias[-508.91313845]\n",
      "Train loss: 135685.4321036538 \n",
      "\n",
      "Epoch 761 ============================\n",
      "weight[-540.95540145], bias[-509.07715339]\n",
      "Train loss: 135680.8312607597 \n",
      "\n",
      "Epoch 762 ============================\n",
      "weight[-540.92824436], bias[-509.24072507]\n",
      "Train loss: 135676.25525244154 \n",
      "\n",
      "Epoch 763 ============================\n",
      "weight[-540.90116068], bias[-509.40385468]\n",
      "Train loss: 135671.7039446465 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 764 ============================\n",
      "weight[-540.87415018], bias[-509.56654343]\n",
      "Train loss: 135667.17720404515 \n",
      "\n",
      "Epoch 765 ============================\n",
      "weight[-540.84721269], bias[-509.7287925]\n",
      "Train loss: 135662.67489802805 \n",
      "\n",
      "Epoch 766 ============================\n",
      "weight[-540.820348], bias[-509.89060308]\n",
      "Train loss: 135658.19689470154 \n",
      "\n",
      "Epoch 767 ============================\n",
      "weight[-540.7935559], bias[-510.05197635]\n",
      "Train loss: 135653.74306288353 \n",
      "\n",
      "Epoch 768 ============================\n",
      "weight[-540.76683622], bias[-510.21291351]\n",
      "Train loss: 135649.3132721004 \n",
      "\n",
      "Epoch 769 ============================\n",
      "weight[-540.74018875], bias[-510.37341572]\n",
      "Train loss: 135644.90739258265 \n",
      "\n",
      "Epoch 770 ============================\n",
      "weight[-540.71361329], bias[-510.53348416]\n",
      "Train loss: 135640.52529526138 \n",
      "\n",
      "Epoch 771 ============================\n",
      "weight[-540.68710966], bias[-510.69312]\n",
      "Train loss: 135636.16685176425 \n",
      "\n",
      "Epoch 772 ============================\n",
      "weight[-540.66067766], bias[-510.85232442]\n",
      "Train loss: 135631.83193441186 \n",
      "\n",
      "Epoch 773 ============================\n",
      "weight[-540.63431709], bias[-511.01109858]\n",
      "Train loss: 135627.5204162141 \n",
      "\n",
      "Epoch 774 ============================\n",
      "weight[-540.60802776], bias[-511.16944364]\n",
      "Train loss: 135623.23217086616 \n",
      "\n",
      "Epoch 775 ============================\n",
      "weight[-540.58180948], bias[-511.32736076]\n",
      "Train loss: 135618.9670727453 \n",
      "\n",
      "Epoch 776 ============================\n",
      "weight[-540.55566205], bias[-511.4848511]\n",
      "Train loss: 135614.72499690653 \n",
      "\n",
      "Epoch 777 ============================\n",
      "weight[-540.52958529], bias[-511.64191582]\n",
      "Train loss: 135610.50581907944 \n",
      "\n",
      "Epoch 778 ============================\n",
      "weight[-540.50357901], bias[-511.79855605]\n",
      "Train loss: 135606.3094156645 \n",
      "\n",
      "Epoch 779 ============================\n",
      "weight[-540.47764301], bias[-511.95477295]\n",
      "Train loss: 135602.13566372907 \n",
      "\n",
      "Epoch 780 ============================\n",
      "weight[-540.4517771], bias[-512.11056767]\n",
      "Train loss: 135597.98444100472 \n",
      "\n",
      "Epoch 781 ============================\n",
      "weight[-540.4259811], bias[-512.26594134]\n",
      "Train loss: 135593.8556258821 \n",
      "\n",
      "Epoch 782 ============================\n",
      "weight[-540.40025481], bias[-512.4208951]\n",
      "Train loss: 135589.7490974089 \n",
      "\n",
      "Epoch 783 ============================\n",
      "weight[-540.37459805], bias[-512.57543009]\n",
      "Train loss: 135585.66473528565 \n",
      "\n",
      "Epoch 784 ============================\n",
      "weight[-540.34901063], bias[-512.72954744]\n",
      "Train loss: 135581.60241986197 \n",
      "\n",
      "Epoch 785 ============================\n",
      "weight[-540.32349236], bias[-512.88324827]\n",
      "Train loss: 135577.5620321335 \n",
      "\n",
      "Epoch 786 ============================\n",
      "weight[-540.29804305], bias[-513.03653372]\n",
      "Train loss: 135573.5434537383 \n",
      "\n",
      "Epoch 787 ============================\n",
      "weight[-540.27266253], bias[-513.18940491]\n",
      "Train loss: 135569.54656695318 \n",
      "\n",
      "Epoch 788 ============================\n",
      "weight[-540.24735059], bias[-513.34186295]\n",
      "Train loss: 135565.57125469059 \n",
      "\n",
      "Epoch 789 ============================\n",
      "weight[-540.22210707], bias[-513.49390896]\n",
      "Train loss: 135561.61740049487 \n",
      "\n",
      "Epoch 790 ============================\n",
      "weight[-540.19693176], bias[-513.64554406]\n",
      "Train loss: 135557.68488853885 \n",
      "\n",
      "Epoch 791 ============================\n",
      "weight[-540.1718245], bias[-513.79676935]\n",
      "Train loss: 135553.77360362074 \n",
      "\n",
      "Epoch 792 ============================\n",
      "weight[-540.14678508], bias[-513.94758594]\n",
      "Train loss: 135549.8834311607 \n",
      "\n",
      "Epoch 793 ============================\n",
      "weight[-540.12181334], bias[-514.09799495]\n",
      "Train loss: 135546.01425719704 \n",
      "\n",
      "Epoch 794 ============================\n",
      "weight[-540.09690909], bias[-514.24799746]\n",
      "Train loss: 135542.1659683837 \n",
      "\n",
      "Epoch 795 ============================\n",
      "weight[-540.07207214], bias[-514.39759458]\n",
      "Train loss: 135538.33845198582 \n",
      "\n",
      "Epoch 796 ============================\n",
      "weight[-540.04730232], bias[-514.54678741]\n",
      "Train loss: 135534.53159587758 \n",
      "\n",
      "Epoch 797 ============================\n",
      "weight[-540.02259943], bias[-514.69557703]\n",
      "Train loss: 135530.74528853825 \n",
      "\n",
      "Epoch 798 ============================\n",
      "weight[-539.99796331], bias[-514.84396454]\n",
      "Train loss: 135526.97941904885 \n",
      "\n",
      "Epoch 799 ============================\n",
      "weight[-539.97339377], bias[-514.99195102]\n",
      "Train loss: 135523.23387708946 \n",
      "\n",
      "Epoch 800 ============================\n",
      "weight[-539.94889063], bias[-515.13953755]\n",
      "Train loss: 135519.50855293547 \n",
      "\n",
      "Epoch 801 ============================\n",
      "weight[-539.92445371], bias[-515.28672523]\n",
      "Train loss: 135515.80333745453 \n",
      "\n",
      "Epoch 802 ============================\n",
      "weight[-539.90008284], bias[-515.43351512]\n",
      "Train loss: 135512.11812210345 \n",
      "\n",
      "Epoch 803 ============================\n",
      "weight[-539.87577783], bias[-515.5799083]\n",
      "Train loss: 135508.45279892487 \n",
      "\n",
      "Epoch 804 ============================\n",
      "weight[-539.8515385], bias[-515.72590584]\n",
      "Train loss: 135504.80726054416 \n",
      "\n",
      "Epoch 805 ============================\n",
      "weight[-539.82736468], bias[-515.87150881]\n",
      "Train loss: 135501.18140016627 \n",
      "\n",
      "Epoch 806 ============================\n",
      "weight[-539.8032562], bias[-516.01671828]\n",
      "Train loss: 135497.57511157278 \n",
      "\n",
      "Epoch 807 ============================\n",
      "weight[-539.77921287], bias[-516.16153532]\n",
      "Train loss: 135493.98828911842 \n",
      "\n",
      "Epoch 808 ============================\n",
      "weight[-539.75523451], bias[-516.30596097]\n",
      "Train loss: 135490.42082772826 \n",
      "\n",
      "Epoch 809 ============================\n",
      "weight[-539.73132096], bias[-516.44999631]\n",
      "Train loss: 135486.8726228946 \n",
      "\n",
      "Epoch 810 ============================\n",
      "weight[-539.70747204], bias[-516.59364238]\n",
      "Train loss: 135483.34357067352 \n",
      "\n",
      "Epoch 811 ============================\n",
      "weight[-539.68368758], bias[-516.73690024]\n",
      "Train loss: 135479.83356768277 \n",
      "\n",
      "Epoch 812 ============================\n",
      "weight[-539.65996739], bias[-516.87977093]\n",
      "Train loss: 135476.34251109764 \n",
      "\n",
      "Epoch 813 ============================\n",
      "weight[-539.6363113], bias[-517.0222555]\n",
      "Train loss: 135472.87029864863 \n",
      "\n",
      "Epoch 814 ============================\n",
      "weight[-539.61271915], bias[-517.16435501]\n",
      "Train loss: 135469.4168286183 \n",
      "\n",
      "Epoch 815 ============================\n",
      "weight[-539.58919076], bias[-517.30607047]\n",
      "Train loss: 135465.98199983826 \n",
      "\n",
      "Epoch 816 ============================\n",
      "weight[-539.56572595], bias[-517.44740294]\n",
      "Train loss: 135462.56571168604 \n",
      "\n",
      "Epoch 817 ============================\n",
      "weight[-539.54232457], bias[-517.58835346]\n",
      "Train loss: 135459.16786408253 \n",
      "\n",
      "Epoch 818 ============================\n",
      "weight[-539.51898642], bias[-517.72892304]\n",
      "Train loss: 135455.7883574889 \n",
      "\n",
      "Epoch 819 ============================\n",
      "weight[-539.49571135], bias[-517.86911272]\n",
      "Train loss: 135452.42709290318 \n",
      "\n",
      "Epoch 820 ============================\n",
      "weight[-539.47249918], bias[-518.00892353]\n",
      "Train loss: 135449.0839718583 \n",
      "\n",
      "Epoch 821 ============================\n",
      "weight[-539.44934974], bias[-518.1483565]\n",
      "Train loss: 135445.7588964183 \n",
      "\n",
      "Epoch 822 ============================\n",
      "weight[-539.42626286], bias[-518.28741264]\n",
      "Train loss: 135442.45176917617 \n",
      "\n",
      "Epoch 823 ============================\n",
      "weight[-539.40323838], bias[-518.42609296]\n",
      "Train loss: 135439.1624932507 \n",
      "\n",
      "Epoch 824 ============================\n",
      "weight[-539.38027613], bias[-518.5643985]\n",
      "Train loss: 135435.89097228312 \n",
      "\n",
      "Epoch 825 ============================\n",
      "weight[-539.35737593], bias[-518.70233025]\n",
      "Train loss: 135432.63711043535 \n",
      "\n",
      "Epoch 826 ============================\n",
      "weight[-539.33453762], bias[-518.83988924]\n",
      "Train loss: 135429.40081238654 \n",
      "\n",
      "Epoch 827 ============================\n",
      "weight[-539.31176103], bias[-518.97707646]\n",
      "Train loss: 135426.18198333008 \n",
      "\n",
      "Epoch 828 ============================\n",
      "weight[-539.289046], bias[-519.11389293]\n",
      "Train loss: 135422.9805289714 \n",
      "\n",
      "Epoch 829 ============================\n",
      "weight[-539.26639236], bias[-519.25033963]\n",
      "Train loss: 135419.7963555247 \n",
      "\n",
      "Epoch 830 ============================\n",
      "weight[-539.24379993], bias[-519.38641759]\n",
      "Train loss: 135416.62936971066 \n",
      "\n",
      "Epoch 831 ============================\n",
      "weight[-539.22126857], bias[-519.52212778]\n",
      "Train loss: 135413.4794787533 \n",
      "\n",
      "Epoch 832 ============================\n",
      "weight[-539.1987981], bias[-519.65747121]\n",
      "Train loss: 135410.3465903773 \n",
      "\n",
      "Epoch 833 ============================\n",
      "weight[-539.17638836], bias[-519.79244886]\n",
      "Train loss: 135407.23061280572 \n",
      "\n",
      "Epoch 834 ============================\n",
      "weight[-539.15403918], bias[-519.92706172]\n",
      "Train loss: 135404.13145475692 \n",
      "\n",
      "Epoch 835 ============================\n",
      "weight[-539.1317504], bias[-520.06131079]\n",
      "Train loss: 135401.0490254419 \n",
      "\n",
      "Epoch 836 ============================\n",
      "weight[-539.10952186], bias[-520.19519704]\n",
      "Train loss: 135397.98323456166 \n",
      "\n",
      "Epoch 837 ============================\n",
      "weight[-539.08735339], bias[-520.32872145]\n",
      "Train loss: 135394.93399230487 \n",
      "\n",
      "Epoch 838 ============================\n",
      "weight[-539.06524484], bias[-520.461885]\n",
      "Train loss: 135391.90120934488 \n",
      "\n",
      "Epoch 839 ============================\n",
      "weight[-539.04319603], bias[-520.59468867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 135388.88479683702 \n",
      "\n",
      "Epoch 840 ============================\n",
      "weight[-539.02120682], bias[-520.72713343]\n",
      "Train loss: 135385.88466641647 \n",
      "\n",
      "Epoch 841 ============================\n",
      "weight[-538.99927703], bias[-520.85922024]\n",
      "Train loss: 135382.90073019516 \n",
      "\n",
      "Epoch 842 ============================\n",
      "weight[-538.9774065], bias[-520.99095009]\n",
      "Train loss: 135379.9329007596 \n",
      "\n",
      "Epoch 843 ============================\n",
      "weight[-538.95559509], bias[-521.12232392]\n",
      "Train loss: 135376.98109116804 \n",
      "\n",
      "Epoch 844 ============================\n",
      "weight[-538.93384262], bias[-521.25334271]\n",
      "Train loss: 135374.0452149481 \n",
      "\n",
      "Epoch 845 ============================\n",
      "weight[-538.91214894], bias[-521.38400741]\n",
      "Train loss: 135371.12518609408 \n",
      "\n",
      "Epoch 846 ============================\n",
      "weight[-538.89051388], bias[-521.51431898]\n",
      "Train loss: 135368.22091906457 \n",
      "\n",
      "Epoch 847 ============================\n",
      "weight[-538.8689373], bias[-521.64427838]\n",
      "Train loss: 135365.33232877983 \n",
      "\n",
      "Epoch 848 ============================\n",
      "weight[-538.84741903], bias[-521.77388655]\n",
      "Train loss: 135362.45933061957 \n",
      "\n",
      "Epoch 849 ============================\n",
      "weight[-538.82595891], bias[-521.90314444]\n",
      "Train loss: 135359.60184042004 \n",
      "\n",
      "Epoch 850 ============================\n",
      "weight[-538.80455679], bias[-522.03205301]\n",
      "Train loss: 135356.75977447184 \n",
      "\n",
      "Epoch 851 ============================\n",
      "weight[-538.78321251], bias[-522.16061319]\n",
      "Train loss: 135353.93304951748 \n",
      "\n",
      "Epoch 852 ============================\n",
      "weight[-538.76192592], bias[-522.28882594]\n",
      "Train loss: 135351.12158274904 \n",
      "\n",
      "Epoch 853 ============================\n",
      "weight[-538.74069686], bias[-522.41669217]\n",
      "Train loss: 135348.3252918051 \n",
      "\n",
      "Epoch 854 ============================\n",
      "weight[-538.71952516], bias[-522.54421284]\n",
      "Train loss: 135345.5440947692 \n",
      "\n",
      "Epoch 855 ============================\n",
      "weight[-538.69841069], bias[-522.67138888]\n",
      "Train loss: 135342.77791016726 \n",
      "\n",
      "Epoch 856 ============================\n",
      "weight[-538.67735328], bias[-522.79822121]\n",
      "Train loss: 135340.02665696412 \n",
      "\n",
      "Epoch 857 ============================\n",
      "weight[-538.65635277], bias[-522.92471078]\n",
      "Train loss: 135337.29025456312 \n",
      "\n",
      "Epoch 858 ============================\n",
      "weight[-538.63540903], bias[-523.05085849]\n",
      "Train loss: 135334.56862280192 \n",
      "\n",
      "Epoch 859 ============================\n",
      "weight[-538.61452188], bias[-523.17666528]\n",
      "Train loss: 135331.86168195106 \n",
      "\n",
      "Epoch 860 ============================\n",
      "weight[-538.59369119], bias[-523.30213208]\n",
      "Train loss: 135329.16935271153 \n",
      "\n",
      "Epoch 861 ============================\n",
      "weight[-538.57291679], bias[-523.42725978]\n",
      "Train loss: 135326.49155621236 \n",
      "\n",
      "Epoch 862 ============================\n",
      "weight[-538.55219853], bias[-523.55204933]\n",
      "Train loss: 135323.82821400804 \n",
      "\n",
      "Epoch 863 ============================\n",
      "weight[-538.53153627], bias[-523.67650162]\n",
      "Train loss: 135321.17924807704 \n",
      "\n",
      "Epoch 864 ============================\n",
      "weight[-538.51092985], bias[-523.80061757]\n",
      "Train loss: 135318.54458081847 \n",
      "\n",
      "Epoch 865 ============================\n",
      "weight[-538.49037912], bias[-523.92439808]\n",
      "Train loss: 135315.92413505042 \n",
      "\n",
      "Epoch 866 ============================\n",
      "weight[-538.46988392], bias[-524.04784408]\n",
      "Train loss: 135313.317834008 \n",
      "\n",
      "Epoch 867 ============================\n",
      "weight[-538.44944412], bias[-524.17095645]\n",
      "Train loss: 135310.72560134 \n",
      "\n",
      "Epoch 868 ============================\n",
      "weight[-538.42905956], bias[-524.2937361]\n",
      "Train loss: 135308.14736110778 \n",
      "\n",
      "Epoch 869 ============================\n",
      "weight[-538.40873009], bias[-524.41618393]\n",
      "Train loss: 135305.5830377826 \n",
      "\n",
      "Epoch 870 ============================\n",
      "weight[-538.38845556], bias[-524.53830084]\n",
      "Train loss: 135303.0325562435 \n",
      "\n",
      "Epoch 871 ============================\n",
      "weight[-538.36823582], bias[-524.66008772]\n",
      "Train loss: 135300.49584177454 \n",
      "\n",
      "Epoch 872 ============================\n",
      "weight[-538.34807073], bias[-524.78154546]\n",
      "Train loss: 135297.97282006356 \n",
      "\n",
      "Epoch 873 ============================\n",
      "weight[-538.32796014], bias[-524.90267495]\n",
      "Train loss: 135295.4634171993 \n",
      "\n",
      "Epoch 874 ============================\n",
      "weight[-538.30790389], bias[-525.02347709]\n",
      "Train loss: 135292.96755966955 \n",
      "\n",
      "Epoch 875 ============================\n",
      "weight[-538.28790185], bias[-525.14395274]\n",
      "Train loss: 135290.48517435882 \n",
      "\n",
      "Epoch 876 ============================\n",
      "weight[-538.26795387], bias[-525.26410281]\n",
      "Train loss: 135288.01618854646 \n",
      "\n",
      "Epoch 877 ============================\n",
      "weight[-538.2480598], bias[-525.38392816]\n",
      "Train loss: 135285.56052990412 \n",
      "\n",
      "Epoch 878 ============================\n",
      "weight[-538.22821949], bias[-525.50342967]\n",
      "Train loss: 135283.1181264941 \n",
      "\n",
      "Epoch 879 ============================\n",
      "weight[-538.2084328], bias[-525.62260822]\n",
      "Train loss: 135280.68890676668 \n",
      "\n",
      "Epoch 880 ============================\n",
      "weight[-538.18869959], bias[-525.74146469]\n",
      "Train loss: 135278.2727995587 \n",
      "\n",
      "Epoch 881 ============================\n",
      "weight[-538.16901971], bias[-525.85999993]\n",
      "Train loss: 135275.86973409096 \n",
      "\n",
      "Epoch 882 ============================\n",
      "weight[-538.14939301], bias[-525.97821483]\n",
      "Train loss: 135273.4796399664 \n",
      "\n",
      "Epoch 883 ============================\n",
      "weight[-538.12981936], bias[-526.09611024]\n",
      "Train loss: 135271.10244716774 \n",
      "\n",
      "Epoch 884 ============================\n",
      "weight[-538.1102986], bias[-526.21368704]\n",
      "Train loss: 135268.73808605594 \n",
      "\n",
      "Epoch 885 ============================\n",
      "weight[-538.0908306], bias[-526.33094607]\n",
      "Train loss: 135266.38648736765 \n",
      "\n",
      "Epoch 886 ============================\n",
      "weight[-538.07141522], bias[-526.4478882]\n",
      "Train loss: 135264.04758221345 \n",
      "\n",
      "Epoch 887 ============================\n",
      "weight[-538.0520523], bias[-526.56451429]\n",
      "Train loss: 135261.7213020759 \n",
      "\n",
      "Epoch 888 ============================\n",
      "weight[-538.03274172], bias[-526.68082519]\n",
      "Train loss: 135259.40757880706 \n",
      "\n",
      "Epoch 889 ============================\n",
      "weight[-538.01348332], bias[-526.79682175]\n",
      "Train loss: 135257.10634462722 \n",
      "\n",
      "Epoch 890 ============================\n",
      "weight[-537.99427698], bias[-526.91250483]\n",
      "Train loss: 135254.81753212228 \n",
      "\n",
      "Epoch 891 ============================\n",
      "weight[-537.97512253], bias[-527.02787526]\n",
      "Train loss: 135252.54107424233 \n",
      "\n",
      "Epoch 892 ============================\n",
      "weight[-537.95601986], bias[-527.14293389]\n",
      "Train loss: 135250.27690429895 \n",
      "\n",
      "Epoch 893 ============================\n",
      "weight[-537.93696881], bias[-527.25768158]\n",
      "Train loss: 135248.02495596404 \n",
      "\n",
      "Epoch 894 ============================\n",
      "weight[-537.91796925], bias[-527.37211914]\n",
      "Train loss: 135245.7851632674 \n",
      "\n",
      "Epoch 895 ============================\n",
      "weight[-537.89902103], bias[-527.48624744]\n",
      "Train loss: 135243.5574605949 \n",
      "\n",
      "Epoch 896 ============================\n",
      "weight[-537.88012402], bias[-527.60006729]\n",
      "Train loss: 135241.34178268668 \n",
      "\n",
      "Epoch 897 ============================\n",
      "weight[-537.86127809], bias[-527.71357954]\n",
      "Train loss: 135239.13806463507 \n",
      "\n",
      "Epoch 898 ============================\n",
      "weight[-537.84248308], bias[-527.82678501]\n",
      "Train loss: 135236.9462418828 \n",
      "\n",
      "Epoch 899 ============================\n",
      "weight[-537.82373887], bias[-527.93968454]\n",
      "Train loss: 135234.76625022097 \n",
      "\n",
      "Epoch 900 ============================\n",
      "weight[-537.80504532], bias[-528.05227894]\n",
      "Train loss: 135232.5980257874 \n",
      "\n",
      "Epoch 901 ============================\n",
      "weight[-537.78640229], bias[-528.16456906]\n",
      "Train loss: 135230.44150506446 \n",
      "\n",
      "Epoch 902 ============================\n",
      "weight[-537.76780965], bias[-528.2765557]\n",
      "Train loss: 135228.29662487772 \n",
      "\n",
      "Epoch 903 ============================\n",
      "weight[-537.74926725], bias[-528.38823969]\n",
      "Train loss: 135226.16332239335 \n",
      "\n",
      "Epoch 904 ============================\n",
      "weight[-537.73077496], bias[-528.49962185]\n",
      "Train loss: 135224.04153511685 \n",
      "\n",
      "Epoch 905 ============================\n",
      "weight[-537.71233265], bias[-528.61070299]\n",
      "Train loss: 135221.93120089115 \n",
      "\n",
      "Epoch 906 ============================\n",
      "weight[-537.69394018], bias[-528.72148393]\n",
      "Train loss: 135219.8322578946 \n",
      "\n",
      "Epoch 907 ============================\n",
      "weight[-537.67559742], bias[-528.83196547]\n",
      "Train loss: 135217.74464463923 \n",
      "\n",
      "Epoch 908 ============================\n",
      "weight[-537.65730423], bias[-528.94214843]\n",
      "Train loss: 135215.668299969 \n",
      "\n",
      "Epoch 909 ============================\n",
      "weight[-537.63906048], bias[-529.05203361]\n",
      "Train loss: 135213.60316305797 \n",
      "\n",
      "Epoch 910 ============================\n",
      "weight[-537.62086603], bias[-529.16162182]\n",
      "Train loss: 135211.54917340857 \n",
      "\n",
      "Epoch 911 ============================\n",
      "weight[-537.60272076], bias[-529.27091386]\n",
      "Train loss: 135209.50627084982 \n",
      "\n",
      "Epoch 912 ============================\n",
      "weight[-537.58462453], bias[-529.37991053]\n",
      "Train loss: 135207.4743955354 \n",
      "\n",
      "Epoch 913 ============================\n",
      "weight[-537.5665772], bias[-529.48861263]\n",
      "Train loss: 135205.45348794197 \n",
      "\n",
      "Epoch 914 ============================\n",
      "weight[-537.54857864], bias[-529.59702096]\n",
      "Train loss: 135203.44348886787 \n",
      "\n",
      "Epoch 915 ============================\n",
      "weight[-537.53062873], bias[-529.7051363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 135201.44433943057 \n",
      "\n",
      "Epoch 916 ============================\n",
      "weight[-537.51272733], bias[-529.81295946]\n",
      "Train loss: 135199.45598106555 \n",
      "\n",
      "Epoch 917 ============================\n",
      "weight[-537.49487431], bias[-529.92049121]\n",
      "Train loss: 135197.47835552462 \n",
      "\n",
      "Epoch 918 ============================\n",
      "weight[-537.47706954], bias[-530.02773236]\n",
      "Train loss: 135195.51140487366 \n",
      "\n",
      "Epoch 919 ============================\n",
      "weight[-537.45931288], bias[-530.13468368]\n",
      "Train loss: 135193.5550714913 \n",
      "\n",
      "Epoch 920 ============================\n",
      "weight[-537.44160422], bias[-530.24134595]\n",
      "Train loss: 135191.6092980675 \n",
      "\n",
      "Epoch 921 ============================\n",
      "weight[-537.42394341], bias[-530.34771996]\n",
      "Train loss: 135189.67402760114 \n",
      "\n",
      "Epoch 922 ============================\n",
      "weight[-537.40633033], bias[-530.45380649]\n",
      "Train loss: 135187.74920339938 \n",
      "\n",
      "Epoch 923 ============================\n",
      "weight[-537.38876486], bias[-530.55960632]\n",
      "Train loss: 135185.83476907483 \n",
      "\n",
      "Epoch 924 ============================\n",
      "weight[-537.37124685], bias[-530.66512021]\n",
      "Train loss: 135183.93066854458 \n",
      "\n",
      "Epoch 925 ============================\n",
      "weight[-537.35377619], bias[-530.77034894]\n",
      "Train loss: 135182.03684602876 \n",
      "\n",
      "Epoch 926 ============================\n",
      "weight[-537.33635274], bias[-530.87529329]\n",
      "Train loss: 135180.1532460483 \n",
      "\n",
      "Epoch 927 ============================\n",
      "weight[-537.31897639], bias[-530.97995401]\n",
      "Train loss: 135178.2798134235 \n",
      "\n",
      "Epoch 928 ============================\n",
      "weight[-537.30164699], bias[-531.08433189]\n",
      "Train loss: 135176.41649327293 \n",
      "\n",
      "Epoch 929 ============================\n",
      "weight[-537.28436443], bias[-531.18842767]\n",
      "Train loss: 135174.56323101118 \n",
      "\n",
      "Epoch 930 ============================\n",
      "weight[-537.26712857], bias[-531.29224213]\n",
      "Train loss: 135172.71997234705 \n",
      "\n",
      "Epoch 931 ============================\n",
      "weight[-537.2499393], bias[-531.39577603]\n",
      "Train loss: 135170.88666328343 \n",
      "\n",
      "Epoch 932 ============================\n",
      "weight[-537.23279648], bias[-531.49903012]\n",
      "Train loss: 135169.0632501135 \n",
      "\n",
      "Epoch 933 ============================\n",
      "weight[-537.21569999], bias[-531.60200515]\n",
      "Train loss: 135167.2496794212 \n",
      "\n",
      "Epoch 934 ============================\n",
      "weight[-537.1986497], bias[-531.70470189]\n",
      "Train loss: 135165.4458980783 \n",
      "\n",
      "Epoch 935 ============================\n",
      "weight[-537.1816455], bias[-531.80712108]\n",
      "Train loss: 135163.6518532437 \n",
      "\n",
      "Epoch 936 ============================\n",
      "weight[-537.16468725], bias[-531.90926348]\n",
      "Train loss: 135161.86749236134 \n",
      "\n",
      "Epoch 937 ============================\n",
      "weight[-537.14777483], bias[-532.01112983]\n",
      "Train loss: 135160.09276315884 \n",
      "\n",
      "Epoch 938 ============================\n",
      "weight[-537.13090812], bias[-532.11272089]\n",
      "Train loss: 135158.32761364584 \n",
      "\n",
      "Epoch 939 ============================\n",
      "weight[-537.11408699], bias[-532.21403738]\n",
      "Train loss: 135156.57199211314 \n",
      "\n",
      "Epoch 940 ============================\n",
      "weight[-537.09731132], bias[-532.31508006]\n",
      "Train loss: 135154.82584713007 \n",
      "\n",
      "Epoch 941 ============================\n",
      "weight[-537.08058099], bias[-532.41584967]\n",
      "Train loss: 135153.08912754385 \n",
      "\n",
      "Epoch 942 ============================\n",
      "weight[-537.06389587], bias[-532.51634694]\n",
      "Train loss: 135151.36178247782 \n",
      "\n",
      "Epoch 943 ============================\n",
      "weight[-537.04725585], bias[-532.6165726]\n",
      "Train loss: 135149.64376132985 \n",
      "\n",
      "Epoch 944 ============================\n",
      "weight[-537.03066079], bias[-532.71652741]\n",
      "Train loss: 135147.93501377088 \n",
      "\n",
      "Epoch 945 ============================\n",
      "weight[-537.01411059], bias[-532.81621207]\n",
      "Train loss: 135146.2354897439 \n",
      "\n",
      "Epoch 946 ============================\n",
      "weight[-536.99760511], bias[-532.91562733]\n",
      "Train loss: 135144.54513946184 \n",
      "\n",
      "Epoch 947 ============================\n",
      "weight[-536.98114424], bias[-533.01477392]\n",
      "Train loss: 135142.86391340595 \n",
      "\n",
      "Epoch 948 ============================\n",
      "weight[-536.96472786], bias[-533.11365256]\n",
      "Train loss: 135141.19176232556 \n",
      "\n",
      "Epoch 949 ============================\n",
      "weight[-536.94835585], bias[-533.21226396]\n",
      "Train loss: 135139.52863723552 \n",
      "\n",
      "Epoch 950 ============================\n",
      "weight[-536.93202808], bias[-533.31060887]\n",
      "Train loss: 135137.87448941474 \n",
      "\n",
      "Epoch 951 ============================\n",
      "weight[-536.91574443], bias[-533.40868799]\n",
      "Train loss: 135136.2292704058 \n",
      "\n",
      "Epoch 952 ============================\n",
      "weight[-536.8995048], bias[-533.50650205]\n",
      "Train loss: 135134.59293201202 \n",
      "\n",
      "Epoch 953 ============================\n",
      "weight[-536.88330905], bias[-533.60405175]\n",
      "Train loss: 135132.965426298 \n",
      "\n",
      "Epoch 954 ============================\n",
      "weight[-536.86715708], bias[-533.70133783]\n",
      "Train loss: 135131.34670558572 \n",
      "\n",
      "Epoch 955 ============================\n",
      "weight[-536.85104875], bias[-533.79836098]\n",
      "Train loss: 135129.73672245562 \n",
      "\n",
      "Epoch 956 ============================\n",
      "weight[-536.83498396], bias[-533.89512191]\n",
      "Train loss: 135128.1354297436 \n",
      "\n",
      "Epoch 957 ============================\n",
      "weight[-536.81896259], bias[-533.99162135]\n",
      "Train loss: 135126.5427805404 \n",
      "\n",
      "Epoch 958 ============================\n",
      "weight[-536.80298451], bias[-534.08785999]\n",
      "Train loss: 135124.95872818943 \n",
      "\n",
      "Epoch 959 ============================\n",
      "weight[-536.78704962], bias[-534.18383853]\n",
      "Train loss: 135123.38322628665 \n",
      "\n",
      "Epoch 960 ============================\n",
      "weight[-536.77115779], bias[-534.27955769]\n",
      "Train loss: 135121.81622867816 \n",
      "\n",
      "Epoch 961 ============================\n",
      "weight[-536.75530891], bias[-534.37501817]\n",
      "Train loss: 135120.257689459 \n",
      "\n",
      "Epoch 962 ============================\n",
      "weight[-536.73950286], bias[-534.47022065]\n",
      "Train loss: 135118.70756297247 \n",
      "\n",
      "Epoch 963 ============================\n",
      "weight[-536.72373953], bias[-534.56516584]\n",
      "Train loss: 135117.16580380793 \n",
      "\n",
      "Epoch 964 ============================\n",
      "weight[-536.70801881], bias[-534.65985444]\n",
      "Train loss: 135115.63236679995 \n",
      "\n",
      "Epoch 965 ============================\n",
      "weight[-536.69234056], bias[-534.75428713]\n",
      "Train loss: 135114.10720702691 \n",
      "\n",
      "Epoch 966 ============================\n",
      "weight[-536.67670469], bias[-534.84846461]\n",
      "Train loss: 135112.5902798098 \n",
      "\n",
      "Epoch 967 ============================\n",
      "weight[-536.66111108], bias[-534.94238757]\n",
      "Train loss: 135111.0815407106 \n",
      "\n",
      "Epoch 968 ============================\n",
      "weight[-536.64555961], bias[-535.0360567]\n",
      "Train loss: 135109.5809455313 \n",
      "\n",
      "Epoch 969 ============================\n",
      "weight[-536.63005017], bias[-535.12947268]\n",
      "Train loss: 135108.08845031227 \n",
      "\n",
      "Epoch 970 ============================\n",
      "weight[-536.61458264], bias[-535.2226362]\n",
      "Train loss: 135106.60401133142 \n",
      "\n",
      "Epoch 971 ============================\n",
      "weight[-536.59915691], bias[-535.31554794]\n",
      "Train loss: 135105.12758510234 \n",
      "\n",
      "Epoch 972 ============================\n",
      "weight[-536.58377288], bias[-535.40820858]\n",
      "Train loss: 135103.6591283737 \n",
      "\n",
      "Epoch 973 ============================\n",
      "weight[-536.56843042], bias[-535.50061879]\n",
      "Train loss: 135102.19859812755 \n",
      "\n",
      "Epoch 974 ============================\n",
      "weight[-536.55312942], bias[-535.59277927]\n",
      "Train loss: 135100.74595157782 \n",
      "\n",
      "Epoch 975 ============================\n",
      "weight[-536.53786978], bias[-535.68469067]\n",
      "Train loss: 135099.3011461698 \n",
      "\n",
      "Epoch 976 ============================\n",
      "weight[-536.52265138], bias[-535.77635367]\n",
      "Train loss: 135097.8641395784 \n",
      "\n",
      "Epoch 977 ============================\n",
      "weight[-536.5074741], bias[-535.86776895]\n",
      "Train loss: 135096.43488970675 \n",
      "\n",
      "Epoch 978 ============================\n",
      "weight[-536.49233785], bias[-535.95893717]\n",
      "Train loss: 135095.01335468548 \n",
      "\n",
      "Epoch 979 ============================\n",
      "weight[-536.4772425], bias[-536.04985901]\n",
      "Train loss: 135093.59949287112 \n",
      "\n",
      "Epoch 980 ============================\n",
      "weight[-536.46218794], bias[-536.14053512]\n",
      "Train loss: 135092.193262845 \n",
      "\n",
      "Epoch 981 ============================\n",
      "weight[-536.44717408], bias[-536.23096617]\n",
      "Train loss: 135090.79462341193 \n",
      "\n",
      "Epoch 982 ============================\n",
      "weight[-536.43220078], bias[-536.32115283]\n",
      "Train loss: 135089.40353359934 \n",
      "\n",
      "Epoch 983 ============================\n",
      "weight[-536.41726796], bias[-536.41109575]\n",
      "Train loss: 135088.0199526554 \n",
      "\n",
      "Epoch 984 ============================\n",
      "weight[-536.40237549], bias[-536.5007956]\n",
      "Train loss: 135086.64384004867 \n",
      "\n",
      "Epoch 985 ============================\n",
      "weight[-536.38752327], bias[-536.59025302]\n",
      "Train loss: 135085.2751554663 \n",
      "\n",
      "Epoch 986 ============================\n",
      "weight[-536.37271119], bias[-536.67946868]\n",
      "Train loss: 135083.9138588131 \n",
      "\n",
      "Epoch 987 ============================\n",
      "weight[-536.35793914], bias[-536.76844323]\n",
      "Train loss: 135082.55991021008 \n",
      "\n",
      "Epoch 988 ============================\n",
      "weight[-536.34320701], bias[-536.85717732]\n",
      "Train loss: 135081.21326999392 \n",
      "\n",
      "Epoch 989 ============================\n",
      "weight[-536.3285147], bias[-536.9456716]\n",
      "Train loss: 135079.8738987151 \n",
      "\n",
      "Epoch 990 ============================\n",
      "weight[-536.31386209], bias[-537.03392672]\n",
      "Train loss: 135078.54175713717 \n",
      "\n",
      "Epoch 991 ============================\n",
      "weight[-536.29924908], bias[-537.12194332]\n",
      "Train loss: 135077.21680623537 \n",
      "\n",
      "Epoch 992 ============================\n",
      "weight[-536.28467557], bias[-537.20972205]\n",
      "Train loss: 135075.89900719552 \n",
      "\n",
      "Epoch 993 ============================\n",
      "weight[-536.27014144], bias[-537.29726356]\n",
      "Train loss: 135074.5883214133 \n",
      "\n",
      "Epoch 994 ============================\n",
      "weight[-536.25564659], bias[-537.38456847]\n",
      "Train loss: 135073.28471049244 \n",
      "\n",
      "Epoch 995 ============================\n",
      "weight[-536.24119092], bias[-537.47163744]\n",
      "Train loss: 135071.98813624398 \n",
      "\n",
      "Epoch 996 ============================\n",
      "weight[-536.22677431], bias[-537.5584711]\n",
      "Train loss: 135070.69856068518 \n",
      "\n",
      "Epoch 997 ============================\n",
      "weight[-536.21239666], bias[-537.64507009]\n",
      "Train loss: 135069.41594603827 \n",
      "\n",
      "Epoch 998 ============================\n",
      "weight[-536.19805787], bias[-537.73143503]\n",
      "Train loss: 135068.14025472951 \n",
      "\n",
      "Epoch 999 ============================\n",
      "weight[-536.18375783], bias[-537.81756657]\n",
      "Train loss: 135066.87144938784 \n",
      "\n",
      "Epoch 1000 ============================\n",
      "weight[-536.16949644], bias[-537.90346533]\n",
      "Train loss: 135065.60949284383 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LinearRegression(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
